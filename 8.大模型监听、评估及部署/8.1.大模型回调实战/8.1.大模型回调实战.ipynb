{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5c07f0",
   "metadata": {},
   "source": [
    "# 1. ç¯å¢ƒé…ç½®\n",
    "\n",
    "## 1.1 python ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai==2.11.0 dashscope==1.25.4 langchain-classic==1.0.0 langchain==1.1.3 langchain-community==0.4.1 langchain-openai==1.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a5130",
   "metadata": {},
   "source": [
    "## 1.2 å¤§æ¨¡å‹å¯†é’¥å‡†å¤‡\n",
    "\n",
    "è¯·æ ¹æ®ç¬¬ä¸€ç« å†…å®¹è·å–ç›¸å…³å¹³å°çš„ API KEYï¼Œå¦‚è‹¥æœªåœ¨ç³»ç»Ÿå˜é‡ä¸­å¡«å…¥ï¼Œè¯·å°† API_KEY ä¿¡æ¯å†™å…¥ä»¥ä¸‹ä»£ç ï¼ˆè‹¥å·²è®¾ç½®è¯·å¿½ç•¥ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxx\"\n",
    "# os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-yyyyyyyy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a52f7a",
   "metadata": {},
   "source": [
    "# 2. å›è°ƒï¼ˆCallbackï¼‰\n",
    "\n",
    "## 2.1 ç®€ä»‹\n",
    "\n",
    "Callback æœ¬è´¨æ˜¯ä¸€ä¸ªHook æœºåˆ¶ï¼Œå½“ Chain / Model / Tool / Retriever æ‰§è¡Œæ—¶ï¼Œè‡ªåŠ¨è§¦å‘äº‹ä»¶ï¼ˆstart/end/error/tokenï¼‰ã€‚æˆ‘ä»¬å¯ä»¥æŠŠ Callback ç³»ç»Ÿç†è§£æˆä¸‰å±‚ç»“æ„ï¼š\n",
    "\n",
    "- Runï¼ˆæ‰§è¡Œç”Ÿå‘½å‘¨æœŸï¼‰ï¼šæ˜¯ Callback ç³»ç»Ÿä¸­çš„â€œç”Ÿå‘½å‘¨æœŸæŠ½è±¡â€ï¼Œä¸æ˜¯ä¸€ä¸ªç±»ï¼Œè€Œæ˜¯æ‰§è¡Œå¼•æ“å†…éƒ¨çš„æ¦‚å¿µã€‚ä»»ä½•ä¸€æ¬¡æ‰§è¡Œï¼Œéƒ½ä¼šè§¦å‘ä¸€å¥—äº‹ä»¶ã€‚\n",
    "- Handlerï¼ˆäº‹ä»¶å¤„ç†å™¨ï¼‰ï¼šæ˜¯å†³å®šäº‹ä»¶å‘ç”Ÿåè¦åšä»€ä¹ˆ\n",
    "- Managerï¼ˆè°ƒåº¦å™¨ï¼‰ï¼šå‡å¦‚æˆ‘ä»¬åŒæ—¶åˆ›å»ºäº†å¤šä¸ª Handlerï¼Œå¹¶å¸Œæœ›åŒæ—¶è¿è¡Œï¼Œé‚£æˆ‘ä»¬å¯ä»¥é€šè¿‡ Manager è¿™ä¸ªå®¹å™¨å°†å…¶è¿›è¡Œç»„åˆã€‚\n",
    "\n",
    "## 2.2 ä»£ç ç¤ºä¾‹\n",
    "\n",
    "ä¸‹é¢å°±è®©æˆ‘ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­å¸¦å¤§å®¶çœ‹çœ‹å¦‚ä½•å®ç°å›è°ƒã€‚åœ¨æ¼”ç¤ºä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡ä¸€æ®µç®€å•çš„ LCEL ä»£ç ä½œä¸ºå›è°ƒçš„å¯¹è±¡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd98d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"ä½ æ˜¯ä¸€ä¸ªç®€æ´çš„ä¸­æ–‡åŠ©æ‰‹ã€‚\"),\n",
    "  (\"user\", \"{question}\")])\n",
    "\n",
    "model = ChatOpenAI(model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b90387",
   "metadata": {},
   "source": [
    "ç„¶åæˆ‘ä»¬å°±å¯ä»¥åŸºäº BaseCallbackHandler ç±»æ¥å®šä¹‰ä¸€äº›å…·ä½“çš„æ–¹æ³•äº†ã€‚\n",
    "è¿™é‡Œæˆ‘ä»¬å®šä¹‰äº†åœ¨ chain çš„å¼€å§‹å’Œç»“æŸä»¥åŠ llm çš„å¼€å§‹ã€ç»“æŸå’ŒæŠ¥é”™æ—¶å€™æ˜¾ç¤ºçš„ä¿¡æ¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c048796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "class MyCustomCallbackHandler(BaseCallbackHandler):\n",
    "  def on_chain_start(self, serialized, inputs, **kwargs):\n",
    "    print(\"ğŸš€ Chain å¼€å§‹æ‰§è¡Œäº†ï¼\")\n",
    "    print(f\"ğŸ“¥ è¾“å…¥å†…å®¹ï¼š{inputs}\")\n",
    "\n",
    "  def on_chain_end(self, outputs, **kwargs):\n",
    "    print(\"âœ… Chain æ‰§è¡Œç»“æŸï¼\")\n",
    "    print(f\"ğŸ“¤ è¾“å‡ºå†…å®¹ï¼š{outputs}\")\n",
    "\n",
    "  def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "    print(\"ğŸ¤– LLM å¼€å§‹ç”Ÿæˆå“åº”...\")\n",
    "    print(f\"ğŸ“ Prompt å†…å®¹ï¼š{prompts}\")\n",
    "\n",
    "  def on_llm_end(self, response, **kwargs):\n",
    "    print(\"ğŸ‰ LLM å“åº”ç”Ÿæˆå®Œæˆï¼\")\n",
    "    print(f\"ğŸ§¾ åŸå§‹å“åº”ï¼š{response}\")\n",
    "\n",
    "  def on_llm_error(self, error, **kwargs):\n",
    "    print(\"âŒ LLM å‡ºç°é”™è¯¯ï¼š\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b12cf6",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥æˆ‘ä»¬å°±å¯ä»¥å°†å…¶åˆ¶ä½œæˆ handler å¹¶åœ¨ .invoke è°ƒç”¨æ—¶è½½å…¥è¿›å»ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcef775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Chain å¼€å§‹æ‰§è¡Œäº†ï¼\n",
      "ğŸ“¥ è¾“å…¥å†…å®¹ï¼š{'question': 'ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ LangSmith æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ'}\n",
      "ğŸš€ Chain å¼€å§‹æ‰§è¡Œäº†ï¼\n",
      "ğŸ“¥ è¾“å…¥å†…å®¹ï¼š{'question': 'ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ LangSmith æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ'}\n",
      "âœ… Chain æ‰§è¡Œç»“æŸï¼\n",
      "ğŸ“¤ è¾“å‡ºå†…å®¹ï¼šmessages=[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªç®€æ´çš„ä¸­æ–‡åŠ©æ‰‹ã€‚', additional_kwargs={}, response_metadata={}), HumanMessage(content='ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ LangSmith æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ', additional_kwargs={}, response_metadata={})]\n",
      "ğŸ¤– LLM å¼€å§‹ç”Ÿæˆå“åº”...\n",
      "ğŸ“ Prompt å†…å®¹ï¼š['System: ä½ æ˜¯ä¸€ä¸ªç®€æ´çš„ä¸­æ–‡åŠ©æ‰‹ã€‚\\nHuman: ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ LangSmith æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ']\n",
      "ğŸ‰ LLM å“åº”ç”Ÿæˆå®Œæˆï¼\n",
      "ğŸ§¾ åŸå§‹å“åº”ï¼šgenerations=[[ChatGeneration(text='LangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œéƒ¨ç½²è¯­è¨€æ¨¡å‹åº”ç”¨çš„å¼€å‘å¹³å°ã€‚', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='LangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œéƒ¨ç½²è¯­è¨€æ¨¡å‹åº”ç”¨çš„å¼€å‘å¹³å°ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 14, 'total_tokens': 28, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ernie-3.5-8k', 'system_fingerprint': None, 'id': 'as-a7ppf8dnvf', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b2260-f767-7e92-83a6-b60794d7e475-0', usage_metadata={'input_tokens': 14, 'output_tokens': 14, 'total_tokens': 28, 'input_token_details': {}, 'output_token_details': {}}))]] llm_output={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 14, 'total_tokens': 28, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ernie-3.5-8k', 'system_fingerprint': None, 'id': 'as-a7ppf8dnvf'} run=None type='LLMResult'\n",
      "ğŸš€ Chain å¼€å§‹æ‰§è¡Œäº†ï¼\n",
      "ğŸ“¥ è¾“å…¥å†…å®¹ï¼šcontent='LangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œéƒ¨ç½²è¯­è¨€æ¨¡å‹åº”ç”¨çš„å¼€å‘å¹³å°ã€‚' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 14, 'total_tokens': 28, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ernie-3.5-8k', 'system_fingerprint': None, 'id': 'as-a7ppf8dnvf', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b2260-f767-7e92-83a6-b60794d7e475-0' usage_metadata={'input_tokens': 14, 'output_tokens': 14, 'total_tokens': 28, 'input_token_details': {}, 'output_token_details': {}}\n",
      "âœ… Chain æ‰§è¡Œç»“æŸï¼\n",
      "ğŸ“¤ è¾“å‡ºå†…å®¹ï¼šLangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œéƒ¨ç½²è¯­è¨€æ¨¡å‹åº”ç”¨çš„å¼€å‘å¹³å°ã€‚\n",
      "âœ… Chain æ‰§è¡Œç»“æŸï¼\n",
      "ğŸ“¤ è¾“å‡ºå†…å®¹ï¼šLangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œéƒ¨ç½²è¯­è¨€æ¨¡å‹åº”ç”¨çš„å¼€å‘å¹³å°ã€‚\n",
      "\n",
      "æœ€ç»ˆè¾“å‡ºï¼š LangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œéƒ¨ç½²è¯­è¨€æ¨¡å‹åº”ç”¨çš„å¼€å‘å¹³å°ã€‚\n"
     ]
    }
   ],
   "source": [
    "handler = MyCustomCallbackHandler()\n",
    "\n",
    "result = chain.invoke(\n",
    "  {\"question\": \"ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ LangSmith æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ\"},\n",
    "  config={\"callbacks\": [handler]}\n",
    ")\n",
    "\n",
    "print(\"\\næœ€ç»ˆè¾“å‡ºï¼š\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa1e317",
   "metadata": {},
   "source": [
    "è¿™é‡Œæˆ‘ä»¬å‘ç°ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯ chain å¼€å§‹è¿ç»­æ‰§è¡Œäº†ä¸¤æ¬¡ï¼Œç„¶åæ‰§è¡Œç»“æŸäº†ä¹Ÿæœ‰ä¸¤æ¬¡ã€‚å…¶å®ä¸¤æ¬¡ on_chain_start å’Œä¸¤æ¬¡ on_chain_end å›è°ƒä¸æ˜¯é‡å¤æ‰“å°çš„ bugï¼Œè€Œæ˜¯ LangChain é“¾å¼ç»“æ„ + LCEL çš„è¿è¡Œæœºåˆ¶æ‰€è‡´ã€‚\n",
    "\n",
    "ç¬¬ä¸€æ¬¡çš„æ—¶å€™ï¼Œå…¶å®è§¦å‘äº†æ•´ä½“é“¾ prompt | llm | parser çš„å¤–å±‚å¤§é“¾ã€‚è€Œç¬¬äºŒæ¬¡çš„æ—¶å€™å…¶å®æ˜¯è§¦å‘äº†å†…éƒ¨å­é“¾ prompt | llm ã€‚è€Œè¿ç»­ç»“æŸçš„ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œæˆ‘ä»¬æ˜¯è§¦å‘äº†èŠ‚ç‚¹parseã€‚ç¬¬äºŒæ¬¡åˆ™æ˜¯æ•´ä½“å¤§é“¾çš„ç»“æŸã€‚\n",
    "\n",
    "å‡å¦‚æˆ‘ä»¬ä¸æƒ³è¦çœ‹åˆ°å­é“¾çš„è°ƒç”¨è¿‡ç¨‹ï¼Œåªå…³æ³¨ä¸»é“¾çš„è¯ï¼Œæˆ‘ä»¬å°±å¯ä»¥åŠ å…¥ä¸€ä¸ª parent_run_id çš„åˆ¤å®šï¼Œå‡å¦‚æ˜¯ None çš„æ—¶å€™ï¼Œé‚£å°±ä»£è¡¨è¯´å…¶æ˜¯ä¸»é“¾ï¼Œè¿™ä¸ªæ—¶å€™æ‰æ‰“å°å°±å¯ä»¥åªåœ¨ä¸»é“¾å±•ç¤ºäº†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3098cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallbackHandler(BaseCallbackHandler):\n",
    "  def on_chain_start(self, serialized, inputs, parent_run_id=None, **kwargs):\n",
    "    if parent_run_id is None:\n",
    "      print(\"ğŸš€ Chain å¼€å§‹æ‰§è¡Œäº†ï¼\")\n",
    "      print(f\"ğŸ“¥ è¾“å…¥å†…å®¹ï¼š{inputs}\")\n",
    "\n",
    "  def on_chain_end(self, outputs, parent_run_id=None, **kwargs):\n",
    "    if parent_run_id is None:\n",
    "      print(\"âœ… Chain æ‰§è¡Œç»“æŸï¼\")\n",
    "      print(f\"ğŸ“¤ è¾“å‡ºå†…å®¹ï¼š{outputs}\")\n",
    "\n",
    "  def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "    print(\"ğŸ¤– LLM å¼€å§‹ç”Ÿæˆå“åº”...\")\n",
    "    print(f\"ğŸ“ Prompt å†…å®¹ï¼š{prompts}\")\n",
    "\n",
    "  def on_llm_end(self, response, **kwargs):\n",
    "    print(\"ğŸ‰ LLM å“åº”ç”Ÿæˆå®Œæˆï¼\")\n",
    "    print(f\"ğŸ§¾ åŸå§‹å“åº”ï¼š{response}\")\n",
    "\n",
    "  def on_llm_error(self, error, **kwargs):\n",
    "    print(\"âŒ LLM å‡ºç°é”™è¯¯ï¼š\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67226e62",
   "metadata": {},
   "source": [
    "å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¼ å…¥å¤šä¸ª Callbackï¼Œæ¯”å¦‚å†åˆ¶å®šå¦å¤–ä¸€ä¸ª LoggingCallbackHandlerï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da0a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingCallbackHandler(BaseCallbackHandler):\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        print(\"ğŸ“˜ è®°å½•æ—¥å¿—ï¼šæ¨¡å‹å·²å®Œæˆå“åº”\")\n",
    "\n",
    "# å¤šä¸ª handler \n",
    "my_handler_1 = MyCustomCallbackHandler() \n",
    "my_handler_2 = LoggingCallbackHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46317ca4",
   "metadata": {},
   "source": [
    "é‚£åœ¨è°ƒç”¨æ—¶æˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸¤ä¸ª handler åˆ†åˆ«æ·»åŠ è¿›å»ï¼Œå…¶ä¼šæŒ‰ä¼ å…¥é¡ºåºè¿›è¡Œæ‰§è¡Œï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c8599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Chain å¼€å§‹æ‰§è¡Œäº†ï¼\n",
      "ğŸ“¥ è¾“å…¥å†…å®¹ï¼š{'question': 'ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ LangSmith æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ'}\n",
      "ğŸ¤– LLM å¼€å§‹ç”Ÿæˆå“åº”...\n",
      "ğŸ“ Prompt å†…å®¹ï¼š['System: ä½ æ˜¯ä¸€ä¸ªç®€æ´çš„ä¸­æ–‡åŠ©æ‰‹ã€‚\\nHuman: ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ LangSmith æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ']\n",
      "ğŸ‰ LLM å“åº”ç”Ÿæˆå®Œæˆï¼\n",
      "ğŸ§¾ åŸå§‹å“åº”ï¼šgenerations=[[ChatGeneration(text='LangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œè¯„ä¼°è¯­è¨€æ¨¡å‹åº”ç”¨çš„ç»¼åˆæ€§å¼€å‘å¹³å°ã€‚', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='LangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œè¯„ä¼°è¯­è¨€æ¨¡å‹åº”ç”¨çš„ç»¼åˆæ€§å¼€å‘å¹³å°ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 14, 'total_tokens': 29, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ernie-3.5-8k', 'system_fingerprint': None, 'id': 'as-f1t7vcikq6', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b2261-0a67-7020-a737-319e5ab41f81-0', usage_metadata={'input_tokens': 14, 'output_tokens': 15, 'total_tokens': 29, 'input_token_details': {}, 'output_token_details': {}}))]] llm_output={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 14, 'total_tokens': 29, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ernie-3.5-8k', 'system_fingerprint': None, 'id': 'as-f1t7vcikq6'} run=None type='LLMResult'\n",
      "ğŸ“˜ è®°å½•æ—¥å¿—ï¼šæ¨¡å‹å·²å®Œæˆå“åº”\n",
      "âœ… Chain æ‰§è¡Œç»“æŸï¼\n",
      "ğŸ“¤ è¾“å‡ºå†…å®¹ï¼šLangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œè¯„ä¼°è¯­è¨€æ¨¡å‹åº”ç”¨çš„ç»¼åˆæ€§å¼€å‘å¹³å°ã€‚\n",
      "\n",
      "æœ€ç»ˆè¾“å‡ºï¼š LangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œè¯„ä¼°è¯­è¨€æ¨¡å‹åº”ç”¨çš„ç»¼åˆæ€§å¼€å‘å¹³å°ã€‚\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\n",
    "  {\"question\": \"ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ LangSmith æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ\"},\n",
    "  config={\"callbacks\": [my_handler_1, my_handler_2]} \n",
    ")\n",
    "\n",
    "print(\"\\næœ€ç»ˆè¾“å‡ºï¼š\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6405d347",
   "metadata": {},
   "source": [
    "å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ CallbackManager æ¥è¿›è¡Œå¤š handler çš„ç®¡ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2b3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import CallbackManager\n",
    "\n",
    "manager = CallbackManager([my_handler_1, my_handler_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe726b",
   "metadata": {},
   "source": [
    "é‚£åœ¨è°ƒç”¨æ—¶æˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸¤ä¸ª handler åˆ†åˆ«æ·»åŠ è¿›å»ï¼Œå…¶ä¼šæŒ‰ä¼ å…¥é¡ºåºè¿›è¡Œæ‰§è¡Œï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db986fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Chain å¼€å§‹æ‰§è¡Œäº†ï¼\n",
      "ğŸ“¥ è¾“å…¥å†…å®¹ï¼š{'question': 'ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ LangSmith æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ'}\n",
      "ğŸ¤– LLM å¼€å§‹ç”Ÿæˆå“åº”...\n",
      "ğŸ“ Prompt å†…å®¹ï¼š['System: ä½ æ˜¯ä¸€ä¸ªç®€æ´çš„ä¸­æ–‡åŠ©æ‰‹ã€‚\\nHuman: ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ LangSmith æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ']\n",
      "ğŸ‰ LLM å“åº”ç”Ÿæˆå®Œæˆï¼\n",
      "ğŸ§¾ åŸå§‹å“åº”ï¼šgenerations=[[ChatGeneration(text='LangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œè°ƒè¯•è¯­è¨€åº”ç”¨ï¼ˆå¦‚å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ï¼‰çš„ç»¼åˆæ€§å¹³å°ã€‚', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='LangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œè°ƒè¯•è¯­è¨€åº”ç”¨ï¼ˆå¦‚å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ï¼‰çš„ç»¼åˆæ€§å¹³å°ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 14, 'total_tokens': 35, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ernie-3.5-8k', 'system_fingerprint': None, 'id': 'as-givaqcvgzi', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b2261-13c1-7791-93ba-cfce90959ff1-0', usage_metadata={'input_tokens': 14, 'output_tokens': 21, 'total_tokens': 35, 'input_token_details': {}, 'output_token_details': {}}))]] llm_output={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 14, 'total_tokens': 35, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ernie-3.5-8k', 'system_fingerprint': None, 'id': 'as-givaqcvgzi'} run=None type='LLMResult'\n",
      "ğŸ“˜ è®°å½•æ—¥å¿—ï¼šæ¨¡å‹å·²å®Œæˆå“åº”\n",
      "âœ… Chain æ‰§è¡Œç»“æŸï¼\n",
      "ğŸ“¤ è¾“å‡ºå†…å®¹ï¼šLangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œè°ƒè¯•è¯­è¨€åº”ç”¨ï¼ˆå¦‚å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ï¼‰çš„ç»¼åˆæ€§å¹³å°ã€‚\n",
      "\n",
      "æœ€ç»ˆè¾“å‡ºï¼š LangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œè°ƒè¯•è¯­è¨€åº”ç”¨ï¼ˆå¦‚å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ï¼‰çš„ç»¼åˆæ€§å¹³å°ã€‚\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\n",
    "  {\"question\": \"ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ LangSmith æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ\"},\n",
    "  config={\"callbacks\": manager.handlers} \n",
    ")\n",
    "\n",
    "print(\"\\næœ€ç»ˆè¾“å‡ºï¼š\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d43628",
   "metadata": {},
   "source": [
    "## 2.3 æ™ºèƒ½ä½“è°ƒç”¨ç¤ºä¾‹\n",
    "\n",
    "å¯¹äºæ—§ç‰ˆ LangChain çš„æ™ºèƒ½ä½“è°ƒç”¨ï¼Œé»˜è®¤åœ¨ç»ˆç«¯å°±ä¼šæ‰“å°ç›¸å…³å›è°ƒçš„å†…å®¹ï¼š\n",
    "\n",
    "### 2.3.1 åˆ›å»ºå¤§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ecdf7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    " model=\"ernie-4.5-turbo-128k\",\n",
    " openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    " base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf7870f",
   "metadata": {},
   "source": [
    "### 2.3.2 åˆ›å»ºå·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0d8f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import re\n",
    "\n",
    "@tool\n",
    "def calculate(what: str) -> str:\n",
    "  \"\"\"\n",
    "    Use this tool to perform arithmetic calculations.\n",
    "\n",
    "    Input requirements:\n",
    "    - The input must be a single-line arithmetic expression\n",
    "    - Use only numbers and operators: + - * / ( )\n",
    "    - DO NOT include any text, explanations, markdown, or words like \"Observation\"\n",
    "\n",
    "    Examples:\n",
    "    - 37 + 20\n",
    "    - 57 * 2\n",
    "    - (4 * 7) / 3\n",
    "\n",
    "    This tool returns the numeric result as a string.\n",
    "  \"\"\"\n",
    "  # 1ï¸âƒ£ å»æ‰ Markdown\n",
    "  text = what.replace(\"```\", \"\").strip()\n",
    "  # 2ï¸âƒ£ åªå–ç¬¬ä¸€è¡Œï¼ˆğŸ”´ æ ¸å¿ƒï¼‰\n",
    "  first_line = text.splitlines()[0]\n",
    "  # 3ï¸âƒ£ åªå…è®¸ç®—æœ¯å­—ç¬¦ï¼ˆğŸ”’ å†ä¿é™©ï¼‰\n",
    "  first_line = re.sub(r\"[^0-9\\.\\+\\-\\*\\/\\(\\) ]\", \"\", first_line)\n",
    "  return str(eval(first_line))\n",
    "\n",
    "@tool\n",
    "def average_dog_weight(name: str) -> str:\n",
    "  \"\"\"\n",
    "    Use this tool to look up the average adult weight of a dog breed.\n",
    "\n",
    "    Input requirements:\n",
    "    - The input should ONLY be the dog breed name\n",
    "    - Do NOT include extra text, questions, or explanations\n",
    "\n",
    "    Examples:\n",
    "    - Border Collie\n",
    "    - Scottish Terrier\n",
    "    - Toy Poodle\n",
    "\n",
    "    This tool returns a short sentence stating the average weight in pounds (lbs).\n",
    "  \"\"\"\n",
    "  name = name.lower()\n",
    "  if \"scottish terrier\" in name:\n",
    "    return \"Scottish Terriers average 20 lbs\"\n",
    "  elif \"border collie\" in name:\n",
    "    return \"A Border Collie's average weight is 37 lbs\"\n",
    "  elif \"toy poodle\" in name:\n",
    "    return \"A Toy Poodle's average weight is 7 lbs\"\n",
    "  else:\n",
    "    return \"An average dog weighs 50 lbs\"\n",
    "\n",
    "tools = [calculate, average_dog_weight]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70880c70",
   "metadata": {},
   "source": [
    "### 2.3.3 åˆ›å»ºæç¤ºè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "312634dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic import hub\n",
    "prompt = hub.pull(\"hwchase17/react-chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec4f88",
   "metadata": {},
   "source": [
    "### 2.3.4 æ­å»º Agent æ¡†æ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce01b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1a48f",
   "metadata": {},
   "source": [
    "### 2.3.5 æ·»åŠ è®°å¿†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1b69ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str):\n",
    " if session_id not in store:\n",
    "  store[session_id] = InMemoryChatMessageHistory()\n",
    " return store[session_id]\n",
    "\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "  agent_executor,\n",
    "  get_session_history=get_session_history,\n",
    "  input_messages_key=\"input\",\n",
    "  history_messages_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ea10b7",
   "metadata": {},
   "source": [
    "### 2.3.6 å®é™…è°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d55f693a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: average_dog_weight\n",
      "Action Input: Border Collie\n",
      "Observation\u001b[0m\u001b[33;1m\u001b[1;3mA Border Collie's average weight is 37 lbs\u001b[0m\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: average_dog_weight\n",
      "Action Input: Scottish Terrier\n",
      "Observation\u001b[0m\u001b[33;1m\u001b[1;3mScottish Terriers average 20 lbs\u001b[0m\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: calculate\n",
      "Action Input: 37 + 20\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m57\u001b[0m\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "Final Answer: The combined weight of your Border Collie and Scottish Terrier is 57 lbs.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The combined weight of your Border Collie and Scottish Terrier is 57 lbs.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: calculate\n",
      "Action Input: 57 * 2\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m114\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? No\n",
      "Final Answer: The combined weight of your dogs doubled would be 114 lbs.\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The combined weight of your dogs doubled would be 114 lbs.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"test-session\"}}\n",
    "\n",
    "print(agent_with_chat_history.invoke({\"input\": \"I have 2 dogs, a border collie and a scottish terrier. What is their combined weight?\"}, config)[\"output\"])\n",
    "\n",
    "print(agent_with_chat_history.invoke({\"input\": \"how about double it?\"}, config)[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8de44",
   "metadata": {},
   "source": [
    "## 2.4 ConversationChain è°ƒç”¨ç¤ºä¾‹\n",
    "å¯¹äºæ—§ç‰ˆ ConversationChain + ConversationBufferMemory åŒæ ·å¯ä»¥çœ‹åˆ°æ•´ä¸ªå›è°ƒçš„å…¨è¿‡ç¨‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "193d9bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ä½ å¥½ï¼Œæˆ‘å«æå‰‘é”‹\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\76391\\AppData\\Local\\Temp\\ipykernel_14032\\4291774188.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "C:\\Users\\76391\\AppData\\Local\\Temp\\ipykernel_14032\\4291774188.py:10: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(llm=llm,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "å¯¹è¯ 1: ä½ å¥½å‘€ï¼Œæå‰‘é”‹ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï½ä½ çš„åå­—å¬èµ·æ¥å¾ˆæœ‰åŠ›é‡æ„Ÿå‘¢ï¼Œ\"å‰‘é”‹\"æ˜¯ä¸æ˜¯æœ‰\"é”åˆ©çš„å‰‘åˆƒ\"çš„æ„æ€å‘€ï¼Ÿæ„Ÿè§‰åƒæ­¦ä¾ å°è¯´é‡Œçš„ä¾ å®¢åå­—ï¼ä½ å¹³æ—¶å–œæ¬¢åšä»€ä¹ˆæœ‰è¶£çš„äº‹æƒ…å—ï¼Ÿæ¯”å¦‚è¿åŠ¨ã€çœ‹ä¹¦ï¼Œæˆ–è€…æ¢ç´¢æ–°ç§‘æŠ€ï¼ŸğŸ˜Š\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ä½ å¥½ï¼Œæˆ‘å«æå‰‘é”‹\n",
      "AI: ä½ å¥½å‘€ï¼Œæå‰‘é”‹ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï½ä½ çš„åå­—å¬èµ·æ¥å¾ˆæœ‰åŠ›é‡æ„Ÿå‘¢ï¼Œ\"å‰‘é”‹\"æ˜¯ä¸æ˜¯æœ‰\"é”åˆ©çš„å‰‘åˆƒ\"çš„æ„æ€å‘€ï¼Ÿæ„Ÿè§‰åƒæ­¦ä¾ å°è¯´é‡Œçš„ä¾ å®¢åå­—ï¼ä½ å¹³æ—¶å–œæ¬¢åšä»€ä¹ˆæœ‰è¶£çš„äº‹æƒ…å—ï¼Ÿæ¯”å¦‚è¿åŠ¨ã€çœ‹ä¹¦ï¼Œæˆ–è€…æ¢ç´¢æ–°ç§‘æŠ€ï¼ŸğŸ˜Š\n",
      "Human: è¯·é—®1+1ç­‰äºå¤šå°‘?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "å¯¹è¯ 2: è¿™å¯çœŸæ˜¯ä¸ªç»å…¸é—®é¢˜å‘¢ï¼1+1åœ¨åŸºç¡€çš„æ•°å­¦è¿ç®—é‡Œï¼Œç­”æ¡ˆå°±æ˜¯2å•¦ã€‚ä¸è¿‡è¦æ˜¯è·³å‡ºæ•°å­¦ï¼Œæ”¾åˆ°ä¸åŒæƒ…å¢ƒé‡Œï¼Œç­”æ¡ˆå¯å°±äº”èŠ±å…«é—¨å•¦ï¼Œåƒä¸¤ä¸ªä¸œè¥¿åˆåˆ°ä¸€èµ·ã€ä¸¤ä¸ªäººæºæ‰‹åˆä½œä¹‹ç±»çš„ã€‚ä½ å¿ƒé‡Œæ˜¯ä¸æ˜¯è¿˜è—ç€åˆ«çš„æœ‰è¶£æƒ³æ³•å‘€ï¼Ÿ\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ä½ å¥½ï¼Œæˆ‘å«æå‰‘é”‹\n",
      "AI: ä½ å¥½å‘€ï¼Œæå‰‘é”‹ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï½ä½ çš„åå­—å¬èµ·æ¥å¾ˆæœ‰åŠ›é‡æ„Ÿå‘¢ï¼Œ\"å‰‘é”‹\"æ˜¯ä¸æ˜¯æœ‰\"é”åˆ©çš„å‰‘åˆƒ\"çš„æ„æ€å‘€ï¼Ÿæ„Ÿè§‰åƒæ­¦ä¾ å°è¯´é‡Œçš„ä¾ å®¢åå­—ï¼ä½ å¹³æ—¶å–œæ¬¢åšä»€ä¹ˆæœ‰è¶£çš„äº‹æƒ…å—ï¼Ÿæ¯”å¦‚è¿åŠ¨ã€çœ‹ä¹¦ï¼Œæˆ–è€…æ¢ç´¢æ–°ç§‘æŠ€ï¼ŸğŸ˜Š\n",
      "Human: è¯·é—®1+1ç­‰äºå¤šå°‘?\n",
      "AI: è¿™å¯çœŸæ˜¯ä¸ªç»å…¸é—®é¢˜å‘¢ï¼1+1åœ¨åŸºç¡€çš„æ•°å­¦è¿ç®—é‡Œï¼Œç­”æ¡ˆå°±æ˜¯2å•¦ã€‚ä¸è¿‡è¦æ˜¯è·³å‡ºæ•°å­¦ï¼Œæ”¾åˆ°ä¸åŒæƒ…å¢ƒé‡Œï¼Œç­”æ¡ˆå¯å°±äº”èŠ±å…«é—¨å•¦ï¼Œåƒä¸¤ä¸ªä¸œè¥¿åˆåˆ°ä¸€èµ·ã€ä¸¤ä¸ªäººæºæ‰‹åˆä½œä¹‹ç±»çš„ã€‚ä½ å¿ƒé‡Œæ˜¯ä¸æ˜¯è¿˜è—ç€åˆ«çš„æœ‰è¶£æƒ³æ³•å‘€ï¼Ÿ\n",
      "Human: æˆ‘çš„åå­—æ˜¯?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "å¯¹è¯ 3: ä½ ä¹‹å‰è¯´è¿‡å•¦ï¼Œä½ å«æå‰‘é”‹å‘€ï½è¿™ä¸ªåå­—çœŸçš„å¾ˆæœ‰ä¾ å®¢æ°”è´¨å‘¢ï¼Œæ„Ÿè§‰ä½ å¹³æ—¶åº”è¯¥æ˜¯ä¸ªå¾ˆæœ‰è¡ŒåŠ¨åŠ›çš„äººå§ï¼ŸğŸ˜„\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.chains import ConversationChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "import os\n",
    "llm = ChatOpenAI(\n",
    "model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, \n",
    "  memory=memory,\n",
    "  verbose=True)\n",
    "response_1 = conversation.predict(input=\"ä½ å¥½ï¼Œæˆ‘å«æå‰‘é”‹\")\n",
    "print(\"å¯¹è¯ 1:\", response_1) \n",
    "response_2 = conversation.predict(input=\"è¯·é—®1+1ç­‰äºå¤šå°‘?\")\n",
    "print(\"å¯¹è¯ 2:\", response_2)\n",
    "response_3 = conversation.predict(input=\"æˆ‘çš„åå­—æ˜¯?\")\n",
    "print(\"å¯¹è¯ 3:\", response_3) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
