{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5c07f0",
   "metadata": {},
   "source": [
    "# 1. 环境配置\n",
    "\n",
    "## 1.1 python 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gradio==6.1.0 openai==2.11.0 dashscope==1.25.4 langchain-classic==1.0.0 langchain==1.1.3 langchain-community==0.4.1 langchain-openai==1.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a5130",
   "metadata": {},
   "source": [
    "## 1.2 大模型密钥准备\n",
    "\n",
    "请根据第一章内容获取相关平台的 API KEY，如若未在系统变量中填入，请将 API_KEY 信息写入以下代码（若已设置请忽略）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxx\"\n",
    "# os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-yyyyyyyy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab074b",
   "metadata": {},
   "source": [
    "## 1.3 LangSmith 环境配置\n",
    "我们需要先前往 LangSmith 的官网并进行注册登录。\n",
    "\n",
    "登录后我们就进入了下面这个初始界面，此时我们需要找到左下角的 Setting ，然后在里面先获取新建一个 API Key。\n",
    "\n",
    "创建完成后，我们就可以将其配置到环境变量中。除了 API_Key 以外，通常 LangSmith 的项目还需要设置是否跟踪、上传地址以及项目名称信息（这个需要自定义设置）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1561a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"your langsmith api key\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"ai-studio-evaluation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a3c13",
   "metadata": {},
   "source": [
    "# 2. 大模型评估\n",
    "\n",
    "## 2.1 大模型评估简介\n",
    "\n",
    "在构建和迭代大模型应用时，我们往往会频繁修改提示词（Prompt）、替换底层模型、或调整系统架构。但一个常被忽视的问题是：这些改动究竟让应用变得更好，还是更糟？\n",
    "\n",
    "如果没有系统性的评估流程，我们只能依赖零散的“肉眼检查”或几道随便试试的“gut check”来判断，而这种主观方式不仅效率低，更无法发现隐藏的退化（regression）问题。\n",
    "\n",
    "随着 LLM 应用规模和复杂度不断增加，评估（Evaluation）成为保障应用质量的关键环节。评估不是单纯为了“算一个分数”，它的根本价值在于：\n",
    "- 提供一致、可量化的对比基准，确保每一版应用都在同一套标准上测试；\n",
    "- 及时发现模型退化，避免提示词更新、模型替换或结构优化带来意外的质量下降；\n",
    "- 帮助开发者理解改动的真实影响，为上线决策提供硬数据；\n",
    "- 推动 LLM 应用进入工程化、可持续优化的循环。\n",
    "\n",
    "因此，LangSmith 提供了一套完整的评估体系，让开发者可以真实、系统地回答以下问题：“随着应用迭代，我们真的在变得更好吗？”\n",
    "\n",
    "这套体系由三个核心组件组成：\n",
    "- Datasets — 用于构建稳定、可复用的测试集，让每次版本对比都公平透明。\n",
    "- Evaluators — 根据输入、参考答案与模型输出计算各种评价指标，可以是用大模型来对特定组合进行评判，也可以设定特定规则来进行分析。\n",
    "- Experiments — 将应用跑在数据集上，附加评估器，生成可视化的评分结果。\n",
    "\n",
    "本质上这个实验打分的逻辑很简单，就是我们创建数据集（Dataset）里的每一条例子（Example）都是有输入（Input）和默认输出（Reference Output）。\n",
    "那评估时，我们将会基于 Input 输入的问题，然后放到一个大模型的应用中运行（Run）并产生出结果（Output）。\n",
    "然后评估器（Evaluator）将根据我们的设定的规则对比 Output 和 Reference Output 进行打分。\n",
    "\n",
    "除了单轮的评估外，假如通过控制变量的方式进行了多轮的评估，我们可以在 LangSmith 上看到多轮结果的对比情况。\n",
    "这样就能够帮助我们更直观地分析实验的结果，从而更快的找到最优的应用组合。\n",
    "\n",
    "下面就通过一个实际的对话模型评估的例子来进行演示！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb00162f",
   "metadata": {},
   "source": [
    "## 2.2 评估 ChatBot 应用\n",
    "\n",
    "基于前面学习到的内容，假如我们要评估一个对话应用的话，我们开展的步骤为：\n",
    "- 明确评估的目的（要改进什么，是准确率还是幻觉？怎么判断变好了还是变坏了？）\n",
    "- 根据评估目的准备测试的数据集（Dataset）\n",
    "- 根据评估目的搭建评估器（Evaluator）\n",
    "- 准备对话应用所需的代码\n",
    "- 启动实验（Experiment）\n",
    "- 查看评估结果\n",
    "- 更改提示词或模型再次启动实验（Experiment）\n",
    "- 对比多次评估结果找到最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92414eac",
   "metadata": {},
   "source": [
    "### 2.2.1 明确评估的目的\n",
    "\n",
    "在搭建 LLM 应用的评估体系之前，我们都需要考虑清楚我们为什么需要做评估？\n",
    "\n",
    "这个“为什么”，绝不仅仅是形式上的流程要求，而是整个评估体系最核心的起点。如果没有这个起点，评估就会变成盲目地跑数据、堆指标，看似“工作很充分”，但实际上并不能解决真实问题。\n",
    "\n",
    "比如对于 ChatBot 类应用，我们评估的目的主要看以下几点：\n",
    "- 评估对话质量：自然度、连贯性、逻辑性等\n",
    "- 评估模型幻觉：输出内容是否是正确的，而不是一本正经的胡说八道\n",
    "- 评估任务完成度：是否能够正确回复用户提出的问题\n",
    "- 安全性与合规性：是否输出了不当内容（偏见、歧视、暴力等）\n",
    "\n",
    "总的来说，ChatBot 的评估目的是确保它能稳定、准确、安全、且符合目标地完成任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f34b78",
   "metadata": {},
   "source": [
    "### 2.2.2 准备测试的数据集（Dataset）\n",
    "\n",
    "确定了评估的目的后，这里我们就可以准备一些数据集来测试模型了。对于真实项目而言，准备 10 - 50 个样例就已经足够了。我们可以通过手动收集或者让 AI 通过格式化输出生成对应的数据集，甚至还可以在项目上线后根据真实的用例进一步补充。\n",
    "\n",
    "在我们这个例子中，我们就通过三条简单的常识性问题进行演示上传方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client = Client() # 创建 LangSmith 客户端\n",
    "dataset_name = \"Chatbot Evaluation Dataset\"\n",
    "datasets = list(client.list_datasets(dataset_name=dataset_name))\n",
    "if datasets:\n",
    "    dataset = datasets[0]\n",
    "    print(f\"Dataset 已存在，直接使用: {dataset.id}\")\n",
    "else:\n",
    "    dataset = client.create_dataset(dataset_name) # 创建一个数据集（如果已存在不会重复创建）\n",
    "    # 向其中加入几条简单测试数据\n",
    "    client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    examples=[{\"inputs\": {\"question\": \"你是谁？\"}, \"outputs\": {\"answer\": \"我是一个智能助手。\"}},\n",
    "        {\"inputs\": {\"question\": \"中国的首都是哪里？\"}, \"outputs\": {\"answer\": \"北京。\"}},\n",
    "        {\"inputs\": {\"question\": \"1+1等于几？\"}, \"outputs\": {\"answer\": \"2\"}}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351688d",
   "metadata": {},
   "source": [
    "### 2.2.3 搭建评估器（Evaluator）\n",
    "\n",
    "评估器的搭建其实是基于评估目的来展开的。通常来说在 LangSmith 里常见的有四种评估器可以使用：\n",
    "- LLM-as-Judge（大模型评分器）：让另一个大模型来判断表现是否好（最常见）\n",
    "- Code Evaluators（代码评估器）：通过硬规则判断的“程序化评估器”\n",
    "- Prebuilt Evaluators（预构建评估器）：LangSmith 内置的评估器\n",
    "- Composite Evaluators（组合评估器）：多评估器组合使用\n",
    "\n",
    "搭建评估器的方式其实很简单，本质上就是写一个函数，输入的内容包括问题、预设好的答案和实际模型输出的答案。输出就是对应的分数。目前最常见的方式就是使用另外一个大模型来作为判官来进行打分。\n",
    "\n",
    "假如是用大模型来进行审阅的话，我们需要把前面三个输入传入用户提示词中，并且也告诉其判卷的规则即可。\n",
    "\n",
    "然后假如模型返回的是“正确”，那么上传上去的结果就是 True，假如返回“不正确”的话，上传的结果就是 False 了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5832b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.wrappers import wrap_openai\n",
    "import os\n",
    "\n",
    "judge_client = wrap_openai(OpenAI(\n",
    "  api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "  base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\"\n",
    "))\n",
    "\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "  prompt = f\"\"\"\n",
    "问题：{inputs['question']}\n",
    "参考答案：{reference_outputs['answer']}\n",
    "模型回答：{outputs['response']}\n",
    "\n",
    "请判断模型回答是否正确，只回复“正确”或“不正确”。\n",
    "\"\"\"\n",
    "  resp = judge_client.chat.completions.create(\n",
    "    model=\"ernie-3.5-8k\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"你是一位专业中文评分老师。\"},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "  ).choices[0].message.content.strip()\n",
    "\n",
    "  return resp == \"正确\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db27859b",
   "metadata": {},
   "source": [
    "除此之外，我们还可以去自定义一些个性化的评估器，比如了解模型的简洁度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concision(outputs: dict, reference_outputs: dict) -> bool:\n",
    "  return int(len(outputs[\"response\"]) <= 2 * len(reference_outputs[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0f860",
   "metadata": {},
   "source": [
    "### 2.2.4 准备对话应用所需的代码\n",
    "\n",
    "设置好了评估器后，我们也要准备一下可运行的 ChatBot 代码。这里为了演示我们使用的就是前面 wrap_openai 时大模型调用的代码进行演示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "raw_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "  base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "traced_client = wrap_openai(raw_client)\n",
    "\n",
    "def my_chatbot(question: str) -> str:\n",
    "  resp = traced_client.chat.completions.create(\n",
    "    model=\"ernie-3.5-8k\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"你是一个简洁中文助手\"},\n",
    "      {\"role\": \"user\", \"content\": question},])\n",
    "  return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d95789",
   "metadata": {},
   "source": [
    "### 2.2.5 启动实验（Experiment）\n",
    "\n",
    "最后，就是把前面做好的数据集、评估器以及对话应用的代码进行组合，并开启实验。\n",
    "\n",
    "这里需要注意的是，LangSmith 内部有要求的输入输出格式，所以我们要单独设置一个 ls_target 函数完成转换。\n",
    "\n",
    "最后通过向 client.evaluate() 转换函数、数据集名称、评估器以及一个前缀 experiment_prefix（默认为 None）。这样就可以开启训练了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd462c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "\n",
    "# LangSmith 需要 inputs→outputs 格式\n",
    "def ls_target(inputs: dict) -> dict:\n",
    "  return {\"response\": my_chatbot(inputs[\"question\"])}\n",
    "\n",
    "dataset_name = \"Chatbot Evaluation Dataset\"\n",
    "\n",
    "# 运行 LangSmith 实验\n",
    "experiment = client.evaluate(\n",
    "  ls_target,\n",
    "  data=dataset_name,\n",
    "  evaluators=[correctness, concision],\n",
    "  experiment_prefix=\"ernie-chatbot-eval\"\n",
    ")\n",
    "\n",
    "print(\"评估完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4991e",
   "metadata": {},
   "source": [
    "### 2.2.6 更改提示词或模型再次启动实验（Experiment）\n",
    "\n",
    "对于单次的结果我们并不能有所对比找到最优解，我们可以尝试更新对话应用所需的代码，比如更换新的模型来重新进测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa3135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_chatbot(question: str) -> str:\n",
    "  resp = traced_client.chat.completions.create(\n",
    "    model=\"ernie-4.5-turbo-128k-preview\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"你是一个简洁中文助手\"},\n",
    "      {\"role\": \"user\", \"content\": question},])\n",
    "  return resp.choices[0].message.content\n",
    "\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "\n",
    "# LangSmith 需要 inputs→outputs 格式\n",
    "def ls_target(inputs: dict) -> dict:\n",
    "  return {\"response\": my_chatbot(inputs[\"question\"])}\n",
    "\n",
    "dataset_name = \"Chatbot Evaluation Dataset\"\n",
    "\n",
    "# 运行 LangSmith 实验\n",
    "experiment = client.evaluate(\n",
    "  ls_target,\n",
    "  data=dataset_name,\n",
    "  evaluators=[correctness, concision],\n",
    "  experiment_prefix=\"ernie-chatbot-eval\"\n",
    ")\n",
    "\n",
    "print(\"评估完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce160d7",
   "metadata": {},
   "source": [
    "## 2.3 评估 RAG 应用\n",
    "\n",
    "除了评估 ChatBot 以外，下面我们再来讲一下如何来评估 RAG 应用。\n",
    "\n",
    "对于 RAG 应用而言，除了评估模型回复的内容是否合理以外，其实还有一个很重要的评估内容是向量数据库里检索到的片段是否真的相关。\n",
    "\n",
    "下面我们就以一篇博客文章为例子看看如何对 RAG 应用进行评估吧！\n",
    "\n",
    "和开展的步骤类似，但是对于 RAG 应用而言，我们还需要添加生成向量数据库的步骤：\n",
    "- 明确评估的目的\n",
    "- 生成向量数据库并准备对话应用所需的代码\n",
    "- 根据评估目的准备测试的数据集（Dataset）\n",
    "- 根据评估目的搭建评估器（Evaluator）\n",
    "- 启动实验（Experiment）\n",
    "- 查看评估结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95076f79",
   "metadata": {},
   "source": [
    "### 2.3.1 明确评估的目的\n",
    "\n",
    "对于 RAG 应用而言，其主要的评估目的包括：\n",
    "- 确认答案是否正确（Correctness）：对比一下提问的问题和回复的答案，看是否两者有所偏差。\n",
    "- 检查回答是否切题、有帮助（Relevance）：对比一下模型回复的答案和数据集上的参考答案，看两者是否有很大的差异。\n",
    "- 检查回答是否“立足于文档”（Groundedness）：当我们从向量数据库里检索到了相关内容，但模型可能内部的知识和文档有冲突，所以这里就是检查模型的回复是否按照向量数据库里检索到的最新知识进行回复。\n",
    "- 检索的文档是否相关（Retrieval Relevance）：很多时候模型回复差的原因可能是检索不到有用的文档，所以提取出对应的文档内容以及提问的内容来进行对比分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6a1ab",
   "metadata": {},
   "source": [
    "### 2.3.2 生成向量数据库\n",
    "这里我们使用的就是 lilianweng 发布在 github 上的三篇 blog 作为原始的数据（与 ReAct Agent 主题相关），并且将这部分内容切分后存放到向量数据库中（InMemoryVectorStore）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "urls = [\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "  \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "  \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs = [d for batch in docs for d in batch]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=250,\n",
    "  chunk_overlap=0\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "  splits,\n",
    "  embedding=DashScopeEmbeddings(\n",
    "    dashscope_api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "    model=\"text-embedding-v1\"\n",
    "  )\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=6) # 找 6 篇相关的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bcb824",
   "metadata": {},
   "source": [
    "### 2.3.3 准备对话应用所需的代码\n",
    "\n",
    "然后我们可以顺手把对话的 RAG 应用也进行创建。运行文件后可以得到以下结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d5508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"ernie-3.5-8k\",\n",
    " openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    " base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "\n",
    "@traceable()\n",
    "def rag_bot(question: str):\n",
    "  docs = retriever.invoke(question)\n",
    "  context = \"\\n\".join(d.page_content for d in docs)\n",
    "  prompt = f\"\"\"\n",
    "你是一名智能问答助手。请严格基于下列提供的参考文档内容回答用户问题：\n",
    "====== 文档内容 ======\n",
    "{context}\n",
    "回答要求：\n",
    "- 必须基于文档内容回答，不允许凭空补充\n",
    "- 最多使用三句话\n",
    "- 若文档中没有相关信息，请直接回答“文档未包含该信息”\n",
    "\"\"\"\n",
    "  answer = llm.invoke([{\"role\": \"system\", \"content\": prompt},\n",
    "    {\"role\": \"user\", \"content\": question}])\n",
    "  return {\"answer\": answer.content,\n",
    "    \"documents\": docs}\n",
    "if __name__ == \"__main__\": # 手动测试\n",
    "  print(rag_bot(\"什么是 ReAct？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe46ee3",
   "metadata": {},
   "source": [
    "### 2.3.4 准备测试的数据集\n",
    "\n",
    "确定数据来源后，我们就可以进行数据集的创建，这里我们就通过三条数据进行演示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc5279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "\n",
    "examples = [\n",
    "  {\"inputs\": {\"question\": \"ReAct 智能体是如何使用自我反思（self-reflection）机制的？\"},\n",
    "  \"outputs\": {\"answer\": \"ReAct 将推理与行动结合，通过执行动作（如搜索信息）、观察反馈，再基于观察结果进行自我反思，从而优化后续推理步骤。\"}},\n",
    "  {\"inputs\": {\"question\": \"在进行 few-shot 提示时可能产生哪些偏差？\"},\n",
    "  \"outputs\": {\"answer\": \"常见偏差包括多数标签偏差、位置偏差（如最近示例影响更大）以及常见词偏差等。\"}},\n",
    "  {\"inputs\": {\"question\": \"常见的五种对抗性攻击有哪些？\"},\n",
    "  \"outputs\": {\"answer\": \"包括：输入词扰动、基于梯度的攻击、越狱提示（jailbreak）攻击、人类红队攻击以及模型红队攻击。\"}}\n",
    "]\n",
    "\n",
    "dataset_name = \"Lilian Weng 博客问答数据集\"\n",
    "if not client.has_dataset(dataset_name=dataset_name): # 如果不存在，就创建\n",
    "  dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "  client.create_examples(dataset_id=dataset.id, examples=examples)\n",
    "  print(\"Dataset created!\")\n",
    "else:\n",
    "  print(\"Dataset already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63640e14",
   "metadata": {},
   "source": [
    "### 2.3.5 搭建评估器\n",
    "根据评估目的，我们可以为该任务创建四个单独的评估器进行评估。\n",
    "\n",
    "这里我们主要是通过大模型对比来进行评分，因此我们需要先设置一个大模型进行评分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"ernie-4.5-turbo-128k-preview\",\n",
    "  openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "  base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a0f21c",
   "metadata": {},
   "source": [
    "第一个我们搭建的评估器是正确性 Correctness，其主要是通过对比模型回复的内容和数据集中的输入和输出来进行评估，因此设置的方式如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3941500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "class CorrectnessGrade(TypedDict):\n",
    "  explanation: Annotated[str, ..., \"请解释你的判断依据\"]\n",
    "  correct: Annotated[bool, ..., \"若学生回答与标准答案一致则为 True\"]\n",
    "\n",
    "correctness_llm = llm.with_structured_output(CorrectnessGrade, method=\"json_schema\", strict=True) # 格式化输出结果\n",
    "\n",
    "correctness_instructions = \"\"\"你将收到：问题（QUESTION）、标准答案（GROUND TRUTH ANSWER）、学生答案（STUDENT ANSWER）。\n",
    "你的任务：只评估事实正确性。并且最后输出布尔值 True 或 False。\n",
    "要求：\n",
    "- 如果学生答案与标准答案在事实内容上相符，即使有额外补充，也应视为正确。\n",
    "- 如果学生答案出现事实性错误或与标准答案冲突，应评为错误。\"\"\"\n",
    "\n",
    "def correctness(inputs, outputs, reference_outputs):\n",
    "  text = f\"\"\"问题：{inputs['question']}\n",
    "标准答案：{reference_outputs['answer']}\n",
    "学生答案：{outputs['answer']}\"\"\"\n",
    "  grade = correctness_llm.invoke([\n",
    "    {\"role\": \"system\", \"content\": correctness_instructions},\n",
    "    {\"role\": \"user\", \"content\": text}])\n",
    "  return grade[\"correct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03028045",
   "metadata": {},
   "source": [
    "第二个评估器是关于相关性 Relevance，主要是对比输入模型的问题和回复的答案看两者是否相关。由于格式化输出的设置，所以返回的值必须是布尔值，也就是 True 或 False："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e72003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevanceGrade(TypedDict):\n",
    "  explanation: Annotated[str, ...]\n",
    "  relevant: Annotated[bool, ...]\n",
    "\n",
    "relevance_instructions = \"\"\"\n",
    "你将收到：问题（QUESTION）和学生答案（STUDENT ANSWER）。\n",
    "请判断学生的回答是否与问题相关、是否有帮助。\n",
    "\"\"\"\n",
    "\n",
    "relevance_llm = llm.with_structured_output(\n",
    "  RelevanceGrade, method=\"json_schema\", strict=True\n",
    ")\n",
    "\n",
    "def relevance(inputs, outputs):\n",
    "  text = f\"问题：{inputs['question']}\\n学生答案：{outputs['answer']}\"\n",
    "  grade = relevance_llm.invoke([\n",
    "    {\"role\": \"system\", \"content\": relevance_instructions},\n",
    "    {\"role\": \"user\", \"content\": text}\n",
    "  ])\n",
    "  return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca803b64",
   "metadata": {},
   "source": [
    "第三个评估器是检查回答是否“立足于文档”（Groundedness），也就是通过搜索到的文档以及模型的回复来确定两者是否存在关联："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd214f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundedGrade(TypedDict):\n",
    "  explanation: Annotated[str, ...]\n",
    "  grounded: Annotated[bool, ...]\n",
    "\n",
    "grounded_instructions = \"\"\"\n",
    "你将收到：事实文档（FACTS）和学生答案（STUDENT ANSWER）。\n",
    "你的任务：判断学生的回答是否完全基于文档提供的事实，而非凭空杜撰。\n",
    "\"\"\"\n",
    "\n",
    "grounded_llm = llm.with_structured_output(\n",
    "  GroundedGrade, method=\"json_schema\", strict=True\n",
    ")\n",
    "\n",
    "def groundedness(inputs, outputs):\n",
    "  docs = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
    "  text = f\"事实文档（FACTS）：\\n{docs}\\n\\n学生答案（STUDENT ANSWER）：\\n{outputs['answer']}\"\n",
    "  grade = grounded_llm.invoke([\n",
    "    {\"role\": \"system\", \"content\": grounded_instructions},\n",
    "    {\"role\": \"user\", \"content\": text}\n",
    "  ])\n",
    "  return grade[\"grounded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a04c647",
   "metadata": {},
   "source": [
    "最后一个评估器是检查检索到的文档是否与问题相关（retrieval_relevance）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d444108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalRelGrade(TypedDict):\n",
    "  explanation: Annotated[str, ...]\n",
    "  relevant: Annotated[bool, ...]\n",
    "\n",
    "retrieval_instructions = \"\"\"\n",
    "你将收到：问题（QUESTION）与检索到的文档（RETRIEVED FACTS）。\n",
    "任务：判断这些文档是否包含回答问题所需的关键信息。\n",
    "\"\"\"\n",
    "\n",
    "retrieval_llm = llm.with_structured_output(\n",
    "  RetrievalRelGrade, method=\"json_schema\", strict=True\n",
    ")\n",
    "\n",
    "def retrieval_relevance(inputs, outputs):\n",
    "  docs = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
    "  text = f\"问题（QUESTION）：\\n{inputs['question']}\\n\\n检索文档（FACTS）：\\n{docs}\"\n",
    "  grade = retrieval_llm.invoke([\n",
    "    {\"role\": \"system\", \"content\": retrieval_instructions},\n",
    "    {\"role\": \"user\", \"content\": text}\n",
    "  ])\n",
    "  return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77f47f",
   "metadata": {},
   "source": [
    "### 2.3.6 启动试验\n",
    "在所有配件准备好后，我们就可以基于前面的内容开始进行测试了：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75582faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "\n",
    "# 与 create_rag_dataset.py 中保持一致\n",
    "dataset_name = \"Lilian Weng 博客问答数据集\"\n",
    "\n",
    "def target(inputs: dict) -> dict:\n",
    "  return rag_bot(inputs[\"question\"])\n",
    "\n",
    "experiment = client.evaluate(\n",
    "  target,      # 你的 RAG 系统（评估对象）\n",
    "  data=dataset_name, # 数据集名称（字符串即可）\n",
    "  evaluators=[    # 四类 RAG 自动评估维度\n",
    "    correctness,    # 正确性：回答是否与标准答案一致\n",
    "    groundedness,    # 依据性：回答是否基于检索文档\n",
    "    relevance,     # 相关性：回答对用户是否有帮助\n",
    "    retrieval_relevance # 检索相关性：检索到的文档是否与问题匹配\n",
    "  ],\n",
    "  experiment_prefix=\"rag-文档匹配评估\", # UI 中的实验名称前缀\n",
    "  metadata={\n",
    "    \"version\": \"ernie-rag-v1\", # 可用于标注你的系统版本\n",
    "    \"description\": \"ERNIE 模型 + 本地向量库的 RAG 系统评估实验\"\n",
    "  },\n",
    ")\n",
    "\n",
    "print(\"✅ RAG 自动评估已完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715eba3",
   "metadata": {},
   "source": [
    "### 2.3.7 查看评估结果\n",
    "\n",
    "在页面中就能看到完整的测试结果了。结果显示除了 Groundedness 以外，其他的结果都并不那么相关，所以我们可以对其进行优化并获取结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter_8_2025_12_15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
