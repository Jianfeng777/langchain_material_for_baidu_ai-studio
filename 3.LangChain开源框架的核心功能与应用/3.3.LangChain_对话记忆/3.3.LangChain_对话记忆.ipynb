{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ç¯å¢ƒé…ç½®\n",
    "\n",
    "## 1.1 python ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gradio==6.1.0 openai==2.11.0 dashscope==1.25.4 langchain-classic==1.0.0 langchain==1.1.3 langchain-community==0.4.1 langchain-openai==1.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 å¤§æ¨¡å‹å¯†é’¥å‡†å¤‡\n",
    "\n",
    "è¯·æ ¹æ®ç¬¬ä¸€ç« å†…å®¹è·å–ç›¸å…³å¹³å°çš„ API KEYï¼Œå¦‚è‹¥æœªåœ¨ç³»ç»Ÿå˜é‡ä¸­å¡«å…¥ï¼Œè¯·å°† API_KEY ä¿¡æ¯å†™å…¥ä»¥ä¸‹ä»£ç ï¼ˆè‹¥å·²è®¾ç½®è¯·å¿½ç•¥ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxx\"\n",
    "# os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-yyyyyyyy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LangChain é‡Œçš„è®°å¿†æœºåˆ¶ï¼ˆMemoryï¼‰\n",
    "\n",
    "åœ¨å¯¹è¯åœºæ™¯ä¸­ï¼Œè®°å¿†ï¼ˆMemoryï¼‰ æ˜¯ä¸€ä¸ªéå¸¸å…³é”®çš„èƒ½åŠ›ã€‚å®ƒè®©å¤§æ¨¡å‹èƒ½å¤Ÿâ€œè®°ä½â€ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œä»è€Œåœ¨åç»­å›ç­”ä¸­ä¿æŒä¸Šä¸‹æ–‡è¿è´¯ã€‚\n",
    "\n",
    "## 2.1 ä¸ºä»€ä¹ˆéœ€è¦è®°å¿†ï¼Ÿ\n",
    "\n",
    "å¤§æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯â€œæ— çŠ¶æ€â€çš„ï¼Œæ¯æ¬¡è°ƒç”¨æ—¶ï¼Œå®ƒåªæ ¹æ®è¾“å…¥ä¸Šä¸‹æ–‡ç”Ÿæˆè¾“å‡ºã€‚ä¸ºäº†è®©å¯¹è¯çœ‹èµ·æ¥â€œè¿ç»­â€ï¼Œæˆ‘ä»¬éœ€è¦é¢å¤–çš„æœºåˆ¶æ¥ä¿å­˜å†å²è®°å½•å¹¶åœ¨åç»­è°ƒç”¨ä¸­ä¼ é€’ï¼š\n",
    "\n",
    "* **ä¸Šä¸‹æ–‡è¿è´¯**ï¼šç”¨æˆ·å‰é¢æåˆ°çš„äººç‰©ã€åœ°ç‚¹ã€é—®é¢˜ï¼Œåœ¨åç»­å¯¹è¯é‡Œä¾ç„¶èƒ½è¢«ç†è§£ã€‚\n",
    "* **ä¸ªæ€§åŒ–è®°å¿†**ï¼šä¿å­˜ç”¨æˆ·çš„åå¥½æˆ–èƒŒæ™¯ä¿¡æ¯ï¼Œä»¥ä¾¿åšå‡ºä¸ªæ€§åŒ–å›ç­”ã€‚\n",
    "* **ä»»åŠ¡è·Ÿè¸ª**ï¼šåœ¨å¤šè½®å¯¹è¯ä¸­æŒç»­æ¨è¿›ä»»åŠ¡ï¼Œä¸éœ€è¦ç”¨æˆ·é‡å¤è¾“å…¥ã€‚\n",
    "\n",
    "\n",
    "æ¯”å¦‚ä¸‹é¢çš„ä¾‹å­ï¼Œæˆ‘ä»¬ä¸æ¨¡å‹è¿›è¡Œå¤šè½®çš„å¯¹è¯åï¼Œç”±äºæˆ‘ä»¬æ²¡æœ‰é…ç½®å’Œè®°å¿†ç›¸å…³çš„å†…å®¹ï¼Œå› æ­¤å…¶è¿˜æ˜¯è®°ä¸ä½æˆ‘ä»¬ç¬¬ä¸€æ¬¡è¯´è¿‡çš„åç§°ä¿¡æ¯ã€‚æ‰€ä»¥å¦‚æœæ²¡æœ‰ Memoryï¼Œå¤§æ¨¡å‹åªèƒ½çœ‹åˆ°æœ€åä¸€å¥ï¼Œæ— æ³•ç†è§£ä¸Šä¸‹æ–‡ã€‚è€Œå¦‚æœç”¨äº† Memoryï¼Œå¤§æ¨¡å‹å¯ä»¥çœ‹åˆ°å‰é¢æ‰€æœ‰å†…å®¹ï¼Œå°±å¯ä»¥åšå‡ºæ›´æœ‰é€»è¾‘çš„å›ç­”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  model=\"ernie-3.5-8k\",\n",
    "  openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "  base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\" \n",
    ")\n",
    "\n",
    "response1 = llm.invoke(\"æˆ‘å«æï¼Œä½ æ˜¯è°ï¼Ÿ\")\n",
    "response2 = llm.invoke(\"ä½ ä½åœ¨å“ªï¼Ÿ\")\n",
    "response3 = llm.invoke(\"è¿˜è®°å¾—æˆ‘å«å•¥ä¸\")\n",
    "\n",
    "print(response3.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é‚£ä¸‹é¢æˆ‘ä»¬å°±æ¥ä»‹ç»ä¸€ä¸‹ LangChain é‡Œå¸¸å¸¸èƒ½å¤Ÿç”¨åˆ°çš„è®°å¿†ç±»å‹å§ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 å…¨é‡è®°å¿†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 ConversationBufferMemoryï¼ˆæ—§ç‰ˆå…¨é‡è®°å¿†ï¼‰\n",
    "\n",
    "#### æ¦‚å¿µ\n",
    "\n",
    "æ—©æœŸçš„ LangChain ä½¿ç”¨ **ConversationBufferMemory** æ¥ä¿å­˜å¯¹è¯å†å²ã€‚å®ƒä¼šå®Œæ•´ä¿å­˜ç”¨æˆ·ä¸æ¨¡å‹ä¹‹é—´çš„æ‰€æœ‰å¯¹è¯å†å²ï¼ŒåŒ…æ‹¬æ¯ä¸€æ¬¡æé—®ï¼ˆuser messageï¼‰å’Œæ¯ä¸€æ¬¡å›ç­”ï¼ˆAI responseï¼‰ï¼Œå¹¶åœ¨æ¯æ¬¡è°ƒç”¨è¯­è¨€æ¨¡å‹æ—¶å°†è¿™äº›å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡é‡æ–°å‘é€ï¼Œç¡®ä¿æ¨¡å‹èƒ½â€œè®°ä½â€ä¹‹å‰å‘ç”Ÿè¿‡çš„æ‰€æœ‰äº¤æµã€‚\n",
    "\n",
    "\n",
    "å®ƒçš„åŸç†å¾ˆç®€å•ï¼š\n",
    "\n",
    "* æ¯æ¬¡å¯¹è¯ä¼šè¿½åŠ è¿›ä¸€ä¸ªâ€œå¯¹è¯ç¼“å­˜â€åˆ—è¡¨ã€‚\n",
    "* è°ƒç”¨æ¨¡å‹æ—¶ï¼ŒLangChain ä¼šæŠŠç¼“å­˜ä¸­çš„æ‰€æœ‰å†…å®¹æ‹¼æ¥èµ·æ¥æ”¾å…¥ promptã€‚\n",
    "* æ‰€ä»¥æ¨¡å‹æ¯æ¬¡å“åº”æ—¶ï¼Œéƒ½ä¼šæ‹¥æœ‰â€œå®Œæ•´ä¸Šä¸‹æ–‡â€ã€‚\n",
    "\n",
    "#### å­˜å‚¨æ ¼å¼\n",
    "\n",
    "ä¸€èˆ¬è€Œè¨€ï¼Œ **ConversationBufferMemory** ä¼šå°†æ‰€æœ‰çš„å¯¹è¯ä¿¡æ¯ä¿å­˜åœ¨ `memory.chat_memory.messages` ï¼Œè¿™æ ·ä¸€ä¸ªåˆ—è¡¨é‡Œé¢ã€‚é‡Œé¢åŒ…å«äº†æ‰€æœ‰å¯¹è¯çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ `HumanMessage` ã€ `AIMessage` ä»¥åŠ `SystemMessage` ç­‰ç­‰ã€‚æ¯”å¦‚åƒä¸‹é¢è¿™æ ·çš„ä¾‹å­ï¼š\n",
    "\n",
    "```python \n",
    "memory.chat_memory.messages = [\n",
    "  HumanMessage(content=\"ä½ å¥½\"),\n",
    "  AIMessage(content=\"ä½ å¥½ï¼æˆ‘å¯ä»¥å¸®ä½ ä»€ä¹ˆï¼Ÿ\"),\n",
    "  HumanMessage(content=\"1+1ç­‰äºå‡ \"),\n",
    "  AIMessage(content=\"ç­‰äº2\"),\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "#### ä»£ç ç¤ºä¾‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.chains import ConversationChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, \n",
    "  memory=memory,\n",
    "  verbose=True)\n",
    "  \n",
    "response_1 = conversation.predict(input=\"ä½ å¥½ï¼Œæˆ‘å«æå‰‘é”‹\")\n",
    "print(\"å¯¹è¯ 1:\", response_1) \n",
    "response_2 = conversation.predict(input=\"è¯·é—®1+1ç­‰äºå¤šå°‘?\")\n",
    "print(\"å¯¹è¯ 2:\", response_2)\n",
    "response_3 = conversation.predict(input=\"æˆ‘çš„åå­—æ˜¯?\")\n",
    "print(\"å¯¹è¯ 3:\", response_3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 RunnableWithMessageHistoryï¼ˆæ–°ç‰ˆå…¨é‡è®°å¿†ï¼‰\n",
    "\n",
    "#### æ¦‚å¿µ\n",
    "\n",
    "RunnableWithMessageHistory æ˜¯ LangChain æ–°ä¸€ä»£çš„å¯¹è¯å†å²ç®¡ç†å·¥å…·ã€‚è®©æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­è®°ä½ä¸Šä¸‹æ–‡ï¼Œè€Œä¸éœ€è¦åœ¨æ¯æ¬¡è°ƒç”¨æ—¶æ‰‹åŠ¨ä¼ å…¥æ‰€æœ‰å†å²ã€‚\n",
    "\n",
    "å…¶å¯¹æ¯” ConversationBufferMemory çš„ä¼˜åŠ¿åœ¨äºï¼š\n",
    "* æ”¯æŒå¤šä¼šè¯ç®¡ç†ï¼šé€šè¿‡ session_id ç®¡ç†ä¸åŒç”¨æˆ·æˆ–ä¸åŒä¼šè¯çš„å†å²ï¼Œäº’ä¸å¹²æ‰°ã€‚\n",
    "* è§£è€¦ï¼šå†å²è®°å½•å’Œé“¾é€»è¾‘åˆ†ç¦»ï¼Œä»»ä½• Runnable éƒ½èƒ½åŠ è®°å¿†ã€‚\n",
    "* å¯æ‰©å±•ï¼šå¯ä»¥æ›¿æ¢å­˜å‚¨å±‚ï¼Œæ¯”å¦‚å­˜æ•°æ®åº“ã€Redisï¼Œè€Œä¸æ˜¯å†…å­˜å˜é‡ã€‚\n",
    "\n",
    "#### ä»£ç ç¤ºä¾‹\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬åƒä¹‹å‰ä¸€æ ·å®šä¹‰å¥½æç¤ºè¯æ¨¡ç‰ˆã€æ¨¡å‹ä»¥åŠå°†è¿™ä¸¤è€…ç»„åˆåœ¨ä¸€èµ·çš„ **LCEL** é“¾ã€‚ä½†æ˜¯è¿™é‡Œå’Œæ™®é€šçš„æç¤ºè¯æ¨¡ç‰ˆä¸ä¸€æ ·çš„æ˜¯ï¼Œæˆ‘ä»¬è¦åœ¨è¿™ä¸ªä¸­é—´åŠ ä¸Šè®°å¿†è¿‡å¾€è®°å¿†éƒ¨åˆ†çš„å†…å®¹ï¼Œè¿™é‡Œç”¨çš„å°±æ˜¯ `MessagesPlaceholder()` æ–¹æ³•ï¼Œå¹¶ä¸”ç»‘å®šå¥½ä¼ å…¥çš„åç§°ã€‚\n",
    "\n",
    "ç„¶åå‘¢æˆ‘ä»¬è¿˜éœ€è¦è®¾ç½®å­˜å‚¨ä½ç½®ï¼ˆ`session_store`ï¼‰åŠè½½å…¥è®°å¿†çš„å‡½æ•°ï¼ˆ`get_session_history`ï¼‰ã€‚è¿™ä¸ªå‡½æ•°å…¶å®åšçš„äº‹æƒ…å°±æ˜¯åˆ¤å®šä¼ å…¥çš„ `session_id` æ˜¯å¦åœ¨å­˜å‚¨çš„å­—å…¸ `session_store` æ˜¯å¦å­˜åœ¨ï¼ˆè¿™ä¸ª `session_id` æ˜¯ç”¨äºåˆ¤å®šæ˜¯å¦æ˜¯åŒä¸€æ®µå¯¹è¯çš„å…³é”®å†…å®¹ï¼‰ï¼Œå‡å¦‚å­˜åœ¨é‚£å°±æŠŠè®°å¿†å¯¼å‡ºæ¥ä¼ ç»™æç¤ºè¯æ¨¡ç‰ˆä¸­çš„å ä½ç¬¦ `MessagesPlaceholder()` ï¼Œå‡å¦‚ä¸å­˜åœ¨çš„è¯å°±åœ¨å­˜å‚¨çš„å­—å…¸é‡Œæ–°å»ºä¸€æ¡è®°å½•ï¼Œä¹Ÿå°±æ˜¯ `{\"session_id\" :ChatMessageHistory()}` è¿™æ ·çš„æ ¼å¼ã€‚\n",
    "\n",
    "å†ç„¶åæˆ‘ä»¬å°±éœ€è¦é€šè¿‡ `RunnableWithMessageHistory` å°†å‰é¢å®šä¹‰å¥½çš„è¿™äº›å†…å®¹æ‹¼è£…èµ·æ¥ï¼ŒåŒ…æ‹¬éœ€è¦è¿›è¡Œè®°å¿†çš„é“¾ï¼ˆ`runnable`ï¼‰ã€ä¼ é€’è®°å¿†çš„æ–¹æ³•ï¼ˆ`get_session_histoy`ï¼‰ã€è¾“å…¥ä¿¡æ¯çš„å˜é‡åç§°ï¼ˆ`input_messages_key`ï¼‰ä»¥åŠä¼ å…¥å†å²è®°å½•çš„å˜é‡åç§°ï¼ˆ`history_messages_key`ï¼‰ã€‚è¿™æ ·ç»‘å®šè¿‡åï¼Œå°±èƒ½å¤Ÿå®ç°è®°å¿†çš„è‡ªåŠ¨å­˜å‚¨åŠè°ƒç”¨äº†ã€‚\n",
    "\n",
    "æœ€åæˆ‘ä»¬è¿˜éœ€è¦åšçš„æ˜¯è®¾ç½®ä¸€ä¸ªå•ç‹¬çš„ `session_id` ï¼Œä¹Ÿå°±æ˜¯è¿™é‡Œçš„ \"test_session1\" æ¥ä½œä¸ºè¿™æ®µå¯¹è¯ç‹¬ç‰¹çš„è®°å¿†æ ‡ç­¾ã€‚ç„¶åå°±å¯ä»¥æ­£å¸¸çš„ä¸å¤§æ¨¡å‹è¿›è¡Œå¯¹è¯æ²Ÿé€šäº†ã€‚ä½†æ˜¯è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¿™é‡Œè°ƒç”¨çš„å°±ä¸æ˜¯å‰é¢åˆ›å»ºçš„ LCEL é“¾äº†ï¼Œè€Œæ˜¯åŒ…è£¹äº† LCEL ä»¥åŠè®°å¿†çš„å†å²è®°å½•é“¾ `RunnableWithMessageHistory` ã€‚å¹¶ä¸”åœ¨è°ƒç”¨çš„æ—¶å€™æˆ‘ä»¬éœ€è¦å°† `session_id` ä½œä¸º config ä¼ å…¥ã€‚ä¼ å…¥åæ¯æ¬¡å½“æˆ‘ä»¬è°ƒç”¨å®Œè·å–å›å¤ï¼ŒLangChain å°±ä¼šæ ¹æ®æˆ‘ä»¬ `session_id` æ‰¾åˆ°å¯¹åº”çš„å†å²è®°å½•ï¼Œå¹¶æŠŠä¿¡æ¯ä¼ å…¥è¿›å»ã€‚åç»­æˆ‘ä»¬å¯ä»¥é€šè¿‡æ‰“å°çš„æ–¹å¼æ¥å¯¹è¿™éƒ¨åˆ†å†å²çš„ä¿¡æ¯è¿›è¡Œäº†è§£ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "    (\"human\", \"{input}\")])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "session_store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    return session_store[session_id]\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    runnable=chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "session_id = \"test_session1\"\n",
    "\n",
    "# ç¬¬ä¸€æ¬¡è°ƒç”¨\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½, 50å­—ä»‹ç»ä¸€ä¸‹\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"ç¬¬ä¸€æ¬¡å›ç­”ï¼š\", response1.content)\n",
    "\n",
    "# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼Œå¸¦å†å²\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"å®ƒæœ‰å“ªäº›åº”ç”¨, 50å­—ä»‹ç»ä¸€ä¸‹\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"ç¬¬äºŒæ¬¡å›ç­”ï¼š\", response2.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 è®°å¿†èŠ‚çœæ–¹æ³•\n",
    "\n",
    "### 2.3.1 ä½¿ç”¨åŸå› \n",
    "\n",
    "ä½¿ç”¨å…¨é‡è®°å¿†è™½ç„¶å¯ä»¥å®Œæ•´ä¿ç•™å¯¹è¯å†å²ï¼Œä½†å®ƒä¹Ÿæœ‰ä¸€äº›æ˜¾è‘—çš„ç¼ºç‚¹æˆ–æ½œåœ¨é—®é¢˜ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ç‚¹ï¼š\n",
    "\n",
    "* ä¸Šä¸‹æ–‡é™åˆ¶ï¼šå¤§æ¨¡å‹æœ‰æœ€å¤§ token é™åˆ¶ï¼ˆä¾‹å¦‚ 8kã€32kï¼‰ï¼Œè¶…è¿‡å°±ä¼šæŠ¥é”™ã€‚\n",
    "* ä½¿ç”¨æˆæœ¬å¢åŠ ï¼šè¾“å…¥ token è¶Šå¤šï¼Œè°ƒç”¨è´¹ç”¨å’Œå“åº”æ—¶é—´è¶Šé«˜ã€‚\n",
    "* å¤§æ¨¡å‹æ— æ³•å‡†ç¡®ç†è§£ä¿¡æ¯ï¼šå†—é•¿çš„ä¸Šä¸‹æ–‡é‡Œå¯èƒ½æºæ‚å¤§é‡æ— å…³ä¿¡æ¯ï¼Œåè€Œå½±å“æ¨¡å‹å‡†ç¡®æ€§ã€‚\n",
    "\n",
    "å› æ­¤ï¼ŒLangChain æä¾›äº†å‡ ç§å¸¸è§çš„â€œèŠ‚çœè®°å¿†â€æ–¹æ³•ï¼Œæ¯”å¦‚ï¼š\n",
    "* ConversationBufferWindowMemoryï¼ˆä¿å­˜å‰ n è½®å¯¹è¯ï¼‰\n",
    "* ConversationTokenBufferMemoryï¼ˆä¿å­˜ n ä¸ª token çš„å¯¹è¯å†…å®¹ï¼‰\n",
    "* ConversationSummaryMemoryï¼ˆæ€»ç»“åä¿å­˜ï¼‰\n",
    "\n",
    "ä½†æ˜¯ç”±äºè¿™äº›æ–¹æ³•éƒ½å³å°†åœ¨ LangChain çš„ 1.0 ç‰ˆæœ¬æ—¶è¢«ä¸¢å¼ƒï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦é€šè¿‡ RunnableWithMessageHistory + trim_messages ï¼ˆè®°å¿†å‰ªè¾‘ï¼‰ çš„æ–¹æ³•é‡æ–°çš„å®ç°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 trim_messagesï¼ˆè®°å¿†å‰ªè¾‘ï¼‰\n",
    "\n",
    "#### æ¦‚å¿µ\n",
    "\n",
    "åœ¨å¯¹è¯è®°å¿†ç®¡ç†ä¸­ï¼ŒLangChain æä¾›äº†ä¸€ä¸ªéå¸¸å®ç”¨çš„å·¥å…·å‡½æ•° â€”â€” **`trim_messages`**ã€‚å®ƒçš„ä½œç”¨æ˜¯å¯¹å·²æœ‰çš„å¯¹è¯å†å²è¿›è¡Œâ€œå‰ªè¾‘â€ï¼Œä»¥ä¾¿æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé¿å…è¶…è¿‡æ¨¡å‹çš„æœ€å¤§ token é™åˆ¶ã€‚å¹¶ä¸”ï¼Œé€šè¿‡çµæ´»çš„å‚æ•°è®¾ç½®ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š\n",
    "\n",
    "* æ§åˆ¶ä¸Šä¸‹æ–‡çš„æœ€å¤§é•¿åº¦\n",
    "* ä¿è¯å…³é”®æ¶ˆæ¯ï¼ˆå¦‚ system æŒ‡ä»¤ï¼‰ä¸ä¸¢å¤±\n",
    "* æ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„è£å‰ªç­–ç•¥\n",
    "\n",
    "å®ƒé€šå¸¸ä¸ **RunnableWithMessageHistory** ç»“åˆä½¿ç”¨ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„å¯¹è¯è®°å¿†ç®¡ç†è§£å†³æ–¹æ¡ˆã€‚\n",
    "\n",
    "#### åŸºæœ¬ç”¨æ³•\n",
    "\n",
    "`trim_messages` æ¥æ”¶ä¸€ç»„æ¶ˆæ¯ï¼ˆMessage åˆ—è¡¨ï¼‰ï¼Œä»¥åŠä¸€äº›è£å‰ªå‚æ•°ï¼Œè¿”å›è£å‰ªåçš„æ¶ˆæ¯åˆ—è¡¨ã€‚\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "trimmed = trim_messages(\n",
    "    messages=history.messages,\n",
    "    token_counter=lambda msgs: sum(len(str(m.content)) for m in msgs),\n",
    "    max_tokens=100,\n",
    "    start_on=\"human\",   # ä» human æ¶ˆæ¯å¼€å§‹æˆªæ–­\n",
    "    include_system=True, # æ˜¯å¦ä¿ç•™ system æç¤º\n",
    "    strategy=\"last\",    # è£å‰ªç­–ç•¥\n",
    "    allow_partial=True   # æ˜¯å¦å…è®¸éƒ¨åˆ†ä¿ç•™\n",
    ")\n",
    "```\n",
    "\n",
    "#### å¸¸è§å‚æ•°è¯´æ˜\n",
    "\n",
    "* **messages**ï¼šéœ€è¦è£å‰ªçš„æ¶ˆæ¯å†å²ã€‚\n",
    "* **token_counter**ï¼šè‡ªå®šä¹‰ token è®¡æ•°å‡½æ•°ï¼ˆä¾‹å¦‚æŒ‰å­—ç¬¦é•¿åº¦æˆ–çœŸå® token æ•°ï¼‰ã€‚\n",
    "* **max_tokens**ï¼šä¸Šä¸‹æ–‡å…è®¸çš„æœ€å¤§ token æ•°ã€‚\n",
    "* **start_on**ï¼šæŒ‡å®šä»å“ªç§è§’è‰²çš„æ¶ˆæ¯å¼€å§‹è£å‰ªï¼ˆå¦‚ human / aiï¼‰ã€‚\n",
    "* **include_system**ï¼šæ˜¯å¦å§‹ç»ˆä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼ˆsystem roleï¼‰ã€‚\n",
    "* **strategy**ï¼šè£å‰ªç­–ç•¥ï¼Œä¾‹å¦‚ `last` è¡¨ç¤ºä¿ç•™æœ€æ–°æ¶ˆæ¯ã€‚\n",
    "* **allow_partial**ï¼šæ˜¯å¦å…è®¸åªä¿ç•™æ¶ˆæ¯çš„ä¸€éƒ¨åˆ†ã€‚\n",
    "\n",
    "#### è£å‰ªç­–ç•¥\n",
    "\n",
    "1. **ä¿ç•™æœ€æ–°ï¼ˆlastï¼‰**ï¼šåªä¿ç•™æœ€è¿‘çš„è‹¥å¹²æ¶ˆæ¯ï¼Œæœ€å¸¸ç”¨ã€‚\n",
    "2. **ä¿ç•™æœ€æ—©ï¼ˆfirstï¼‰**ï¼šä¿ç•™å¼€å¤´çš„ä¸€äº›ä¸Šä¸‹æ–‡ï¼ˆè¾ƒå°‘ä½¿ç”¨ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 ConversationBufferWindowMemoryï¼ˆä¿å­˜å‰ n è½®å¯¹è¯ï¼‰\n",
    "\n",
    "#### æ¦‚å¿µ\n",
    "\n",
    "ConversationBufferWindowMemory æ˜¯ LangChain æä¾›çš„ä¸€ç§â€œæœ‰é™é•¿åº¦çš„å¯¹è¯è®°å¿†æ–¹å¼â€ï¼Œå®ƒä¸ä¼šä¿ç•™å®Œæ•´å¯¹è¯å†å²ï¼Œè€Œæ˜¯ä»…ä¿ç•™æœ€è¿‘ N è½®æ¶ˆæ¯ï¼ˆç”±ç”¨æˆ·è®¾å®šï¼‰ã€‚\n",
    "\n",
    "#### å·¥ä½œæœºåˆ¶\n",
    "\n",
    "* æ¯è½®å¯¹è¯ï¼ˆç”¨æˆ·è¾“å…¥ + æ¨¡å‹è¾“å‡ºï¼‰ä¼šè¢«è®°å½•ä¸‹æ¥ã€‚\n",
    "* ç³»ç»Ÿä»…ä¿ç•™æœ€è¿‘è®¾å®šçš„ k è½®å¯¹è¯ï¼Œå…¶ä½™éƒ¨åˆ†ä¼šè¢«â€œé—å¿˜â€ã€‚\n",
    "* æ¯æ¬¡è°ƒç”¨æ¨¡å‹æ—¶ï¼Œä»…å‘é€è¿™ N è½®å†…å®¹ä½œä¸º prompt ä¸Šä¸‹æ–‡ã€‚\n",
    "\n",
    "#### ä»£ç ç¤ºä¾‹\n",
    "\n",
    "é‚£å¯¹äºè¦ä¿ç•™å‰å‰ n è½®å¯¹è¯çš„è®°å¿†è€Œè¨€ï¼Œæˆ‘ä»¬è®°å¿†å‰ªè£çš„æ–¹å¼å°±æ˜¯å…ˆå»æ ¹æ® session_id è·å–å¯¹åº”çš„å®Œæ•´è®°å¿†ä¿¡æ¯ï¼Œç„¶åä»é‡Œé¢æå–å‡ºå¯¹åº”çš„å†…å®¹ã€‚é‚£è£å‰ªçš„è¯ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯å»çœ‹è¿™é‡Œé¢æœ‰å¤šå°‘æ¡ä¿¡æ¯ï¼ˆé€šè¿‡ `len()` çš„æ–¹å¼å»è®¡ç®—ï¼‰ï¼Œç„¶ååªç•™ä¸‹æœ€è¿‘å‘ç”Ÿçš„ 2 æ¡èŠå¤©è®°å½•å³å¯ï¼ˆä¸€æ¡ AIMessage å’Œ ä¸€æ¡ HumanMessageï¼‰ã€‚é‚£æ ¹æ®è¿™ä¸ªæ–¹æ³•æˆ‘ä»¬è®¾è®¡äº† `trim_by_conversation_pairs()` è¿™ä¸ªå‡½æ•°ï¼š\n",
    "\n",
    "```python\n",
    "def trim_by_conversation_pairs(history, max_pairs=1, include_system=True):\n",
    "    \"\"\"\n",
    "    æŒ‰å¯¹è¯æ®µæ•°è£å‰ªå†å²ï¼Œæ¯æ®µ = Human + AI ä¸¤æ¡æ¶ˆæ¯\n",
    "    é»˜è®¤åªä¿ç•™æœ€è¿‘ max_pairs æ®µ\n",
    "    \"\"\"\n",
    "    trimmed = trim_messages(\n",
    "        history.messages, # ä»ä¿å­˜çš„å­—å…¸é‡Œè·å–ä¿¡æ¯åˆ—è¡¨\n",
    "        token_counter=lambda msgs: len(msgs),  # æŒ‰æ¶ˆæ¯æ¡æ•°ç»Ÿè®¡ï¼ˆåˆ—è¡¨é‡Œæœ‰å¤šå°‘å†…å®¹ï¼‰\n",
    "        max_tokens=max_pairs,  # ä¿ç•™ max_pairs æ®µ\n",
    "        start_on=\"human\",  # ç¡®ä¿æ¯æ¬¡ä¼ å…¥æ¨¡å‹çš„å¯¹è¯å†å²éƒ½ä» HumanMessage å¼€å§‹  \n",
    "        include_system=include_system, # ä¿ç•™ç³»ç»Ÿæç¤ºè¯ä¿¡æ¯\n",
    "        strategy=\"last\",  # ä¿ç•™æœ€æ–°çš„å†…å®¹\n",
    "    )\n",
    "    history.messages[:] = trimmed  # å°†å‰ªè£å¥½çš„ä¿¡æ¯æ›´æ–°\n",
    "```\n",
    "\n",
    "æˆ‘ä»¬åªéœ€è¦ä¼ å…¥å¯¹åº”çš„å†å²ä¿¡æ¯ï¼Œç„¶åå°±èƒ½å¤Ÿåœ¨ä¼ å…¥åˆ°æç¤ºè¯æ¨¡ç‰ˆä¸­çš„è®°å¿†å˜é‡å‰ï¼Œå°†è®°å¿†è¿›è¡Œåˆ å‡ï¼Œä»è€ŒèŠ‚çœå…¶ä¸­æ‰€èŠ±è´¹ token çš„æ•°é‡ã€‚æœ€åæˆ‘ä»¬è®¾ç½®ä¸€ä¸ªæ–°çš„ session_id ï¼Œå¹¶æŠŠè®°å½•æ‰“å°å‡ºæ¥ã€‚é‚£è¿™æ ·å°±èƒ½çœ‹åˆ°åªä¿ç•™äº†æœ€è¿‘çš„ä¸€æ¡æ¶ˆæ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import trim_messages\n",
    "import os\n",
    "\n",
    "def trim_by_conversation_pairs(history, max_pairs=1, include_system=True):\n",
    "    \"\"\"\n",
    "    æŒ‰å¯¹è¯æ®µæ•°è£å‰ªå†å²ï¼Œæ¯æ®µ = Human + AI ä¸¤æ¡æ¶ˆæ¯\n",
    "    é»˜è®¤åªä¿ç•™æœ€è¿‘ max_pairs æ®µ\n",
    "    \"\"\"\n",
    "    trimmed = trim_messages(\n",
    "        history.messages, # ä»ä¿å­˜çš„å­—å…¸é‡Œè·å–ä¿¡æ¯åˆ—è¡¨\n",
    "        token_counter=lambda msgs: len(msgs),  # æŒ‰æ¶ˆæ¯æ¡æ•°ç»Ÿè®¡ï¼ˆåˆ—è¡¨é‡Œæœ‰å¤šå°‘å†…å®¹ï¼‰\n",
    "        max_tokens=max_pairs,  # ä¿ç•™ max_pairs æ®µ\n",
    "        start_on=\"human\",  # ç¡®ä¿æ¯æ¬¡ä¼ å…¥æ¨¡å‹çš„å¯¹è¯å†å²éƒ½ä» HumanMessage å¼€å§‹  \n",
    "        include_system=include_system, # ä¿ç•™ç³»ç»Ÿæç¤ºè¯ä¿¡æ¯\n",
    "        strategy=\"last\",  # ä¿ç•™æœ€æ–°çš„å†…å®¹\n",
    "    )\n",
    "    history.messages[:] = trimmed  # å°†å‰ªè£å¥½çš„ä¿¡æ¯æ›´æ–°\n",
    "\n",
    "session_store = {}\n",
    "def get_session_history(session_id, max_pairs=1):\n",
    "    \"\"\"æ¯æ¬¡è·å–å†å²æ—¶è‡ªåŠ¨è£å‰ªï¼Œä¿è¯ä¸ä¼šæ— é™å¢é•¿\"\"\"\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    history = session_store[session_id]\n",
    "    trim_by_conversation_pairs(history, max_pairs=max_pairs)  # æŒ‰å¯¹è¯æ®µæ•°è£å‰ª\n",
    "    return history\n",
    "\n",
    "llm = ChatOpenAI(model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful assistant.\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "  (\"human\", \"{input}\")])\n",
    "chain = prompt | llm\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "  runnable=chain,\n",
    "  get_session_history=get_session_history,\n",
    "  input_messages_key=\"input\",\n",
    "  history_messages_key=\"chat_history\")\n",
    "\n",
    "session_id = \"test_session2\"\n",
    "\n",
    "# ç¬¬ä¸€æ¬¡è°ƒç”¨\n",
    "response1 = chain_with_history.invoke(\n",
    "  {\"input\": \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½, 50å­—ä»‹ç»ä¸€ä¸‹\"},\n",
    "  config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"ç¬¬ä¸€æ¬¡å›ç­”ï¼š\", response1.content)\n",
    "\n",
    "# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼Œå¸¦å†å²\n",
    "response2 = chain_with_history.invoke(\n",
    "  {\"input\": \"å®ƒæœ‰å“ªäº›åº”ç”¨, 50å­—ä»‹ç»ä¸€ä¸‹\"},\n",
    "  config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"ç¬¬äºŒæ¬¡å›ç­”ï¼š\", response2.content)\n",
    "\n",
    "# æ‰“å°è£å‰ªåçš„å†å²\n",
    "print(\"\\n=== è£å‰ªåçš„ä¼šè¯å†å²ï¼ˆåªä¿ç•™æœ€è¿‘ä¸€è½®ï¼‰ ===\")\n",
    "for i, msg in enumerate(session_store[session_id].messages, 1):\n",
    "    print(i, msg.type, msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 ConversationTokenBufferMemoryï¼ˆåŸºäº Token çš„è®°å¿†è£å‰ªï¼‰\n",
    "\n",
    "é™¤äº†â€œä¿ç•™å‰ n è½®å¯¹è¯â€è¿™ç§ç›¸å¯¹å®½æ¾çš„æ–¹å¼å¤–ï¼Œè¿˜æœ‰ä¸€ç§æ›´ä¸¥æ ¼çš„èŠ‚çœèµ„æºæ–¹æ³• â€”â€” **åŸºäº Token çš„è®°å¿†è£å‰ª**ã€‚è¿™ç§æ–¹æ³•ä¼šé™å®šå¯¹è¯å†å²æœ€å¤šåªèƒ½å ç”¨å›ºå®šæ•°é‡çš„ **token**ï¼Œä¸€æ—¦è¶…è¿‡ï¼Œå°±ä¼šè‡ªåŠ¨ä¸¢å¼ƒæœ€æ—©çš„å†…å®¹ï¼Œä»è€Œä¿è¯ä¸Šä¸‹æ–‡å§‹ç»ˆå¤„äºå¯æ§èŒƒå›´å†…ã€‚\n",
    "\n",
    "è¿™ç§æœºåˆ¶è¿‡å»åœ¨ LangChain ä¸­å¯¹åº”çš„å®ç°å°±æ˜¯ **ConversationTokenBufferMemory**ã€‚å®ƒçš„ç‰¹ç‚¹æ˜¯ï¼š\n",
    "\n",
    "* ä¸å…³å¿ƒâ€œå¯¹è¯è½®æ•°â€ï¼Œåªå…³å¿ƒ token æ€»é‡ã€‚\n",
    "* å½“å¯¹è¯é€æ¸å˜é•¿æ—¶ï¼Œä¼šä¼˜å…ˆä¿ç•™æœ€è¿‘çš„å†…å®¹ï¼ŒæŠŠæ—§æ¶ˆæ¯é€æ­¥ä¸¢å¼ƒã€‚\n",
    "* æ›´åŠ é€‚åˆå¯¹ **ä¸Šä¸‹æ–‡é•¿åº¦** å’Œ **è®¡ç®—æˆæœ¬** æœ‰ä¸¥æ ¼é™åˆ¶çš„åœºæ™¯ã€‚\n",
    "\n",
    "#### Token çš„æ¦‚å¿µ\n",
    "\n",
    "è¦ç†è§£è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå¼„æ¸…æ¥š **token** æ˜¯ä»€ä¹ˆã€‚ç®€å•æ¥è¯´ï¼š\n",
    "\n",
    "* **Token â‰  å­—ç¬¦**ï¼šåœ¨ NLP æ¨¡å‹é‡Œï¼Œä¸€ä¸ª token å¹¶ä¸æ˜¯å•ä¸ªæ±‰å­—æˆ–å­—æ¯ï¼Œè€Œæ˜¯æ¨¡å‹åˆ†è¯åçš„æœ€å°å•ä½ã€‚\n",
    "* **ä¸¾ä¾‹**ï¼š\n",
    "\n",
    "  * è‹±æ–‡å•è¯ *\"apple\"* å¯èƒ½å°±æ˜¯ 1 ä¸ª tokenã€‚\n",
    "  * ä¸­æ–‡â€œè‹¹æœâ€é€šå¸¸ä¼šè¢«æ‹†åˆ†ä¸º 1~2 ä¸ª tokenã€‚\n",
    "  * è¾ƒé•¿çš„è‹±æ–‡å•è¯å¯èƒ½ä¼šè¢«æ‹†æˆå¤šä¸ª tokenã€‚\n",
    "* **æ„ä¹‰**ï¼štoken æ˜¯æ¨¡å‹å¤„ç†çš„æœ€åŸºæœ¬å•å…ƒï¼Œæ¨¡å‹çš„ä¸Šä¸‹æ–‡é™åˆ¶ã€è®¡ç®—å¼€é”€ã€è°ƒç”¨è´¹ç”¨éƒ½ç›´æ¥å’Œ token æ•°é‡æŒ‚é’©ã€‚\n",
    "\n",
    "å› æ­¤ï¼ŒConversationTokenBufferMemory çš„æœ¬è´¨ï¼Œå°±æ˜¯é€šè¿‡ä¸¥æ ¼é™åˆ¶ **token æ•°é‡**ï¼Œæ¥æ§åˆ¶ä¸Šä¸‹æ–‡å¤§å°ï¼Œé¿å…è¶…å‡ºæ¨¡å‹çš„å¤„ç†ä¸Šé™ã€‚\n",
    "\n",
    "\n",
    "#### ç»“åˆ RunnableMemory ä¸ trim_messages çš„å®ç°\n",
    "\n",
    "åœ¨æ–°ç‰ˆ LCEL æ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ **RunnableMemory** æ­é… `trim_messages` æ¥å®ç°ç±»ä¼¼çš„åŠŸèƒ½ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import trim_messages\n",
    "import os\n",
    "\n",
    "# å®šä¹‰ token è®¡ç®—æ–¹æ³•ï¼ˆç®€å•è®¤å®šä¸€ä¸ªå­—ç¬¦ä¸º1ä¸ª tokenï¼‰\n",
    "def count_tokens_by_len(msgs):\n",
    "    return sum(len(str(m.content)) for m in msgs)\n",
    "\n",
    "def trim_history_inplace(history, max_tokens=60):\n",
    "    trimmed = trim_messages(\n",
    "        history.messages, # ä¼ å…¥å†å²ä¿¡æ¯\n",
    "        token_counter=count_tokens_by_len, # æŒ‰ä¸€ä¸ªå­—ç¬¦çš„æ–¹å¼è®¡ç®—æ¯ä¸€æ®µå†…å®¹çš„ token\n",
    "        max_tokens=max_tokens, # ä¼ å…¥çš„æœ€å¤§ token æ•°é‡\n",
    "        start_on=\"human\", # ç¡®ä¿æ¯æ¬¡ä¼ å…¥æ¨¡å‹çš„å¯¹è¯å†å²éƒ½ä» HumanMessage å¼€å§‹\n",
    "        include_system=True, # åŒ…å«ç³»ç»Ÿæç¤ºè¯\n",
    "        strategy=\"last\",) # ä¿ç•™æœ€æ–°çš„å†…å®¹\n",
    "    history.messages[:] = trimmed  # åŸåœ°æ›´æ–°\n",
    "\n",
    "llm = ChatOpenAI(model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful assistant.\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "  (\"human\", \"{input}\")])\n",
    "chain = prompt | llm\n",
    "\n",
    "session_store = {}\n",
    "\n",
    "def get_session_history(session_id, max_tokens=60):\n",
    "  if session_id not in session_store:\n",
    "    session_store[session_id] = ChatMessageHistory()\n",
    "  history = session_store[session_id]\n",
    "  trim_history_inplace(history, max_tokens=max_tokens) # æ¯æ¬¡å–ä¹‹å‰è‡ªåŠ¨è£å‰ª\n",
    "  return history\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "  runnable=chain,\n",
    "  get_session_history=get_session_history,\n",
    "  input_messages_key=\"input\",\n",
    "  history_messages_key=\"chat_history\")\n",
    "\n",
    "session_id = \"test_session3\"\n",
    "\n",
    "response1 = chain_with_history.invoke(\n",
    "  {\"input\": \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½, 50å­—ä»‹ç»ä¸€ä¸‹\"},\n",
    "  config={\"configurable\": {\"session_id\": session_id}})\n",
    "print(\"ç¬¬ä¸€æ¬¡å›ç­”ï¼š\", response1.content)\n",
    "\n",
    "response2 = chain_with_history.invoke(\n",
    "  {\"input\": \"å®ƒæœ‰å“ªäº›åº”ç”¨, 100å­—ä»‹ç»ä¸€ä¸‹\"},\n",
    "  config={\"configurable\": {\"session_id\": session_id}})\n",
    "print(\"ç¬¬äºŒæ¬¡å›ç­”ï¼š\", response2.content)\n",
    "\n",
    "# æ‰“å°è£å‰ªåçš„å†å²\n",
    "print(\"\\n=== è£å‰ªåçš„ä¼šè¯å†å²ï¼ˆåªä¿ç•™æœ€æ–°çš„ n ä¸ª tokenï¼‰ ===\")\n",
    "for i, msg in enumerate(session_store[session_id].messages, 1):\n",
    "  print(i, msg.type, msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ€è·¯è§£æ\n",
    "\n",
    "* è¿™é‡Œæˆ‘ä»¬è‡ªå®šä¹‰äº† `count_tokens_by_len` æ¥ä¼°ç®— token æ•°é‡ï¼ˆè™½ç„¶å’ŒçœŸå®åˆ†è¯ç•¥æœ‰å·®è·ï¼Œä½†æ˜¯å¯ä»¥éå¸¸ç›´è§‚çš„è¿›è¡Œå±•ç¤ºï¼‰ã€‚\n",
    "* `trim_history_inplace` æ¯æ¬¡éƒ½ä¼šæ£€æŸ¥å½“å‰çš„å¯¹è¯å†å²ï¼Œå¹¶åœ¨å¿…è¦æ—¶æ‰§è¡Œâ€œåŸåœ°è£å‰ªâ€ã€‚\n",
    "* è¿™ç§å®ç°æ–¹å¼èƒ½ä¿è¯ **ä¸Šä¸‹æ–‡ä¸ä¼šæ— é™è†¨èƒ€**ï¼Œè€Œä¸”å¯ä»¥çµæ´»è°ƒæ•´ `max_tokens` çš„é˜ˆå€¼ã€‚\n",
    "\n",
    "ç»“åˆ **RunnableWithMessageHistory** ä½¿ç”¨æ—¶ï¼Œè¿™æ ·çš„å‰ªè£é€»è¾‘å°±èƒ½åœ¨æ¯æ¬¡è°ƒç”¨å‰è‡ªåŠ¨ç”Ÿæ•ˆï¼Œä»è€Œç¡®ä¿ä¸Šä¸‹æ–‡å§‹ç»ˆä¿æŒåœ¨å®‰å…¨èŒƒå›´ä¹‹å†…ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 ConversationSummaryMemoryï¼ˆæ€»ç»“åä¿å­˜ï¼‰\n",
    "\n",
    "é™¤äº†â€œä¿ç•™æœ€è¿‘å‡ è½®â€æˆ–â€œæŒ‰ token æ•°é‡è£å‰ªâ€ä»¥å¤–ï¼Œè¿˜æœ‰ä¸€ç§æ›´æ™ºèƒ½çš„è®°å¿†æ–¹å¼ â€”â€” **ConversationSummaryMemory**ã€‚å®ƒçš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼š\n",
    "\n",
    "* å½“å¯¹è¯é€æ¸å˜é•¿æ—¶ï¼Œä¸æ˜¯ç®€å•åœ°ä¸¢å¼ƒæ—§å†…å®¹ï¼›\n",
    "* è€Œæ˜¯é€šè¿‡ **å¤§æ¨¡å‹ç”Ÿæˆæ‘˜è¦** çš„æ–¹å¼ï¼ŒæŠŠå†å²ä¿¡æ¯å‹ç¼©æˆç®€çŸ­æ¦‚è¿°ï¼Œç„¶åå†ä¿å­˜ä¸‹æ¥ã€‚\n",
    "\n",
    "è¿™ç§æ–¹å¼çš„ä¼˜åŠ¿åœ¨äºï¼š\n",
    "\n",
    "* **ä¿¡æ¯ä¿ç•™æ›´å®Œæ•´**ï¼šå³ä½¿ä¸¢å¼ƒäº†åŸå§‹æ¶ˆæ¯ï¼Œé‡è¦çš„äº‹å®ä¸ä¸Šä¸‹æ–‡ä¾ç„¶èƒ½é€šè¿‡æ‘˜è¦ç»§ç»­å­˜åœ¨ã€‚\n",
    "* **ä¸Šä¸‹æ–‡æ›´ç´§å‡‘**ï¼šæ‘˜è¦æ¯”åŸæ–‡çŸ­å¾—å¤šï¼Œå¤§å¹…å‡å°‘ token å ç”¨ã€‚\n",
    "* **é€»è¾‘æ›´æ¸…æ™°**ï¼šé€šè¿‡æ¨¡å‹ç”Ÿæˆçš„æ€»ç»“ï¼Œèƒ½å»é™¤å†—ä½™ï¼Œä¿ç•™æ ¸å¿ƒè„‰ç»œã€‚\n",
    "\n",
    "\n",
    "#### å·¥ä½œæœºåˆ¶\n",
    "\n",
    "1. ç”¨æˆ·ä¸æ¨¡å‹çš„å¯¹è¯ä¸æ–­ç§¯ç´¯ã€‚\n",
    "2. å½“å†å²è¿‡é•¿æ—¶ï¼Œè§¦å‘æ€»ç»“é€»è¾‘ã€‚\n",
    "3. æ¨¡å‹ä¼šæŠŠæ—©æœŸçš„å¯¹è¯â€œå‹ç¼©â€ä¸ºä¸€æ®µæ‘˜è¦ï¼ˆä¾‹å¦‚â€œç”¨æˆ·ä¸»è¦åœ¨å’¨è¯¢æ—…æ¸¸è·¯çº¿å’Œæœºç¥¨ä¿¡æ¯â€ï¼‰ã€‚\n",
    "4. æ—§çš„è¯¦ç»†æ¶ˆæ¯è¢«ä¸¢å¼ƒï¼Œæ‘˜è¦æ›¿ä»£å®ƒè¿›å…¥è®°å¿†ä¸­ã€‚\n",
    "5. åç»­çš„è°ƒç”¨ä¼šåœ¨ä¸Šä¸‹æ–‡ä¸­ç»§ç»­æºå¸¦è¿™ä¸ªæ‘˜è¦ï¼Œä»è€Œä¿æŒå¯¹å†å²çš„ç†è§£ã€‚\n",
    "\n",
    "#### RunnableWithMessageHistory + è‡ªå®šä¹‰å‰ªè£\n",
    "\n",
    "å› ä¸º trim_messages æ–¹æ³•æ— æ³•å®Œæˆå…ˆæ€»ç»“åä¿å­˜çš„ä»»åŠ¡ï¼Œå› æ­¤åœ¨æ–°ç‰ˆ LCEL çš„è¯­å¢ƒä¸‹ï¼Œæˆ‘ä»¬é€šè¿‡ **RunnableWithMessageHistory** æ­é…è‡ªå®šä¹‰é€»è¾‘æ¥å®ç°â€œæ€»ç»“åä¿å­˜â€çš„åŠŸèƒ½ã€‚ä¸‹é¢ç»™å‡ºä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ä»£ç ï¼Œæ¼”ç¤ºå¦‚ä½•åœ¨æ¯æ¬¡å¯¹è¯åï¼Œå¯¹æœ€åä¸€æ¡ AI å›å¤è¿›è¡Œæ‘˜è¦ï¼Œä»è€ŒèŠ‚çœä¸Šä¸‹æ–‡ã€‚\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import AIMessage\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful assistant.\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "  (\"human\", \"{input}\")])\n",
    "chain = prompt | llm\n",
    "\n",
    "def summarize_text_with_llm(text: str, max_len: int = 50) -> str:\n",
    "    \"\"\"ç”¨å¤§æ¨¡å‹æ€»ç»“æ–‡æœ¬ï¼Œæ§åˆ¶åœ¨ max_len å­—ç¬¦å†…\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"è¯·åœ¨ä¸è¶…è¿‡{max_len}ä¸ªå­—å†…ï¼Œç®€è¦æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼š\\n\\n{text}\"\n",
    "    )\n",
    "\n",
    "    # è°ƒç”¨å¤§æ¨¡å‹ï¼Œä¼ å…¥æ›¿æ¢å¥½å‚æ•°çš„æç¤ºè¯ï¼Œè¿”å›ä¸€ä¸ªæ¶ˆæ¯å¯¹è±¡æˆ–å­—ç¬¦ä¸²\n",
    "    summary = llm.invoke(prompt.format(text=text, max_len=max_len))\n",
    "\n",
    "    return summary.content # è¿”å›æ–‡æœ¬å†…å®¹\n",
    "\n",
    "session_store = {}\n",
    "def get_session_history(session_id: str, summary_max_len: int = 50):\n",
    "    \"\"\"è·å–ä¼šè¯å†å²ï¼Œå¹¶å°†æœ€åä¸€æ¡AIMessageæ€»ç»“åæ›¿æ¢\"\"\"\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    history = session_store[session_id]\n",
    "    # å¦‚æœä¼šè¯é‡Œæœ‰æ¶ˆæ¯ï¼Œä¸”æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯ AI å›å¤ï¼Œå°±åšæ€»ç»“\n",
    "    if history.messages and isinstance(history.messages[-1], AIMessage):\n",
    "        last_ai_msg = history.messages[-1] # å–å‡ºæœ€åä¸€æ¡ AI å›å¤\n",
    "        summarized = summarize_text_with_llm(last_ai_msg.content, max_len=summary_max_len) # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œæ€»ç»“ï¼Œæ§åˆ¶åœ¨ summary_max_len å­—ç¬¦å†…\n",
    "        history.messages[-1] = AIMessage(content=summarized)  # æ›¿æ¢ä¸ºæ€»ç»“å†…å®¹\n",
    "    return history # è¿”å›æ›´æ–°åçš„å†å²è®°å½•\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "  runnable=chain,\n",
    "  get_session_history=get_session_history,\n",
    "  input_messages_key=\"input\",\n",
    "  history_messages_key=\"chat_history\")\n",
    "\n",
    "session_id = \"test_session4\"\n",
    "\n",
    "response1 = chain_with_history.invoke(\n",
    "  {\"input\": \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½, 50å­—ä»‹ç»ä¸€ä¸‹\"},\n",
    "  config={\"configurable\": {\"session_id\": session_id}})\n",
    "print(\"ç¬¬ä¸€æ¬¡å›ç­”ï¼š\", response1.content)\n",
    "\n",
    "response2 = chain_with_history.invoke(\n",
    "  {\"input\": \"å®ƒæœ‰å“ªäº›åº”ç”¨, 50å­—ä»‹ç»ä¸€ä¸‹\"},\n",
    "  config={\"configurable\": {\"session_id\": session_id}})\n",
    "print(\"ç¬¬äºŒæ¬¡å›ç­”ï¼š\", response2.content)\n",
    "\n",
    "# æ‰“å°è£å‰ªåçš„å†å²\n",
    "print(\"\\n=== æ€»ç»“åçš„ä¼šè¯å†å² ===\")\n",
    "for i, msg in enumerate(session_store[session_id].messages, 1):\n",
    "  print(i, msg.type, msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ€è·¯è§£æ\n",
    "\n",
    "1. **é“¾çš„æ„å»º**ï¼šç”¨ `ChatPromptTemplate` + `ChatOpenAI` æ„å»ºåŸºæœ¬å¯¹è¯é“¾ã€‚\n",
    "2. **æ‘˜è¦é€»è¾‘**ï¼šè‡ªå®šä¹‰ `summarize_text_with_llm`ï¼Œè°ƒç”¨ LLM å¯¹ AI çš„å›å¤åšç®€çŸ­æ€»ç»“ã€‚\n",
    "3. **å†å²ç®¡ç†**ï¼šåœ¨ `get_session_history` ä¸­ï¼Œæ¯æ¬¡æ›´æ–°å†å²æ—¶ï¼Œç”¨æ‘˜è¦æ›¿æ¢æœ€åä¸€æ¡ AI å›å¤ã€‚\n",
    "4. **RunnableWithMessageHistory**ï¼šæŠŠé“¾ä¸è®°å¿†æœºåˆ¶ç»“åˆï¼Œè®©å®ƒåœ¨å¤šè½®å¯¹è¯ä¸­è‡ªåŠ¨è°ƒç”¨æ‘˜è¦é€»è¾‘ã€‚\n",
    "\n",
    "é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å®ç°äº†ç±»ä¼¼ **ConversationSummaryMemory** çš„æ•ˆæœï¼šåœ¨ä¿æŒä¸Šä¸‹æ–‡è¿è´¯çš„åŒæ—¶ï¼Œå¤§å¹…åº¦å‡å°‘äº†å†å²æ¶ˆæ¯çš„å†—ä½™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradio å‰ç«¯è®°å¿†\n",
    "\n",
    "å‡å¦‚æˆ‘ä»¬å¸Œæœ›èƒ½å¤ŸæŠŠèŠå¤©è®°å½•æ˜¾ç¤ºåœ¨gradioä¸Šï¼Œæˆ‘ä»¬å°±éœ€è¦Gradio.Chatbotè¿™ä¸ªç»„ä»¶ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "    (\"human\", \"{input}\")])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "session_store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    return session_store[session_id]\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    runnable=chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "session_id = \"session_test5\"\n",
    "def chat(message, history):\n",
    "    history = history or []\n",
    "    resp = chain_with_history.invoke(\n",
    "        {\"input\": message},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "    reply = resp.content if hasattr(resp, \"content\") else str(resp)\n",
    "    history = history + [\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "        {\"role\": \"assistant\", \"content\": reply},\n",
    "    ]\n",
    "    return \"\", history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ğŸ¤– èŠå¤©æœºå™¨äººï¼ˆå†…ç½®è®°å¿†ï¼‰\")\n",
    "    chatbot = gr.Chatbot(label=\"LangChain ChatBot\")\n",
    "    msg = gr.Textbox(placeholder=\"å’Œæˆ‘èŠèŠå§...\", lines=1)\n",
    "    msg.submit(chat, [msg, chatbot], [msg, chatbot])\n",
    "    gr.ClearButton([msg, chatbot])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ–è€…ä½¿ç”¨å‰é¢è¯´çš„ ChatInterface å®ç°ï¼Œæ­¤æ—¶ä¼šæ›´åŠ ç®€åŒ–ä¸€äº›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "import os\n",
    "import gradio as gr\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    return session_store[session_id]\n",
    "\n",
    "session_store = {}\n",
    "\n",
    "def llm_response(content, history):\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"ernie-3.5-8k\",\n",
    "        openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "        base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\",\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    \n",
    "    session_id = \"user_1\"\n",
    "    \n",
    "    chain_with_history = RunnableWithMessageHistory(\n",
    "        runnable=chain,\n",
    "        get_session_history=get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "    )\n",
    "    resp = chain_with_history.invoke(\n",
    "            {\"input\": content},\n",
    "            config={\"configurable\": {\"session_id\": session_id}},\n",
    "        )\n",
    "    reply = resp.content if hasattr(resp, \"content\") else str(resp)\n",
    "    return reply\n",
    "\n",
    "demo = gr.ChatInterface(fn=llm_response)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
