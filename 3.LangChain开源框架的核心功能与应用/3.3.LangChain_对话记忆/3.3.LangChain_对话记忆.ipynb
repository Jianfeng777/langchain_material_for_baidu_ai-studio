{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 环境配置\n",
    "\n",
    "## 1.1 python 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: gradio==6.1.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (6.1.0)\n",
      "Requirement already satisfied: openai==2.11.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: dashscope==1.25.4 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (1.25.4)\n",
      "Requirement already satisfied: langchain-classic==1.0.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain==1.1.3 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: langchain-community==0.4.1 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-openai==1.1.3 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (4.12.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (1.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (0.125.0)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.1 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (2.0.1)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (1.2.3)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (3.0.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (2.3.5)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (3.11.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (2.3.3)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (12.0.0)\n",
      "Requirement already satisfied: pydantic<=2.12.4,>=2.11.10 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (2.12.4)\n",
      "Requirement already satisfied: pydub in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (0.0.21)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (6.0.3)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (4.15.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio==6.1.0) (0.38.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from openai==2.11.0) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from openai==2.11.0) (0.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from openai==2.11.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from openai==2.11.0) (4.67.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from dashscope==1.25.4) (3.13.2)\n",
      "Requirement already satisfied: requests in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from dashscope==1.25.4) (2.32.5)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from dashscope==1.25.4) (1.9.0)\n",
      "Requirement already satisfied: cryptography in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from dashscope==1.25.4) (46.0.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from dashscope==1.25.4) (2025.11.12)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langchain-classic==1.0.0) (1.2.2)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langchain-classic==1.0.0) (1.1.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.17 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langchain-classic==1.0.0) (0.5.0)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=1.4.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langchain-classic==1.0.0) (2.0.45)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langchain==1.1.3) (1.0.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langchain-community==0.4.1) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langchain-community==0.4.1) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langchain-community==0.4.1) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langchain-community==0.4.1) (0.4.3)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langchain-openai==1.1.3) (0.12.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from gradio-client==2.0.1->gradio==6.1.0) (2025.12.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from aiohttp->dashscope==1.25.4) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from aiohttp->dashscope==1.25.4) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from aiohttp->dashscope==1.25.4) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from aiohttp->dashscope==1.25.4) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from aiohttp->dashscope==1.25.4) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from aiohttp->dashscope==1.25.4) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from aiohttp->dashscope==1.25.4) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from anyio<5.0,>=3.0->gradio==6.1.0) (3.11)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1) (0.9.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from fastapi<1.0,>=0.115.2->gradio==6.1.0) (0.0.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio==6.1.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio==6.1.0) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio==6.1.0) (3.20.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio==6.1.0) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio==6.1.0) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio==6.1.0) (0.20.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-classic==1.0.0) (1.33)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-classic==1.0.0) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-classic==1.0.0) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.1.3) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.1.3) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.1.3) (0.3.0)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.1.3) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain==1.1.3) (1.12.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic==1.0.0) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic==1.0.0) (0.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from pandas<3.0,>=1.0->gradio==6.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from pandas<3.0,>=1.0->gradio==6.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from pandas<3.0,>=1.0->gradio==6.1.0) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from pydantic<=2.12.4,>=2.11.10->gradio==6.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from pydantic<=2.12.4,>=2.11.10->gradio==6.1.0) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from pydantic<=2.12.4,>=2.11.10->gradio==6.1.0) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community==0.4.1) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from requests->dashscope==1.25.4) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from requests->dashscope==1.25.4) (2.6.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from sqlalchemy<3.0.0,>=1.4.0->langchain-classic==1.0.0) (3.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai==1.1.3) (2025.11.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from typer<1.0,>=0.12->gradio==6.1.0) (8.3.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from typer<1.0,>=0.12->gradio==6.1.0) (14.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio==6.1.0) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio==6.1.0) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==6.1.0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==6.1.0) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==6.1.0) (0.1.2)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from cryptography->dashscope==1.25.4) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\lib\\site-packages (from cffi>=2.0.0->cryptography->dashscope==1.25.4) (2.23)\n"
     ]
    }
   ],
   "source": [
    "! pip install gradio==6.1.0 openai==2.11.0 dashscope==1.25.4 langchain-classic==1.0.0 langchain==1.1.3 langchain-community==0.4.1 langchain-openai==1.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 大模型密钥准备\n",
    "\n",
    "请根据第一章内容获取相关平台的 API KEY，如若未在系统变量中填入，请将 API_KEY 信息写入以下代码（若已设置请忽略）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxx\"\n",
    "# os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-yyyyyyyy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LangChain 里的记忆机制（Memory）\n",
    "\n",
    "在对话场景中，记忆（Memory） 是一个非常关键的能力。它让大模型能够“记住”之前的对话内容，从而在后续回答中保持上下文连贯。\n",
    "\n",
    "## 2.1 为什么需要记忆？\n",
    "\n",
    "大模型本质上是“无状态”的，每次调用时，它只根据输入上下文生成输出。为了让对话看起来“连续”，我们需要额外的机制来保存历史记录并在后续调用中传递：\n",
    "\n",
    "* **上下文连贯**：用户前面提到的人物、地点、问题，在后续对话里依然能被理解。\n",
    "* **个性化记忆**：保存用户的偏好或背景信息，以便做出个性化回答。\n",
    "* **任务跟踪**：在多轮对话中持续推进任务，不需要用户重复输入。\n",
    "\n",
    "\n",
    "比如下面的例子，我们与模型进行多轮的对话后，由于我们没有配置和记忆相关的内容，因此其还是记不住我们第一次说过的名称信息。所以如果没有 Memory，大模型只能看到最后一句，无法理解上下文。而如果用了 Memory，大模型可以看到前面所有内容，就可以做出更有逻辑的回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们才刚开始对话，我还不清楚您的名字呢。不过，如果您愿意告诉我，我会很乐意记住并称呼您的！您希望我怎么称呼您呢？\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  model=\"ernie-3.5-8k\",\n",
    "  openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "  base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\" \n",
    ")\n",
    "\n",
    "response1 = llm.invoke(\"我叫李，你是谁？\")\n",
    "response2 = llm.invoke(\"你住在哪？\")\n",
    "response3 = llm.invoke(\"还记得我叫啥不\")\n",
    "\n",
    "print(response3.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那下面我们就来介绍一下 LangChain 里常常能够用到的记忆类型吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 全量记忆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 ConversationBufferMemory（旧版全量记忆）\n",
    "\n",
    "#### 概念\n",
    "\n",
    "早期的 LangChain 使用 **ConversationBufferMemory** 来保存对话历史。它会完整保存用户与模型之间的所有对话历史，包括每一次提问（user message）和每一次回答（AI response），并在每次调用语言模型时将这些内容作为上下文重新发送，确保模型能“记住”之前发生过的所有交流。\n",
    "\n",
    "\n",
    "它的原理很简单：\n",
    "\n",
    "* 每次对话会追加进一个“对话缓存”列表。\n",
    "* 调用模型时，LangChain 会把缓存中的所有内容拼接起来放入 prompt。\n",
    "* 所以模型每次响应时，都会拥有“完整上下文”。\n",
    "\n",
    "#### 存储格式\n",
    "\n",
    "一般而言， **ConversationBufferMemory** 会将所有的对话信息保存在 `memory.chat_memory.messages` ，这样一个列表里面。里面包含了所有对话的信息，包括 `HumanMessage` 、 `AIMessage` 以及 `SystemMessage` 等等。比如像下面这样的例子：\n",
    "\n",
    "```python \n",
    "memory.chat_memory.messages = [\n",
    "  HumanMessage(content=\"你好\"),\n",
    "  AIMessage(content=\"你好！我可以帮你什么？\"),\n",
    "  HumanMessage(content=\"1+1等于几\"),\n",
    "  AIMessage(content=\"等于2\"),\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "#### 代码示例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 你好，我叫李剑锋\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\76391\\AppData\\Local\\Temp\\ipykernel_2316\\1408029818.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "C:\\Users\\76391\\AppData\\Local\\Temp\\ipykernel_2316\\1408029818.py:11: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(llm=llm,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "对话 1: 你好，李剑锋！很高兴认识你。你的名字很有力量感呢，\"剑锋\"让人联想到锐利的剑刃，很有武侠风范！不知道你平时有什么兴趣爱好吗？比如喜欢看书、运动，或者对科技感兴趣？\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好，我叫李剑锋\n",
      "AI: 你好，李剑锋！很高兴认识你。你的名字很有力量感呢，\"剑锋\"让人联想到锐利的剑刃，很有武侠风范！不知道你平时有什么兴趣爱好吗？比如喜欢看书、运动，或者对科技感兴趣？\n",
      "Human: 请问1+1等于多少?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "对话 2: 1+1 是一个非常基础的数学问题呢！在常规的算术运算中，1+1 等于 2。不过，如果你是在脑筋急转弯或者某些特殊情境下提问，答案可能会不一样哦！但在这里，我就给你一个标准的数学答案吧：1+1=2！\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好，我叫李剑锋\n",
      "AI: 你好，李剑锋！很高兴认识你。你的名字很有力量感呢，\"剑锋\"让人联想到锐利的剑刃，很有武侠风范！不知道你平时有什么兴趣爱好吗？比如喜欢看书、运动，或者对科技感兴趣？\n",
      "Human: 请问1+1等于多少?\n",
      "AI: 1+1 是一个非常基础的数学问题呢！在常规的算术运算中，1+1 等于 2。不过，如果你是在脑筋急转弯或者某些特殊情境下提问，答案可能会不一样哦！但在这里，我就给你一个标准的数学答案吧：1+1=2！\n",
      "Human: 我的名字是?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "对话 3: 你之前告诉过我，你的名字是李剑锋呀！这个名字真的很有特色，让人印象深刻呢！\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.chains import ConversationChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, \n",
    "  memory=memory,\n",
    "  verbose=True)\n",
    "  \n",
    "response_1 = conversation.predict(input=\"你好，我叫李剑锋\")\n",
    "print(\"对话 1:\", response_1) \n",
    "response_2 = conversation.predict(input=\"请问1+1等于多少?\")\n",
    "print(\"对话 2:\", response_2)\n",
    "response_3 = conversation.predict(input=\"我的名字是?\")\n",
    "print(\"对话 3:\", response_3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 RunnableWithMessageHistory（新版全量记忆）\n",
    "\n",
    "#### 概念\n",
    "\n",
    "RunnableWithMessageHistory 是 LangChain 新一代的对话历史管理工具。让模型在多轮对话中记住上下文，而不需要在每次调用时手动传入所有历史。\n",
    "\n",
    "其对比 ConversationBufferMemory 的优势在于：\n",
    "* 支持多会话管理：通过 session_id 管理不同用户或不同会话的历史，互不干扰。\n",
    "* 解耦：历史记录和链逻辑分离，任何 Runnable 都能加记忆。\n",
    "* 可扩展：可以替换存储层，比如存数据库、Redis，而不是内存变量。\n",
    "\n",
    "#### 代码示例\n",
    "\n",
    "首先我们像之前一样定义好提示词模版、模型以及将这两者组合在一起的 **LCEL** 链。但是这里和普通的提示词模版不一样的是，我们要在这个中间加上记忆过往记忆部分的内容，这里用的就是 `MessagesPlaceholder()` 方法，并且绑定好传入的名称。\n",
    "\n",
    "然后呢我们还需要设置存储位置（`session_store`）及载入记忆的函数（`get_session_history`）。这个函数其实做的事情就是判定传入的 `session_id` 是否在存储的字典 `session_store` 是否存在（这个 `session_id` 是用于判定是否是同一段对话的关键内容），假如存在那就把记忆导出来传给提示词模版中的占位符 `MessagesPlaceholder()` ，假如不存在的话就在存储的字典里新建一条记录，也就是 `{\"session_id\" :ChatMessageHistory()}` 这样的格式。\n",
    "\n",
    "再然后我们就需要通过 `RunnableWithMessageHistory` 将前面定义好的这些内容拼装起来，包括需要进行记忆的链（`runnable`）、传递记忆的方法（`get_session_histoy`）、输入信息的变量名称（`input_messages_key`）以及传入历史记录的变量名称（`history_messages_key`）。这样绑定过后，就能够实现记忆的自动存储及调用了。\n",
    "\n",
    "最后我们还需要做的是设置一个单独的 `session_id` ，也就是这里的 \"test_session1\" 来作为这段对话独特的记忆标签。然后就可以正常的与大模型进行对话沟通了。但是这里需要注意的是，我们这里调用的就不是前面创建的 LCEL 链了，而是包裹了 LCEL 以及记忆的历史记录链 `RunnableWithMessageHistory` 。并且在调用的时候我们需要将 `session_id` 作为 config 传入。传入后每次当我们调用完获取回复，LangChain 就会根据我们 `session_id` 找到对应的历史记录，并把信息传入进去。后续我们可以通过打印的方式来对这部分历史的信息进行了解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一次回答： 人工智能是模拟人类智能的科学与技术，通过算法和模型让机器具备学习、推理、感知等能力，以解决复杂问题。\n",
      "第二次回答： 人工智能应用广泛，涵盖智能语音助手、自动驾驶、医疗影像诊断、金融风控及个性化推荐等诸多领域。\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "    (\"human\", \"{input}\")])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "session_store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    return session_store[session_id]\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    runnable=chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "session_id = \"test_session1\"\n",
    "\n",
    "# 第一次调用\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"什么是人工智能, 50字介绍一下\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"第一次回答：\", response1.content)\n",
    "\n",
    "# 第二次调用，带历史\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"它有哪些应用, 50字介绍一下\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"第二次回答：\", response2.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 记忆节省方法\n",
    "\n",
    "### 2.3.1 使用原因\n",
    "\n",
    "使用全量记忆虽然可以完整保留对话历史，但它也有一些显著的缺点或潜在问题，主要包括以下几点：\n",
    "\n",
    "* 上下文限制：大模型有最大 token 限制（例如 8k、32k），超过就会报错。\n",
    "* 使用成本增加：输入 token 越多，调用费用和响应时间越高。\n",
    "* 大模型无法准确理解信息：冗长的上下文里可能掺杂大量无关信息，反而影响模型准确性。\n",
    "\n",
    "因此，LangChain 提供了几种常见的“节省记忆”方法，比如：\n",
    "* ConversationBufferWindowMemory（保存前 n 轮对话）\n",
    "* ConversationTokenBufferMemory（保存 n 个 token 的对话内容）\n",
    "* ConversationSummaryMemory（总结后保存）\n",
    "\n",
    "但是由于这些方法都即将在 LangChain 的 1.0 版本时被丢弃，因此我们需要通过 RunnableWithMessageHistory + trim_messages （记忆剪辑） 的方法重新的实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 trim_messages（记忆剪辑）\n",
    "\n",
    "#### 概念\n",
    "\n",
    "在对话记忆管理中，LangChain 提供了一个非常实用的工具函数 —— **`trim_messages`**。它的作用是对已有的对话历史进行“剪辑”，以便控制上下文长度，避免超过模型的最大 token 限制。并且，通过灵活的参数设置，我们可以：\n",
    "\n",
    "* 控制上下文的最大长度\n",
    "* 保证关键消息（如 system 指令）不丢失\n",
    "* 根据需求选择合适的裁剪策略\n",
    "\n",
    "它通常与 **RunnableWithMessageHistory** 结合使用，形成一个完整的对话记忆管理解决方案。\n",
    "\n",
    "#### 基本用法\n",
    "\n",
    "`trim_messages` 接收一组消息（Message 列表），以及一些裁剪参数，返回裁剪后的消息列表。\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "trimmed = trim_messages(\n",
    "    messages=history.messages,\n",
    "    token_counter=lambda msgs: sum(len(str(m.content)) for m in msgs),\n",
    "    max_tokens=100,\n",
    "    start_on=\"human\",   # 从 human 消息开始截断\n",
    "    include_system=True, # 是否保留 system 提示\n",
    "    strategy=\"last\",    # 裁剪策略\n",
    "    allow_partial=True   # 是否允许部分保留\n",
    ")\n",
    "```\n",
    "\n",
    "#### 常见参数说明\n",
    "\n",
    "* **messages**：需要裁剪的消息历史。\n",
    "* **token_counter**：自定义 token 计数函数（例如按字符长度或真实 token 数）。\n",
    "* **max_tokens**：上下文允许的最大 token 数。\n",
    "* **start_on**：指定从哪种角色的消息开始裁剪（如 human / ai）。\n",
    "* **include_system**：是否始终保留系统消息（system role）。\n",
    "* **strategy**：裁剪策略，例如 `last` 表示保留最新消息。\n",
    "* **allow_partial**：是否允许只保留消息的一部分。\n",
    "\n",
    "#### 裁剪策略\n",
    "\n",
    "1. **保留最新（last）**：只保留最近的若干消息，最常用。\n",
    "2. **保留最早（first）**：保留开头的一些上下文（较少使用）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 ConversationBufferWindowMemory（保存前 n 轮对话）\n",
    "\n",
    "#### 概念\n",
    "\n",
    "ConversationBufferWindowMemory 是 LangChain 提供的一种“有限长度的对话记忆方式”，它不会保留完整对话历史，而是仅保留最近 N 轮消息（由用户设定）。\n",
    "\n",
    "#### 工作机制\n",
    "\n",
    "* 每轮对话（用户输入 + 模型输出）会被记录下来。\n",
    "* 系统仅保留最近设定的 k 轮对话，其余部分会被“遗忘”。\n",
    "* 每次调用模型时，仅发送这 N 轮内容作为 prompt 上下文。\n",
    "\n",
    "#### 代码示例\n",
    "\n",
    "那对于要保留前前 n 轮对话的记忆而言，我们记忆剪裁的方式就是先去根据 session_id 获取对应的完整记忆信息，然后从里面提取出对应的内容。那裁剪的话，本质上就是去看这里面有多少条信息（通过 `len()` 的方式去计算），然后只留下最近发生的 2 条聊天记录即可（一条 AIMessage 和 一条 HumanMessage）。那根据这个方法我们设计了 `trim_by_conversation_pairs()` 这个函数：\n",
    "\n",
    "```python\n",
    "def trim_by_conversation_pairs(history, max_pairs=1, include_system=True):\n",
    "    \"\"\"\n",
    "    按对话段数裁剪历史，每段 = Human + AI 两条消息\n",
    "    默认只保留最近 max_pairs 段\n",
    "    \"\"\"\n",
    "    trimmed = trim_messages(\n",
    "        history.messages, # 从保存的字典里获取信息列表\n",
    "        token_counter=lambda msgs: len(msgs),  # 按消息条数统计（列表里有多少内容）\n",
    "        max_tokens=max_pairs,  # 保留 max_pairs 段\n",
    "        start_on=\"human\",  # 确保每次传入模型的对话历史都从 HumanMessage 开始  \n",
    "        include_system=include_system, # 保留系统提示词信息\n",
    "        strategy=\"last\",  # 保留最新的内容\n",
    "    )\n",
    "    history.messages[:] = trimmed  # 将剪裁好的信息更新\n",
    "```\n",
    "\n",
    "我们只需要传入对应的历史信息，然后就能够在传入到提示词模版中的记忆变量前，将记忆进行删减，从而节省其中所花费 token 的数量。最后我们设置一个新的 session_id ，并把记录打印出来。那这样就能看到只保留了最近的一条消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一次回答： 人工智能是模拟人类智能的计算机系统，能学习、推理、解决问题，在多领域展现强大能力并持续拓展应用边界。\n",
      "第二次回答： 它（以AI为例）应用广泛，涵盖智能客服、医疗诊断、自动驾驶、教育辅导、金融风控等领域，提升效率与服务质量。\n",
      "\n",
      "=== 裁剪后的会话历史（只保留最近一轮） ===\n",
      "1 human 它有哪些应用, 50字介绍一下\n",
      "2 ai 它（以AI为例）应用广泛，涵盖智能客服、医疗诊断、自动驾驶、教育辅导、金融风控等领域，提升效率与服务质量。\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import trim_messages\n",
    "import os\n",
    "\n",
    "def trim_by_conversation_pairs(history, max_pairs=1, include_system=True):\n",
    "    \"\"\"\n",
    "    按对话段数裁剪历史，每段 = Human + AI 两条消息\n",
    "    默认只保留最近 max_pairs 段\n",
    "    \"\"\"\n",
    "    trimmed = trim_messages(\n",
    "        history.messages, # 从保存的字典里获取信息列表\n",
    "        token_counter=lambda msgs: len(msgs),  # 按消息条数统计（列表里有多少内容）\n",
    "        max_tokens=max_pairs,  # 保留 max_pairs 段\n",
    "        start_on=\"human\",  # 确保每次传入模型的对话历史都从 HumanMessage 开始  \n",
    "        include_system=include_system, # 保留系统提示词信息\n",
    "        strategy=\"last\",  # 保留最新的内容\n",
    "    )\n",
    "    history.messages[:] = trimmed  # 将剪裁好的信息更新\n",
    "\n",
    "session_store = {}\n",
    "def get_session_history(session_id, max_pairs=1):\n",
    "    \"\"\"每次获取历史时自动裁剪，保证不会无限增长\"\"\"\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    history = session_store[session_id]\n",
    "    trim_by_conversation_pairs(history, max_pairs=max_pairs)  # 按对话段数裁剪\n",
    "    return history\n",
    "\n",
    "llm = ChatOpenAI(model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful assistant.\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "  (\"human\", \"{input}\")])\n",
    "chain = prompt | llm\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "  runnable=chain,\n",
    "  get_session_history=get_session_history,\n",
    "  input_messages_key=\"input\",\n",
    "  history_messages_key=\"chat_history\")\n",
    "\n",
    "session_id = \"test_session2\"\n",
    "\n",
    "# 第一次调用\n",
    "response1 = chain_with_history.invoke(\n",
    "  {\"input\": \"什么是人工智能, 50字介绍一下\"},\n",
    "  config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"第一次回答：\", response1.content)\n",
    "\n",
    "# 第二次调用，带历史\n",
    "response2 = chain_with_history.invoke(\n",
    "  {\"input\": \"它有哪些应用, 50字介绍一下\"},\n",
    "  config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"第二次回答：\", response2.content)\n",
    "\n",
    "# 打印裁剪后的历史\n",
    "print(\"\\n=== 裁剪后的会话历史（只保留最近一轮） ===\")\n",
    "for i, msg in enumerate(session_store[session_id].messages, 1):\n",
    "    print(i, msg.type, msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 ConversationTokenBufferMemory（基于 Token 的记忆裁剪）\n",
    "\n",
    "除了“保留前 n 轮对话”这种相对宽松的方式外，还有一种更严格的节省资源方法 —— **基于 Token 的记忆裁剪**。这种方法会限定对话历史最多只能占用固定数量的 **token**，一旦超过，就会自动丢弃最早的内容，从而保证上下文始终处于可控范围内。\n",
    "\n",
    "这种机制过去在 LangChain 中对应的实现就是 **ConversationTokenBufferMemory**。它的特点是：\n",
    "\n",
    "* 不关心“对话轮数”，只关心 token 总量。\n",
    "* 当对话逐渐变长时，会优先保留最近的内容，把旧消息逐步丢弃。\n",
    "* 更加适合对 **上下文长度** 和 **计算成本** 有严格限制的场景。\n",
    "\n",
    "#### Token 的概念\n",
    "\n",
    "要理解这种方式，我们需要先弄清楚 **token** 是什么。简单来说：\n",
    "\n",
    "* **Token ≠ 字符**：在 NLP 模型里，一个 token 并不是单个汉字或字母，而是模型分词后的最小单位。\n",
    "* **举例**：\n",
    "\n",
    "  * 英文单词 *\"apple\"* 可能就是 1 个 token。\n",
    "  * 中文“苹果”通常会被拆分为 1~2 个 token。\n",
    "  * 较长的英文单词可能会被拆成多个 token。\n",
    "* **意义**：token 是模型处理的最基本单元，模型的上下文限制、计算开销、调用费用都直接和 token 数量挂钩。\n",
    "\n",
    "因此，ConversationTokenBufferMemory 的本质，就是通过严格限制 **token 数量**，来控制上下文大小，避免超出模型的处理上限。\n",
    "\n",
    "\n",
    "#### 结合 RunnableMemory 与 trim_messages 的实现\n",
    "\n",
    "在新版 LCEL 框架下，我们可以通过 **RunnableMemory** 搭配 `trim_messages` 来实现类似的功能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一次回答： 人工智能是模拟人类智能的科技，能通过学习、推理等自主处理信息，在多领域实现智能决策与任务执行。\n",
      "第二次回答： 它若指人工智能，应用广泛。在医疗领域辅助诊断疾病、制定治疗方案；教育上实现个性化学习规划；交通中助力自动驾驶；金融里进行风险评估与智能投顾；工业里优化生产流程、质量检测；还能用于智能客服、智能家居等，极大改变生活与工作模式。\n",
      "\n",
      "=== 裁剪后的会话历史（只保留最新的 n 个 token） ===\n",
      "1 human 它有哪些应用, 100字介绍一下\n",
      "2 ai 它若指人工智能，应用广泛。在医疗领域辅助诊断疾病、制定治疗方案；教育上实现个性化学习规划；交通中助力自动驾驶；金融里进行风险评估与智能投顾；工业里优化生产流程、质量检测；还能用于智能客服、智能家居等，极大改变生活与工作模式。\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import trim_messages\n",
    "import os\n",
    "\n",
    "# 定义 token 计算方法（简单认定一个字符为1个 token）\n",
    "def count_tokens_by_len(msgs):\n",
    "    return sum(len(str(m.content)) for m in msgs)\n",
    "\n",
    "def trim_history_inplace(history, max_tokens=60):\n",
    "    trimmed = trim_messages(\n",
    "        history.messages, # 传入历史信息\n",
    "        token_counter=count_tokens_by_len, # 按一个字符的方式计算每一段内容的 token\n",
    "        max_tokens=max_tokens, # 传入的最大 token 数量\n",
    "        start_on=\"human\", # 确保每次传入模型的对话历史都从 HumanMessage 开始\n",
    "        include_system=True, # 包含系统提示词\n",
    "        strategy=\"last\",) # 保留最新的内容\n",
    "    history.messages[:] = trimmed  # 原地更新\n",
    "\n",
    "llm = ChatOpenAI(model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful assistant.\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "  (\"human\", \"{input}\")])\n",
    "chain = prompt | llm\n",
    "\n",
    "session_store = {}\n",
    "\n",
    "def get_session_history(session_id, max_tokens=60):\n",
    "  if session_id not in session_store:\n",
    "    session_store[session_id] = ChatMessageHistory()\n",
    "  history = session_store[session_id]\n",
    "  trim_history_inplace(history, max_tokens=max_tokens) # 每次取之前自动裁剪\n",
    "  return history\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "  runnable=chain,\n",
    "  get_session_history=get_session_history,\n",
    "  input_messages_key=\"input\",\n",
    "  history_messages_key=\"chat_history\")\n",
    "\n",
    "session_id = \"test_session3\"\n",
    "\n",
    "response1 = chain_with_history.invoke(\n",
    "  {\"input\": \"什么是人工智能, 50字介绍一下\"},\n",
    "  config={\"configurable\": {\"session_id\": session_id}})\n",
    "print(\"第一次回答：\", response1.content)\n",
    "\n",
    "response2 = chain_with_history.invoke(\n",
    "  {\"input\": \"它有哪些应用, 100字介绍一下\"},\n",
    "  config={\"configurable\": {\"session_id\": session_id}})\n",
    "print(\"第二次回答：\", response2.content)\n",
    "\n",
    "# 打印裁剪后的历史\n",
    "print(\"\\n=== 裁剪后的会话历史（只保留最新的 n 个 token） ===\")\n",
    "for i, msg in enumerate(session_store[session_id].messages, 1):\n",
    "  print(i, msg.type, msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 思路解析\n",
    "\n",
    "* 这里我们自定义了 `count_tokens_by_len` 来估算 token 数量（虽然和真实分词略有差距，但是可以非常直观的进行展示）。\n",
    "* `trim_history_inplace` 每次都会检查当前的对话历史，并在必要时执行“原地裁剪”。\n",
    "* 这种实现方式能保证 **上下文不会无限膨胀**，而且可以灵活调整 `max_tokens` 的阈值。\n",
    "\n",
    "结合 **RunnableWithMessageHistory** 使用时，这样的剪裁逻辑就能在每次调用前自动生效，从而确保上下文始终保持在安全范围之内。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 ConversationSummaryMemory（总结后保存）\n",
    "\n",
    "除了“保留最近几轮”或“按 token 数量裁剪”以外，还有一种更智能的记忆方式 —— **ConversationSummaryMemory**。它的核心思路是：\n",
    "\n",
    "* 当对话逐渐变长时，不是简单地丢弃旧内容；\n",
    "* 而是通过 **大模型生成摘要** 的方式，把历史信息压缩成简短概述，然后再保存下来。\n",
    "\n",
    "这种方式的优势在于：\n",
    "\n",
    "* **信息保留更完整**：即使丢弃了原始消息，重要的事实与上下文依然能通过摘要继续存在。\n",
    "* **上下文更紧凑**：摘要比原文短得多，大幅减少 token 占用。\n",
    "* **逻辑更清晰**：通过模型生成的总结，能去除冗余，保留核心脉络。\n",
    "\n",
    "\n",
    "#### 工作机制\n",
    "\n",
    "1. 用户与模型的对话不断积累。\n",
    "2. 当历史过长时，触发总结逻辑。\n",
    "3. 模型会把早期的对话“压缩”为一段摘要（例如“用户主要在咨询旅游路线和机票信息”）。\n",
    "4. 旧的详细消息被丢弃，摘要替代它进入记忆中。\n",
    "5. 后续的调用会在上下文中继续携带这个摘要，从而保持对历史的理解。\n",
    "\n",
    "#### RunnableWithMessageHistory + 自定义剪裁\n",
    "\n",
    "因为 trim_messages 方法无法完成先总结后保存的任务，因此在新版 LCEL 的语境下，我们通过 **RunnableWithMessageHistory** 搭配自定义逻辑来实现“总结后保存”的功能。下面给出一个完整的示例代码，演示如何在每次对话后，对最后一条 AI 回复进行摘要，从而节省上下文。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一次回答： 人工智能是模拟人类智能的计算机系统，能学习、推理、解决问题，涵盖机器学习、自然语言处理等技术，改变生活工作方式。\n",
      "第二次回答： 人工智能应用广泛，如智能语音助手、自动驾驶、医疗影像诊断，提升效率，革新多行业服务模式。\n",
      "\n",
      "=== 总结后的会话历史 ===\n",
      "1 human 什么是人工智能, 50字介绍一下\n",
      "2 ai 人工智能模拟人类智能，涵盖机器学习等技术，能学习推理，改变生活工作方式。\n",
      "3 human 它有哪些应用, 50字介绍一下\n",
      "4 ai 人工智能应用广泛，如智能语音助手、自动驾驶、医疗影像诊断，提升效率，革新多行业服务模式。\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import AIMessage\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful assistant.\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "  (\"human\", \"{input}\")])\n",
    "chain = prompt | llm\n",
    "\n",
    "def summarize_text_with_llm(text: str, max_len: int = 50) -> str:\n",
    "    \"\"\"用大模型总结文本，控制在 max_len 字符内\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"请在不超过{max_len}个字内，简要总结以下内容：\\n\\n{text}\"\n",
    "    )\n",
    "\n",
    "    # 调用大模型，传入替换好参数的提示词，返回一个消息对象或字符串\n",
    "    summary = llm.invoke(prompt.format(text=text, max_len=max_len))\n",
    "\n",
    "    return summary.content # 返回文本内容\n",
    "\n",
    "session_store = {}\n",
    "def get_session_history(session_id: str, summary_max_len: int = 50):\n",
    "    \"\"\"获取会话历史，并将最后一条AIMessage总结后替换\"\"\"\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    history = session_store[session_id]\n",
    "    # 如果会话里有消息，且最后一条消息是 AI 回复，就做总结\n",
    "    if history.messages and isinstance(history.messages[-1], AIMessage):\n",
    "        last_ai_msg = history.messages[-1] # 取出最后一条 AI 回复\n",
    "        summarized = summarize_text_with_llm(last_ai_msg.content, max_len=summary_max_len) # 调用大模型进行总结，控制在 summary_max_len 字符内\n",
    "        history.messages[-1] = AIMessage(content=summarized)  # 替换为总结内容\n",
    "    return history # 返回更新后的历史记录\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "  runnable=chain,\n",
    "  get_session_history=get_session_history,\n",
    "  input_messages_key=\"input\",\n",
    "  history_messages_key=\"chat_history\")\n",
    "\n",
    "session_id = \"test_session4\"\n",
    "\n",
    "response1 = chain_with_history.invoke(\n",
    "  {\"input\": \"什么是人工智能, 50字介绍一下\"},\n",
    "  config={\"configurable\": {\"session_id\": session_id}})\n",
    "print(\"第一次回答：\", response1.content)\n",
    "\n",
    "response2 = chain_with_history.invoke(\n",
    "  {\"input\": \"它有哪些应用, 50字介绍一下\"},\n",
    "  config={\"configurable\": {\"session_id\": session_id}})\n",
    "print(\"第二次回答：\", response2.content)\n",
    "\n",
    "# 打印裁剪后的历史\n",
    "print(\"\\n=== 总结后的会话历史 ===\")\n",
    "for i, msg in enumerate(session_store[session_id].messages, 1):\n",
    "  print(i, msg.type, msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 思路解析\n",
    "\n",
    "1. **链的构建**：用 `ChatPromptTemplate` + `ChatOpenAI` 构建基本对话链。\n",
    "2. **摘要逻辑**：自定义 `summarize_text_with_llm`，调用 LLM 对 AI 的回复做简短总结。\n",
    "3. **历史管理**：在 `get_session_history` 中，每次更新历史时，用摘要替换最后一条 AI 回复。\n",
    "4. **RunnableWithMessageHistory**：把链与记忆机制结合，让它在多轮对话中自动调用摘要逻辑。\n",
    "\n",
    "通过这种方式，我们实现了类似 **ConversationSummaryMemory** 的效果：在保持上下文连贯的同时，大幅度减少了历史消息的冗余。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradio 前端记忆\n",
    "\n",
    "假如我们希望能够把聊天记录显示在gradio上，我们就需要Gradio.Chatbot这个组件！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\76391\\.conda\\envs\\chapter_3_2025_12_15\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(model=\"ernie-3.5-8k\",\n",
    "openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "    (\"human\", \"{input}\")])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "session_store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    return session_store[session_id]\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    runnable=chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "session_id = \"session_test5\"\n",
    "def chat(message, history):\n",
    "    history = history or []\n",
    "    resp = chain_with_history.invoke(\n",
    "        {\"input\": message},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "    reply = resp.content if hasattr(resp, \"content\") else str(resp)\n",
    "    history = history + [\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "        {\"role\": \"assistant\", \"content\": reply},\n",
    "    ]\n",
    "    return \"\", history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 🤖 聊天机器人（内置记忆）\")\n",
    "    chatbot = gr.Chatbot(label=\"LangChain ChatBot\")\n",
    "    msg = gr.Textbox(placeholder=\"和我聊聊吧...\", lines=1)\n",
    "    msg.submit(chat, [msg, chatbot], [msg, chatbot])\n",
    "    gr.ClearButton([msg, chatbot])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者使用前面说的 ChatInterface 实现，此时会更加简化一些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "import os\n",
    "import gradio as gr\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    return session_store[session_id]\n",
    "\n",
    "session_store = {}\n",
    "\n",
    "def llm_response(content, history):\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"ernie-3.5-8k\",\n",
    "        openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "        base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\",\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    \n",
    "    session_id = \"user_1\"\n",
    "    \n",
    "    chain_with_history = RunnableWithMessageHistory(\n",
    "        runnable=chain,\n",
    "        get_session_history=get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "    )\n",
    "    resp = chain_with_history.invoke(\n",
    "            {\"input\": content},\n",
    "            config={\"configurable\": {\"session_id\": session_id}},\n",
    "        )\n",
    "    reply = resp.content if hasattr(resp, \"content\") else str(resp)\n",
    "    return reply\n",
    "\n",
    "demo = gr.ChatInterface(fn=llm_response)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter_3_2025_12_15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
