{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3869a687",
   "metadata": {},
   "source": [
    "# 1. 环境配置\n",
    "\n",
    "## 1.1 python 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: langgraph-checkpoint-postgres in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: psycopg in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.2 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langgraph-checkpoint-postgres) (3.0.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langgraph-checkpoint-postgres) (3.11.5)\n",
      "Requirement already satisfied: psycopg-pool>=3.2.0 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langgraph-checkpoint-postgres) (3.3.0)\n",
      "Requirement already satisfied: langchain-core>=0.2.38 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (1.1.2)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from psycopg[binary]) (4.15.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from psycopg[binary]) (2025.2)\n",
      "Collecting psycopg-binary==3.3.2 (from psycopg[binary])\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/25/32/716c57b28eefe02a57a4c9d5bf956849597f5ea476c7010397199e56cfde/psycopg_binary-3.3.2-cp312-cp312-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 3.5/3.5 MB 34.8 MB/s  0:00:00\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (0.4.56)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (9.1.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (0.28.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\76391\\.conda\\envs\\agent\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<4.0.0,>=2.1.2->langgraph-checkpoint-postgres) (2.6.1)\n",
      "Installing collected packages: psycopg-binary\n",
      "Successfully installed psycopg-binary-3.3.2\n"
     ]
    }
   ],
   "source": [
    "! pip install openai==2.11.0 dashscope==1.25.4 langchain-classic==1.0.0 langchain==1.1.3 langchain-community==0.4.1 langchain-openai==1.1.3 arxiv==2.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f043cd21",
   "metadata": {},
   "source": [
    "## 1.2 大模型密钥准备\n",
    "\n",
    "请根据第一章内容获取相关平台的 API KEY，如若未在系统变量中填入，请将 API_KEY 信息写入以下代码（若已设置请忽略）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d46c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxx\"\n",
    "# os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-yyyyyyyy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63366a8b",
   "metadata": {},
   "source": [
    "## 1.3 实践代码\n",
    "\n",
    "为了能够顺利的演示内置中间件的使用详情，这里我们使用一段简单的智能体代码演示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47619fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "论文编号 1605.08386 的信息如下：\n",
      "\n",
      "- **发表日期**: 2016-05-26\n",
      "- **标题**: Heat-bath random walks with Markov bases\n",
      "- **作者**: Caprice Stanley, Tobias Windisch\n",
      "- **摘要**: 研究了由有限集的允许移动构成的格点图，这些移动可以是任意长度。我们证明了在固定整数矩阵的纤维上的这些图的直径可以从上方由一个常数来限制。然后研究了这些图上的热浴随机游走的混合行为。我们还给出了移动集的显式条件，使得热浴随机游走（Glauber动力学的一种推广）在固定维度下是一个扩展器。\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatTongyi\n",
    "import os\n",
    "llm = ChatTongyi(api_key=os.environ.get(\"DASHSCOPE_API_KEY\"), model=\"qwen-turbo\")\n",
    "\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "tools = load_tools([\"arxiv\"])\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver \n",
    "memory = InMemorySaver()\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "agent = create_agent(model=llm, \n",
    "                     tools=tools, \n",
    "                     system_prompt=\"You are a helpful assistant\", \n",
    "                     checkpointer=memory)\n",
    "\n",
    "result1 = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"请使用 arxiv 工具查询论文编号 1605.08386\"}]}, config={\"configurable\": {\"thread_id\": \"user_1\"}})\n",
    "print(result1[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174917d",
   "metadata": {},
   "source": [
    "# 2. 智能体记忆\n",
    "\n",
    "## 2.1 简介\n",
    "在传统的对话系统里，每次调用模型都是“无状态”的：输入一句话 → 得到回复 → 结束。\n",
    "\n",
    "但对于 智能体（Agent） 来说，它需要“活在上下文中”：\n",
    "- 记得你是谁（用户信息）\n",
    "- 记得自己做过什么（工具调用记录）\n",
    "- 记得任务做到哪了（任务状态）\n",
    "- 甚至要能忘掉不重要的事（记忆清理）\n",
    "\n",
    "因此，LangGraph 的 Memory 系统让 Agent 具备了三种能力：\n",
    "- 连续性 —— 让模型能延续上一次的任务。\n",
    "- 个性化 —— 让模型能记住用户习惯和风格。\n",
    "- 合规性 —— 让模型能安全“忘记”数据、保留可追溯性。\n",
    "\n",
    "LangGraph 将记忆分为两大类（按作用范围）：\n",
    "- 短期记忆（checkpointer）：作用在当前对话（Thread）中，记录用户输入、模型回复、工具调用等。\n",
    "- 长期记忆（store）：作用在多个对话中，记录用户长期习惯、任务状态等。\n",
    "\n",
    "下面我们就来详细的赘述一下这两大类记忆的作用和实现方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20214ed",
   "metadata": {},
   "source": [
    "## 2.2 短期记忆\n",
    "\n",
    "### 2.2.1 短期记忆原理\n",
    "\n",
    "LangGraph 在每个会话线程（Thread）中维护一个 State，每次调用 Graph 时都会读取上次的 State，每完成一个节点或工具调用，就会更新并持久化 State。State 存储在 Checkpointer 中，可随时恢复会话。\n",
    "\n",
    "我们前面添加的 Memory 其实都是短期记忆（通过 config 里添加 thread_id 实现）。\n",
    "对于短期记忆而言，我们可以在多个位置获取到这部分信息。比如在工具被使用时，可以通过增加 runtime 参数获取 AgentState（runtime.context）信息：\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def update_user_info(\n",
    "  runtime: ToolRuntime[CustomContext, CustomState],\n",
    ") -> Command:\n",
    "  \"\"\"Look up and update user info.\"\"\"\n",
    "  user_id = runtime.context.user_id \n",
    "  name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n",
    "```\n",
    "对于其他的中间件而言，获取的方式也是类似的。具体可以参考前面中间件创建时关于 Runtime 的介绍。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aeef75",
   "metadata": {},
   "source": [
    "### 2.2.2 短期记忆保存方式\n",
    "除了保存在本地的内存中，在实际的生产环境里我们可以将其存储到 SQL 数据库中：\n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "\n",
    "with PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "    checkpointer.setup()  # 自动建表\n",
    "    agent = create_agent(\n",
    "        model=llm,\n",
    "        tools=[arxiv],\n",
    "        checkpointer=checkpointer,\n",
    "    )\n",
    "```\n",
    "\n",
    "这样，短期记忆会存到数据库里，即使程序重启也能恢复。这种常用于多用户或长时对话的应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3d9e8",
   "metadata": {},
   "source": [
    "### 2.2.3 短期记忆管理\n",
    "\n",
    "对于短期记忆而言，假如把所有的对话记录都进行保存，显然是不现实的。我们需要在使用的过程中定期的对其进行筛选（Filter），从而控制上下文的长度。\n",
    "\n",
    "在 LangChain 中对短期记忆提供了以下几种处理方式：\n",
    "- 自定义中间件：记忆剪裁（Memory Trimming） 和记忆删除（Memory Deletion）\n",
    "- 内置中间件：记忆总结（SummarizationMiddleware）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1a1fe",
   "metadata": {},
   "source": [
    "#### 记忆剪裁\n",
    "\n",
    "在之前讲 RunnableWithMessageHistory 的时候就提到了如何利用 LangChain 提供的方法 trim_messages 进行记忆剪裁。\n",
    "\n",
    "假如我们想要实现类似的效果，可以通过 before_model 的方式对 AgentState 内的 messages 信息进行调整:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61bf85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain.agents.middleware import before_model\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any\n",
    "\n",
    "@before_model\n",
    "def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "  \"\"\"只保留最新的三条信息\"\"\"\n",
    "  messages = state[\"messages\"]\n",
    "  if len(messages) <= 3:\n",
    "    return None\n",
    "  first_msg = messages[0]\n",
    "  recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n",
    "  new_messages = [first_msg] + recent_messages\n",
    "  print(\"Trimmed messages:\", new_messages)\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      RemoveMessage(id=REMOVE_ALL_MESSAGES),\n",
    "      *new_messages]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e5f111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed messages: [HumanMessage(content='请使用 arxiv 工具查询论文编号 1605.08386,', additional_kwargs={}, response_metadata={}, id='7e5f8f85-d50d-4dcf-9114-84c0ea5175b6'), AIMessage(content='', additional_kwargs={'tool_calls': [{'function': {'arguments': '{\"query\": \"1605.08386\"}', 'name': 'arxiv'}, 'id': 'call_8db6d047fd8241939ef9c6', 'index': 0, 'type': 'function'}]}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'tool_calls', 'request_id': '4b5666f4-5c55-42cd-b5c9-7836045196f8', 'token_usage': {'input_tokens': 233, 'output_tokens': 28, 'prompt_tokens_details': {'cached_tokens': 128}, 'total_tokens': 261}}, id='lc_run--019b220f-3c3c-7813-a8cf-c9255d41deeb-0', tool_calls=[{'name': 'arxiv', 'args': {'query': '1605.08386'}, 'id': 'call_8db6d047fd8241939ef9c6', 'type': 'tool_call'}]), ToolMessage(content='Published: 2016-05-26\\nTitle: Heat-bath random walks with Markov bases\\nAuthors: Caprice Stanley, Tobias Windisch\\nSummary: Graphs on lattice points are studied whose edges come from a finite set of allowed moves of arbitrary length. We show that the diameter of these graphs on fibers of a fixed integer matrix can be bounded from above by a constant. We then study the mixing behaviour of heat-bath random walks on these graphs. We also state explicit conditions on the set of moves so that the heat-bath random walk, a generalization of the Glauber dynamics, is an expander in fixed dimension.', name='arxiv', id='5ee833de-a179-43dc-b7ca-03cf632d338a', tool_call_id='call_8db6d047fd8241939ef9c6'), AIMessage(content='论文编号 1605.08386 的标题是《Heat-bath random walks with Markov bases》，作者是 Caprice Stanley 和 Tobias Windisch。该论文发布于 2016 年 5 月 26 日。\\n\\n这篇论文研究了基于格点的图，其边由有限的允许移动集合生成。论文展示了在固定整数矩阵的纤维上，这些图的直径可以被一个常数上界限制。接着，论文探讨了这些图上的热浴随机游走的混合行为。此外，还提出了关于移动集的显式条件，以确保热浴随机游走（一种广义的 Glauber 动态）在固定维度下是一个扩展器。', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': '6eac25d1-d4c0-4612-9d12-c0f9f2abbc1a', 'token_usage': {'input_tokens': 410, 'output_tokens': 157, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 567}}, id='lc_run--019b220f-4aa0-7423-8ab1-d6c76b5dc423-0'), HumanMessage(content='没太懂，可以更详细的讲解一下吗', additional_kwargs={}, response_metadata={}, id='da327430-799e-4ba2-a6af-581bbb4653d5')]\n",
      "Trimmed messages: [HumanMessage(content='请使用 arxiv 工具查询论文编号 1605.08386,', additional_kwargs={}, response_metadata={}, id='7e5f8f85-d50d-4dcf-9114-84c0ea5175b6'), AIMessage(content='论文编号 1605.08386 的标题是《Heat-bath random walks with Markov bases》，作者是 Caprice Stanley 和 Tobias Windisch。该论文发布于 2016 年 5 月 26 日。\\n\\n这篇论文研究了基于格点的图，其边由有限的允许移动集合生成。论文展示了在固定整数矩阵的纤维上，这些图的直径可以被一个常数上界限制。接着，论文探讨了这些图上的热浴随机游走的混合行为。此外，还提出了关于移动集的显式条件，以确保热浴随机游走（一种广义的 Glauber 动态）在固定维度下是一个扩展器。', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': '6eac25d1-d4c0-4612-9d12-c0f9f2abbc1a', 'token_usage': {'input_tokens': 410, 'output_tokens': 157, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 567}}, id='lc_run--019b220f-4aa0-7423-8ab1-d6c76b5dc423-0'), HumanMessage(content='没太懂，可以更详细的讲解一下吗', additional_kwargs={}, response_metadata={}, id='da327430-799e-4ba2-a6af-581bbb4653d5'), AIMessage(content='这篇论文《Heat-bath random walks with Markov bases》（热浴随机游走与马尔可夫基）主要研究了一类基于格点的图结构，以及在这些图上进行的“热浴随机游走”行为。以下是更详细的讲解：\\n\\n---\\n\\n### 1. **背景和基本概念**\\n- **格点（Lattice points）**：指的是在整数坐标上的点，比如二维平面上的 (x, y) 其中 x 和 y 都是整数。\\n- **图（Graphs）**：由节点（格点）和边（连接两个格点的线段）组成。这里的边是由有限的“允许移动”集合生成的。例如，一个简单的移动可能是从 (x, y) 到 (x+1, y) 或 (x, y+1)。\\n- **纤维（Fibers）**：这是一个数学术语，通常指满足某种约束条件的一组格点。例如，如果有一个矩阵 A，纤维可能是指所有满足 Ax = b 的格点 x。\\n\\n---\\n\\n### 2. **论文的核心内容**\\n#### a. **图的直径（Diameter of the Graph）**\\n- 论文证明了在固定整数矩阵的纤维上，这些图的“直径”（即图中最长的最短路径长度）可以被一个常数所限制。这意味着无论你选择哪个纤维，图中的任意两个点之间都可以通过一个相对短的路径连接。\\n- 这个结果非常有用，因为它表明这些图具有良好的连通性，不会出现某些区域难以到达的情况。\\n\\n#### b. **热浴随机游走（Heat-bath random walks）**\\n- 热浴随机游走是一种随机过程，它模拟了系统如何在状态之间转移，类似于物理中的热平衡过程。\\n- 在这种模型中，每个格点的状态会根据周围的状态进行更新，类似于“热浴”对系统的影响。\\n- 论文还提到，这种随机游走是 Glauber 动态的一种推广。Glauber 动态是用于模拟自旋系统的经典方法，广泛应用于统计物理学。\\n\\n#### c. **扩展器（Expander）**\\n- 扩展器是一种图结构，其特点是任何子集的节点都能快速地与外部节点相连。换句话说，这样的图不容易被分割成孤立的部分。\\n- 论文提出了关于“允许移动”集合的条件，使得热浴随机游走在固定维度下是一个扩展器。这表明这种随机游走能够高效地探索整个图。\\n\\n---\\n\\n### 3. **实际意义**\\n- **理论贡献**：论文提供了关于图结构和随机游走的理论分析，特别是在高维空间中的表现。\\n- **应用前景**：这类研究可能在统计物理、计算机科学（如随机算法设计）、甚至机器学习中找到应用。例如，随机游走可以用来模拟复杂的系统，或者用于优化问题的求解。\\n\\n---\\n\\n### 4. **总结**\\n这篇论文的核心在于：\\n- 分析了基于格点的图结构，并证明了它们的直径可以被控制。\\n- 研究了热浴随机游走在这些图上的行为，特别是它的混合速度（即系统达到平衡的速度）。\\n- 提出了条件，使得这种随机游走成为一种高效的扩展器，从而能够在固定维度下快速探索整个图。\\n\\n如果你对某个具体部分（如“马尔可夫基”或“Glauber 动态”）感兴趣，我可以进一步解释！', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': 'c5aae02d-658c-4ba3-8f45-cf0c6a058699', 'token_usage': {'input_tokens': 587, 'output_tokens': 737, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 1324}}, id='lc_run--019b220f-52b2-7c51-ac2d-c068be99415d-0'), HumanMessage(content='还是不懂，你还记得我们查的是哪篇论文嘛？', additional_kwargs={}, response_metadata={}, id='5dccbbc0-4787-4515-9fa3-b8ab263b4b5a')]\n",
      "{'messages': [HumanMessage(content='请使用 arxiv 工具查询论文编号 1605.08386,', additional_kwargs={}, response_metadata={}, id='7e5f8f85-d50d-4dcf-9114-84c0ea5175b6'), AIMessage(content='论文编号 1605.08386 的标题是《Heat-bath random walks with Markov bases》，作者是 Caprice Stanley 和 Tobias Windisch。该论文发布于 2016 年 5 月 26 日。\\n\\n这篇论文研究了基于格点的图，其边由有限的允许移动集合生成。论文展示了在固定整数矩阵的纤维上，这些图的直径可以被一个常数上界限制。接着，论文探讨了这些图上的热浴随机游走的混合行为。此外，还提出了关于移动集的显式条件，以确保热浴随机游走（一种广义的 Glauber 动态）在固定维度下是一个扩展器。', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': '6eac25d1-d4c0-4612-9d12-c0f9f2abbc1a', 'token_usage': {'input_tokens': 410, 'output_tokens': 157, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 567}}, id='lc_run--019b220f-4aa0-7423-8ab1-d6c76b5dc423-0'), HumanMessage(content='没太懂，可以更详细的讲解一下吗', additional_kwargs={}, response_metadata={}, id='da327430-799e-4ba2-a6af-581bbb4653d5'), AIMessage(content='这篇论文《Heat-bath random walks with Markov bases》（热浴随机游走与马尔可夫基）主要研究了一类基于格点的图结构，以及在这些图上进行的“热浴随机游走”行为。以下是更详细的讲解：\\n\\n---\\n\\n### 1. **背景和基本概念**\\n- **格点（Lattice points）**：指的是在整数坐标上的点，比如二维平面上的 (x, y) 其中 x 和 y 都是整数。\\n- **图（Graphs）**：由节点（格点）和边（连接两个格点的线段）组成。这里的边是由有限的“允许移动”集合生成的。例如，一个简单的移动可能是从 (x, y) 到 (x+1, y) 或 (x, y+1)。\\n- **纤维（Fibers）**：这是一个数学术语，通常指满足某种约束条件的一组格点。例如，如果有一个矩阵 A，纤维可能是指所有满足 Ax = b 的格点 x。\\n\\n---\\n\\n### 2. **论文的核心内容**\\n#### a. **图的直径（Diameter of the Graph）**\\n- 论文证明了在固定整数矩阵的纤维上，这些图的“直径”（即图中最长的最短路径长度）可以被一个常数所限制。这意味着无论你选择哪个纤维，图中的任意两个点之间都可以通过一个相对短的路径连接。\\n- 这个结果非常有用，因为它表明这些图具有良好的连通性，不会出现某些区域难以到达的情况。\\n\\n#### b. **热浴随机游走（Heat-bath random walks）**\\n- 热浴随机游走是一种随机过程，它模拟了系统如何在状态之间转移，类似于物理中的热平衡过程。\\n- 在这种模型中，每个格点的状态会根据周围的状态进行更新，类似于“热浴”对系统的影响。\\n- 论文还提到，这种随机游走是 Glauber 动态的一种推广。Glauber 动态是用于模拟自旋系统的经典方法，广泛应用于统计物理学。\\n\\n#### c. **扩展器（Expander）**\\n- 扩展器是一种图结构，其特点是任何子集的节点都能快速地与外部节点相连。换句话说，这样的图不容易被分割成孤立的部分。\\n- 论文提出了关于“允许移动”集合的条件，使得热浴随机游走在固定维度下是一个扩展器。这表明这种随机游走能够高效地探索整个图。\\n\\n---\\n\\n### 3. **实际意义**\\n- **理论贡献**：论文提供了关于图结构和随机游走的理论分析，特别是在高维空间中的表现。\\n- **应用前景**：这类研究可能在统计物理、计算机科学（如随机算法设计）、甚至机器学习中找到应用。例如，随机游走可以用来模拟复杂的系统，或者用于优化问题的求解。\\n\\n---\\n\\n### 4. **总结**\\n这篇论文的核心在于：\\n- 分析了基于格点的图结构，并证明了它们的直径可以被控制。\\n- 研究了热浴随机游走在这些图上的行为，特别是它的混合速度（即系统达到平衡的速度）。\\n- 提出了条件，使得这种随机游走成为一种高效的扩展器，从而能够在固定维度下快速探索整个图。\\n\\n如果你对某个具体部分（如“马尔可夫基”或“Glauber 动态”）感兴趣，我可以进一步解释！', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': 'c5aae02d-658c-4ba3-8f45-cf0c6a058699', 'token_usage': {'input_tokens': 587, 'output_tokens': 737, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 1324}}, id='lc_run--019b220f-52b2-7c51-ac2d-c068be99415d-0'), HumanMessage(content='还是不懂，你还记得我们查的是哪篇论文嘛？', additional_kwargs={}, response_metadata={}, id='5dccbbc0-4787-4515-9fa3-b8ab263b4b5a'), AIMessage(content='是的，我们查的是论文编号 **1605.08386**，其标题是《Heat-bath random walks with Markov bases》（热浴随机游走与马尔可夫基），作者是 Caprice Stanley 和 Tobias Windisch。这篇论文发表于 2016 年 5 月 26 日。\\n\\n如果你觉得之前的解释太复杂或者太技术化，我们可以从更基础的角度来理解它。你希望我以哪种方式重新讲解？比如用比喻、类比或者生活中的例子？', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': '4ba892b1-3703-49aa-ad7c-39b3c48031d1', 'token_usage': {'input_tokens': 1171, 'output_tokens': 118, 'prompt_tokens_details': {'cached_tokens': 128}, 'total_tokens': 1289}}, id='lc_run--019b220f-691a-7a21-b39f-1c5122680d54-0')]}\n"
     ]
    }
   ],
   "source": [
    "agent = create_agent(\n",
    "  model=llm,\n",
    "  tools=tools,\n",
    "  system_prompt=\"You are a helpful assistant\",\n",
    "  middleware=[trim_messages],\n",
    "  checkpointer=memory\n",
    ")\n",
    "\n",
    "config1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "result1 = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"请使用 arxiv 工具查询论文编号 1605.08386,\"}]}, config=config1)\n",
    "result2 = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"没太懂，可以更详细的讲解一下吗\"}]}, config=config1)\n",
    "result3 = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"还是不懂，你还记得我们查的是哪篇论文嘛？\"}]}, config=config1)\n",
    "\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bcf364",
   "metadata": {},
   "source": [
    "#### 记忆删除\n",
    "\n",
    "在前面的记忆剪裁里，我们是通过 RemoveMessage(id=REMOVE_ALL_MESSAGES) 先把所有的信息删除，然后把我们需要的部分内容添加到列表中。\n",
    "\n",
    "但其实可以灵活的运用 RemoveMessage(id=...) 的方法，根据要求定向的删除某些内容。比如说我们希望每次模型调用完后（after_model）删除掉最开始的两条信息，那我们就可以按下面的方式实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0356e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import after_model\n",
    "@after_model\n",
    "def delete_old_messages(state: AgentState, runtime: Runtime) -> dict | None:\n",
    "  \"\"\"删除最前面两条的记忆\"\"\"\n",
    "  messages = state[\"messages\"]\n",
    "  if len(messages) > 2:\n",
    "    return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e7286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "  model=llm,\n",
    "  tools=tools,\n",
    "  system_prompt=\"Please be concise and to the point.\",\n",
    "  middleware=[delete_old_messages],\n",
    "  checkpointer=InMemorySaver(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aff8e2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [ToolMessage(content='Published: 2016-05-26\\nTitle: Heat-bath random walks with Markov bases\\nAuthors: Caprice Stanley, Tobias Windisch\\nSummary: Graphs on lattice points are studied whose edges come from a finite set of allowed moves of arbitrary length. We show that the diameter of these graphs on fibers of a fixed integer matrix can be bounded from above by a constant. We then study the mixing behaviour of heat-bath random walks on these graphs. We also state explicit conditions on the set of moves so that the heat-bath random walk, a generalization of the Glauber dynamics, is an expander in fixed dimension.', name='arxiv', id='81b1fb00-557d-4ea9-8dd1-05cbf03893cb', tool_call_id='call_c41dc14780d0463eb46020'), AIMessage(content='论文编号 1605.08386 的标题是 \"Heat-bath random walks with Markov bases\"，作者是 Caprice Stanley 和 Tobias Windisch。该论文于 2016 年 5 月 26 日发表。论文研究了基于格点的图，其边由有限的允许移动集构成。论文展示了在固定整数矩阵的纤维上这些图的直径可以被常数上界限制。随后研究了这些图上的热浴随机游走的混合行为，并给出了移动集的显式条件，使得热浴随机游走（Glauber 动态的一种推广）在固定维度下是一个扩展器。', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': '69b3a173-1c48-401a-9c8a-35622810d72d', 'token_usage': {'input_tokens': 412, 'output_tokens': 146, 'prompt_tokens_details': {'cached_tokens': 128}, 'total_tokens': 558}}, id='lc_run--019b220f-a20c-7671-9ecc-9310f8a5494d-0')]}\n"
     ]
    }
   ],
   "source": [
    "config1 = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "result1 = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"请使用 arxiv 工具查询论文编号 1605.08386,\"}]}, config=config1)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8f603",
   "metadata": {},
   "source": [
    "#### SummarizationMiddleware：对话历史摘要\n",
    "\n",
    "从中间件的名称就可以简单的看出，这个中间件主要作用在输入模型前的提示词上的（插入点位置在 before_model）。我们都知道 Agent 在与工作以及大模型的交互过程中会产生大量的对话记录，那这些对话记录的话我们不可能一五一十的都保存起来，因为可能会：\n",
    "- ❌ 超出模型 token 上限（比如 GPT-4o 的 128k）\n",
    "- ❌ 上下文越来越混乱，模型“找不到重点”\n",
    "- ❌ 成本飙升\n",
    "\n",
    "所以这个时候，这个中间件自动帮你处理：\n",
    "- 检测是否超过 token 阈值（比如 4000）\n",
    "- 把历史消息进行摘要压缩\n",
    "- 保留最新若干条消息（比如 20 条）\n",
    "\n",
    "从而让模型“继续记得发生了什么”，但只用更少的 token 表达。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43218650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "middleware = SummarizationMiddleware(\n",
    "    model=llm,\n",
    "    trigger=(\"tokens\", 1000),\n",
    "    keep=(\"messages\", 1),\n",
    "    token_counter=lambda messages: sum(len(m.content) for m in messages if hasattr(m, \"content\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1073ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    "    middleware=[middleware],\n",
    "    checkpointer=InMemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f20f99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\n论文编号 1605.08386 的信息如下：  \\n\\n- **发布日期**: 2016-05-26  \\n- **标题**: Heat-bath random walks with Markov bases  \\n- **作者**: Caprice Stanley, Tobias Windisch  \\n- **摘要**: 研究了由有限允许移动集生成的格点图，这些移动可以是任意长度的。我们证明了在固定整数矩阵的纤维上，这些图的直径可以从上面被一个常数限制。然后研究了这些图上的热浴随机游走的混合行为。我们还给出了移动集的显式条件，使得热浴随机游走（Glauber动力学的一种推广）在固定维度下是一个扩展器。', additional_kwargs={}, response_metadata={}, id='5f152fc8-2379-43d6-8fd4-f58e1e9f3d5a'), HumanMessage(content='还是不懂，你还记得我们查的是哪篇论文嘛？', additional_kwargs={}, response_metadata={}, id='238a1ad8-cc6c-4a5d-b476-aea97393690d'), AIMessage(content='我们查的是论文编号 1605.08386，标题是 \"Heat-bath random walks with Markov bases\"。这篇论文由 Caprice Stanley 和 Tobias Windisch 撰写，发布于 2016 年 5 月 26 日。论文主要研究了由有限允许移动集生成的格点图，并探讨了这些图在固定整数矩阵的纤维上的直径特性以及热浴随机游走的混合行为。', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': 'dc46bc0b-b5c0-408d-b70d-3b837e560431', 'token_usage': {'input_tokens': 395, 'output_tokens': 103, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 498}}, id='lc_run--019b220f-ff76-7f12-83da-df80f297cde8-0')]}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "result1 = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"请使用 arxiv 工具查询论文编号 1605.08386,\"}]}, config=config)\n",
    "result2 = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"没太懂，可以更详细的讲解一下吗\"}]}, config=config)\n",
    "result3 = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"还是不懂，你还记得我们查的是哪篇论文嘛？\"}]}, config=config)\n",
    "\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a661361",
   "metadata": {},
   "source": [
    "这样我们就成功实现了记忆的压缩了。但是除此之外我们还有一个问题是，这个记忆到底只是在传入给模型时候压缩了，还是实际存在 InMemorySaver() 也被改变了呢。\n",
    "\n",
    "为了解决这个问题，我们可以将 agent 的记忆打印出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "409f0f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSnapshot(values={'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\n论文编号 1605.08386 的信息如下：  \\n\\n- **发布日期**: 2016-05-26  \\n- **标题**: Heat-bath random walks with Markov bases  \\n- **作者**: Caprice Stanley, Tobias Windisch  \\n- **摘要**: 研究了由有限允许移动集生成的格点图，这些移动可以是任意长度的。我们证明了在固定整数矩阵的纤维上，这些图的直径可以从上面被一个常数限制。然后研究了这些图上的热浴随机游走的混合行为。我们还给出了移动集的显式条件，使得热浴随机游走（Glauber动力学的一种推广）在固定维度下是一个扩展器。', additional_kwargs={}, response_metadata={}, id='5f152fc8-2379-43d6-8fd4-f58e1e9f3d5a'), HumanMessage(content='还是不懂，你还记得我们查的是哪篇论文嘛？', additional_kwargs={}, response_metadata={}, id='238a1ad8-cc6c-4a5d-b476-aea97393690d'), AIMessage(content='我们查的是论文编号 1605.08386，标题是 \"Heat-bath random walks with Markov bases\"。这篇论文由 Caprice Stanley 和 Tobias Windisch 撰写，发布于 2016 年 5 月 26 日。论文主要研究了由有限允许移动集生成的格点图，并探讨了这些图在固定整数矩阵的纤维上的直径特性以及热浴随机游走的混合行为。', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': 'dc46bc0b-b5c0-408d-b70d-3b837e560431', 'token_usage': {'input_tokens': 395, 'output_tokens': 103, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 498}}, id='lc_run--019b220f-ff76-7f12-83da-df80f297cde8-0')]}, next=(), config={'configurable': {'thread_id': '3', 'checkpoint_ns': '', 'checkpoint_id': '1f0d9b4a-53fa-6b89-800d-7953dbca8caf'}}, metadata={'source': 'loop', 'step': 13, 'parents': {}}, created_at='2025-12-15T12:50:33.823936+00:00', parent_config={'configurable': {'thread_id': '3', 'checkpoint_ns': '', 'checkpoint_id': '1f0d9b4a-46bd-6dac-800c-631c310caec4'}}, tasks=(), interrupts=())\n"
     ]
    }
   ],
   "source": [
    "snapshot = agent.get_state(config) # ✅ 关键\n",
    "print(snapshot)           # 这里是一个 StateSnapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec98842",
   "metadata": {},
   "source": [
    "从结果上看，SummarizationMiddleware 并不是单纯的输入预处理， 它在执行过程中直接修改了智能体的内部状态（state）。所以可以看出这个中间件并不是仅在传入前将内容修改，而是修改后会同步更新给内部的短期记忆。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396df9b",
   "metadata": {},
   "source": [
    "## 2.3 长期记忆\n",
    "\n",
    "### 2.3.1 简介\n",
    "不同于短期记忆解决的是模型在一次对话（thread）中如何记得刚才说过的话。长期记忆要解决的是模型如何在多次对话之间持续记住重要的事情，所以只有特定部分的内容会被记录，比如：\n",
    "- 用户的兴趣（喜欢技术性回答）\n",
    "- 历史事件（上次谈到了 LangGraph 的结构）\n",
    "- 个性化偏好（希望回答简洁、英文风格）\n",
    "\n",
    "长期记忆是通过一个名为 Store 的组件来实现的。我们可以把它理解成一个“键值数据库”，保存记忆文档（Memory Documents）。\n",
    "\n",
    "每条记忆都会存在某个命名空间（namespace）下，拥有唯一的键（key），并且保存的内容是一个JSON 对象（value）。\n",
    "\n",
    "我们可以使用各种方法不断扩展、更新和搜索，包括：\n",
    "- .put() → 添加或更新记忆\n",
    "- .get() → 获取指定记忆\n",
    "- .search() → 搜索相关内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a50759",
   "metadata": {},
   "source": [
    "### 2.3.2 添加并获取指定记忆\n",
    "假如我们要向内写入一条长期记忆的话，可以通过："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c49df966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "user_id = \"user_1\"\n",
    "application_context = \"preferences\"\n",
    "namespace = (user_id, application_context)\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "store.put(\n",
    "    namespace,\n",
    "    \"a-memory\",\n",
    "    {\n",
    "        \"rules\": [\n",
    "            \"User likes short, direct language\",\n",
    "            \"User only speaks English & python\",\n",
    "        ],\n",
    "        \"my-key\": \"my-value\",\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4549e8",
   "metadata": {},
   "source": [
    "这里就是保存到 namespace 中，并且传入的 key 是 \"a-memory\", 对应的 value 是后面的字典内容（结构化的知识）。\n",
    "\n",
    "假如我们希望从长期记忆里查找指定信息，可以通过 .get() 方法指定 namespace 和对应的 key 信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34969a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item(namespace=['user_1', 'preferences'], key='a-memory', value={'rules': ['User likes short, direct language', 'User only speaks English & python'], 'my-key': 'my-value'}, created_at='2025-12-15T12:50:37.870077+00:00', updated_at='2025-12-15T12:50:37.870077+00:00')\n"
     ]
    }
   ],
   "source": [
    "item = store.get(namespace, \"a-memory\")\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52490f7e",
   "metadata": {},
   "source": [
    "此时就会将相关信息返回出来，但这种方法主要针对的是精确知道位置的情况下使用。\n",
    "\n",
    "### 2.3.3 搜索相关记忆\n",
    "\n",
    "但很多场景下，我们并不知道精确搜索的位置，此时我们就需要使用 RAG 的方式实现检索。\n",
    "\n",
    "此时我们需要给 InMemoryStore 加入 Embedding 索引，让 .search() 支持向量语义检索（真实项目中 embed 需要换成你所用的 embedding 模型，dim 换成真实的维度）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ccfd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "def embed(texts: list[str]) -> list[list[float]]:\n",
    "    return [[1.0, 2.0] * len(texts)]\n",
    "\n",
    "store = InMemoryStore(index={\"embed\": embed, \"dims\": 2}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667aae7",
   "metadata": {},
   "source": [
    "我们就可以通过 .search 的方式在 namespace进行检索了。当然我们这里可以通过 filter 添加一些硬性条件（相当于 SQL 中的 where），然后 query 里就是对应查找的问题内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3071f037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Item(namespace=['user_2', 'preferences'], key='a-memory', value={'rules': ['User likes short, direct language', 'User only speaks English & python'], 'my-key': 'my-value'}, created_at='2025-12-15T12:50:56.680205+00:00', updated_at='2025-12-15T12:50:56.680205+00:00', score=0.9999999999999998)]\n"
     ]
    }
   ],
   "source": [
    "user_id = \"user_2\"\n",
    "application_context = \"preferences\"\n",
    "namespace = (user_id, application_context)\n",
    "\n",
    "store.put(\n",
    "    namespace,\n",
    "    \"a-memory\",\n",
    "    {\n",
    "        \"rules\": [\n",
    "            \"User likes short, direct language\",\n",
    "            \"User only speaks English & python\",\n",
    "        ],\n",
    "        \"my-key\": \"my-value\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 搜索前需要先写入记忆\n",
    "items = store.search( \n",
    "    namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\"\n",
    ")\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c3ef5",
   "metadata": {},
   "source": [
    "### 2.3.4 实际应用\n",
    "\n",
    "我们可以通过设计两个工具来进行长期记忆的保存和使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed2574f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev', additional_kwargs={}, response_metadata={}, id='1a8354f9-cad8-4606-ab30-059fe7414f66'), AIMessage(content='', additional_kwargs={'tool_calls': [{'function': {'arguments': '{\"user_id\": \"abc123\", \"user_info\": {\"name\": \"Foo\", \"age\": 25, \"email\": \"foo@langchain.dev\"}}', 'name': 'save_user_info'}, 'id': 'call_860c1056dbbf4fd6b3b5b9', 'index': 0, 'type': 'function'}]}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'tool_calls', 'request_id': '2151d08c-1994-4e30-9c3a-0c5699c23141', 'token_usage': {'input_tokens': 256, 'output_tokens': 52, 'prompt_tokens_details': {'cached_tokens': 128}, 'total_tokens': 308}}, id='lc_run--019b2210-9ff7-7210-84d7-51d8c537370f-0', tool_calls=[{'name': 'save_user_info', 'args': {'user_id': 'abc123', 'user_info': {'name': 'Foo', 'age': 25, 'email': 'foo@langchain.dev'}}, 'id': 'call_860c1056dbbf4fd6b3b5b9', 'type': 'tool_call'}]), ToolMessage(content='Successfully saved user info.', name='save_user_info', id='3d8a3585-d831-4675-8ef3-31e32eceec48', tool_call_id='call_860c1056dbbf4fd6b3b5b9'), AIMessage(content=\"The user information has been successfully saved. Let me know if there's anything else I can assist you with!\", additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': 'cecb98e8-1a43-4635-bf15-22a5f7a16cd3', 'token_usage': {'input_tokens': 326, 'output_tokens': 22, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 348}}, id='lc_run--019b2210-a33f-7f72-96ec-26a9a9f8d8fd-0')]}\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "\n",
    "# Access memory\n",
    "@tool\n",
    "def get_user_info(user_id: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Look up user info.\"\"\"\n",
    "    store = runtime.store\n",
    "    user_info = store.get((\"users\",), user_id)\n",
    "    return str(user_info.value) if user_info else \"Unknown user\"\n",
    "\n",
    "# Update memory\n",
    "@tool\n",
    "def save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Save user info.\"\"\"\n",
    "    store = runtime.store\n",
    "    store.put((\"users\",), user_id, user_info)\n",
    "    return \"Successfully saved user info.\"\n",
    "\n",
    "store = InMemoryStore()\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_user_info, save_user_info],\n",
    "    store=store\n",
    ")\n",
    "\n",
    "# First session: save user info\n",
    "result1 = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\"}]\n",
    "})\n",
    "\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2a22549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"Get user info for user with id 'abc123'\", additional_kwargs={}, response_metadata={}, id='575def4f-737c-4db8-88ec-121ee25b0a7b'), AIMessage(content='', additional_kwargs={'tool_calls': [{'function': {'arguments': '{\"user_id\": \"abc123\"}', 'name': 'get_user_info'}, 'id': 'call_4c51ba5c880941e38e8eb2', 'index': 0, 'type': 'function'}]}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'tool_calls', 'request_id': 'e7bd92a9-8d66-435d-a0cf-ae770c3ca3dd', 'token_usage': {'input_tokens': 240, 'output_tokens': 24, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 264}}, id='lc_run--019b2210-bc52-7470-bb69-64fdea050a48-0', tool_calls=[{'name': 'get_user_info', 'args': {'user_id': 'abc123'}, 'id': 'call_4c51ba5c880941e38e8eb2', 'type': 'tool_call'}]), ToolMessage(content=\"{'name': 'Foo', 'age': 25, 'email': 'foo@langchain.dev'}\", name='get_user_info', id='6d248c58-829a-4aef-9177-c172985ca869', tool_call_id='call_4c51ba5c880941e38e8eb2'), AIMessage(content=\"The user information for the user with ID 'abc123' is as follows:\\n\\n- **Name**: Foo\\n- **Age**: 25\\n- **Email**: foo@langchain.dev\", additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': 'e164e6e9-b759-4322-bce1-35a4c3dd1495', 'token_usage': {'input_tokens': 300, 'output_tokens': 41, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 341}}, id='lc_run--019b2210-bebd-7471-9f38-ef37c24a2d8e-0')]}\n"
     ]
    }
   ],
   "source": [
    "# Second session: get user info\n",
    "result2 = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id 'abc123'\"}]\n",
    "})\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb313fcb",
   "metadata": {},
   "source": [
    "### 2.3.5 长期记忆持久化\n",
    "\n",
    "对于 InMemoryStore 这种长期记忆而言，其本质上也是保存在内存之中的。假如我们当前的代码结束了，内存自然也就释放了，也就没办法真正长期保存了。\n",
    "\n",
    "所以假如我们想要在生产中持久化的保存这部分的长期记忆，我们需要将其保存在数据库中。这样即使程序结束，重新启动再 load 一次文件，记忆依然存在。\n",
    "\n",
    "因此建议在开发测试是使用 InMemoryStore 的方式，比较方便调试。而在生产环节使用 PostgresStore 或自定义 DB，从而确保长期记忆能够一直保存。\n",
    "\n",
    "```python\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5432/postgres\"\n",
    "\n",
    "with PostgresStore.from_conn_string(DB_URI) as store:\n",
    "    # 第一次使用必须执行（建表）\n",
    "    store.setup()\n",
    "\n",
    "    # 写入一条长期记忆\n",
    "    store.put(\n",
    "        namespace=(\"user_1\", \"profile\"),\n",
    "        key=\"preference\",\n",
    "        value={\n",
    "            \"language\": \"Chinese\",\n",
    "            \"style\": \"short answers\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 读取\n",
    "    item = store.get(\n",
    "        namespace=(\"user_1\", \"profile\"),\n",
    "        key=\"preference\"\n",
    "    )\n",
    "\n",
    "    print(item.value)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863410a",
   "metadata": {},
   "source": [
    "### 2.3.6 长期记忆清理\n",
    "\n",
    "假如我们希望清理部分的长期记忆，我们主要有三种方式：\n",
    "- 覆盖（最常用，直接基于同一个 namespace 和 key 进行写入即可）\n",
    "\n",
    "```python\n",
    "store.put(\n",
    "    (\"user_1\", \"preference\"),\n",
    "    key=\"answer_style\",   # 👈 固定 key\n",
    "    value={\"text\": \"用户更喜欢详细解释\"}\n",
    ")\n",
    "```\n",
    "\n",
    "- 删除（明确否定时，此时可以给 Value 添加 None 实现是按出）\n",
    "\n",
    "```python\n",
    "store.put(\n",
    "    (\"user_1\", \"preference\"),\n",
    "    key=\"answer_style\",\n",
    "    value=None   # 👈 删除\n",
    ")\n",
    "```\n",
    "\n",
    "- 过期（TTL，不适用于 InMemoryStore，但适用于真实的数据库，如 PostgresStore，后续定期调用 store.start_ttl_sweeper() 即可清理）\n",
    "\n",
    "```python\n",
    "store.put((\"user_1\", \"behavior\"),\n",
    "    key=\"recent_topic\",\n",
    "    value={\"text\": \"最近在研究 LangGraph\"},\n",
    "    ttl=60 * 24  # 24 小时后过期\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679fd00a",
   "metadata": {},
   "source": [
    "# 3. 上下文工程\n",
    "\n",
    "## 3.1 简介\n",
    "\n",
    "在前面三种短期记忆的记忆编辑操作中，我们尝试按照规则去除不必要的历史信息，主要目的是为了减少输入的长度和提高计算效率。\n",
    "\n",
    "但是直接将旧记忆移除或者总结的方式很容易造成重要信息的丢失，因此在实际应用中，我们需要更细致的策略来平衡信息的保留与信息的去除。\n",
    "\n",
    "而上下文工程则是一种更全面的策略，其在不影响具体的记忆的情况下对传给模型的提示词进行处理，从而实现管息的添加与删除。\n",
    "\n",
    "除此之外，上下文工程还如何组织、增强和动态调整上下文，使得模型能够根据当前任务或对话的需要，有效地利用已有信息。\n",
    "\n",
    "在 LangChain 的文档重点对三个方面的上下文控制进行了介绍：\n",
    "- Model Context（模型上下文）：控制传给模型的信息（如提示词、消息历史、工具等）\n",
    "- Tool Context（工具上下文）：控制工具访问和修改的内容（如状态、记忆、配置等）\n",
    "- Life-cycle Context（生命周期上下文）：控制模型和工具之间的操作（如摘要、审查、日志等）\n",
    "\n",
    "下面我们就来逐个的介绍一下内部的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd14ab0f",
   "metadata": {},
   "source": [
    "## 3.2 Model Context （模型上下文）\n",
    "\n",
    "所谓的 Model Context 指的是每次调用 LLM 时输入给它的内容，它是一次性使用的上下文（Transient），不会自动写入 memory，但对模型响应质量影响巨大。\n",
    "\n",
    "Model Context 主要有五个组成部分：\n",
    "- System Prompt：模型角色和行为说明\n",
    "- Messages：之前的对话消息\n",
    "- Tools：当前可以调用的工具\n",
    "- Model：当前使用的 LLM\n",
    "- Response Format：最终输出格式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63133bc3",
   "metadata": {},
   "source": [
    "### 3.2.1 System Prompt （系统提示词）\n",
    "\n",
    "对于系统提示词而言，不同的用户、不同的任务、对模型的要求是不同的，比如：\n",
    "- 用户是管理员：System Prompt 加上「你可以执行高级操作」\n",
    "- 用户喜欢“幽默风格”：System Prompt 加上「用风趣语言回答」\n",
    "- 如果是长对话：System Prompt 可以提示模型「回答尽量简洁」\n",
    "\n",
    "我们可以通过获取动态 AgentState 中的 messages 来进行动态的系统提示词调整："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dcbf6d",
   "metadata": {},
   "source": [
    "### 3.2.2 Messages （对话上下文）\n",
    "\n",
    "除了系统提示词以外，Messages （对话上下文）也是上下文工程的重点。\n",
    "\n",
    "不同于记忆剪裁直接对 Messages 进行总结或者删除，上下文工程主要是在模型调用前对历史信息进行筛选，从而实现在不修改短期记忆的情况下调整 Messages 内的信息。\n",
    "\n",
    "在这里我们可以用到一个内置中间件 ContextEditingMiddleware ，其核心任务是在调用模型之前，遍历消息列表，根据策略自动裁剪、修改、清除上下文消息，以达到控制 token 长度 / 避免冗余 的目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1edc1bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='请使用 arxiv 工具查询论文编号 1605.08386，100字即可', additional_kwargs={}, response_metadata={}, id='2dd2f49e-c53b-4273-b821-f3783342ea76'), AIMessage(content='', additional_kwargs={'tool_calls': [{'function': {'arguments': '{\"query\": \"1605.08386\"}', 'name': 'arxiv'}, 'id': 'call_cc66b99d8dff4a919047db', 'index': 0, 'type': 'function'}]}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'tool_calls', 'request_id': '235fca32-c9c7-4970-a02b-899858d12bd3', 'token_usage': {'input_tokens': 238, 'output_tokens': 28, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 266}}, id='lc_run--019b2210-d1cb-75f0-858d-88cf9168bb51-0', tool_calls=[{'name': 'arxiv', 'args': {'query': '1605.08386'}, 'id': 'call_cc66b99d8dff4a919047db', 'type': 'tool_call'}]), ToolMessage(content='Published: 2016-05-26\\nTitle: Heat-bath random walks with Markov bases\\nAuthors: Caprice Stanley, Tobias Windisch\\nSummary: Graphs on lattice points are studied whose edges come from a finite set of allowed moves of arbitrary length. We show that the diameter of these graphs on fibers of a fixed integer matrix can be bounded from above by a constant. We then study the mixing behaviour of heat-bath random walks on these graphs. We also state explicit conditions on the set of moves so that the heat-bath random walk, a generalization of the Glauber dynamics, is an expander in fixed dimension.', name='arxiv', id='ae23adf7-1efd-4dfe-996b-5b252e173db4', tool_call_id='call_cc66b99d8dff4a919047db'), AIMessage(content='论文编号 1605.08386 的标题是 \"Heat-bath random walks with Markov bases\"，发表于 2016-05-26，作者是 Caprice Stanley 和 Tobias Windisch。该论文研究了基于格点的图，其边由有限的允许移动组成，并探讨了这些图在固定整数矩阵纤维上的直径和热浴随机游走的混合行为。', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': 'd4717dd1-4f93-44db-9bf9-d2ffeb53c763', 'token_usage': {'input_tokens': 415, 'output_tokens': 93, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 508}}, id='lc_run--019b2210-db94-7570-b6b1-e25fb058068d-0'), HumanMessage(content='请使用 arxiv 工具查询论文编号 1706.03762，100字即可', additional_kwargs={}, response_metadata={}, id='20450449-03ad-41f1-9035-88526b4d34ae'), AIMessage(content='', additional_kwargs={'tool_calls': [{'function': {'arguments': '{\"query\": \"1706.03762\"}', 'name': 'arxiv'}, 'id': 'call_695ab3d258934b22a4352f', 'index': 0, 'type': 'function'}]}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'tool_calls', 'request_id': '1aec4e43-a246-4851-acd0-bb34cd14444a', 'token_usage': {'input_tokens': 545, 'output_tokens': 28, 'prompt_tokens_details': {'cached_tokens': 128}, 'total_tokens': 573}}, id='lc_run--019b2210-e0fd-7f80-87a5-92392b9a8211-0', tool_calls=[{'name': 'arxiv', 'args': {'query': '1706.03762'}, 'id': 'call_695ab3d258934b22a4352f', 'type': 'tool_call'}]), ToolMessage(content='Published: 2023-08-02\\nTitle: Attention Is All You Need\\nAuthors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\\nSummary: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.', name='arxiv', id='3753e410-200a-4eb4-be8c-55f1e72a6aa1', tool_call_id='call_695ab3d258934b22a4352f'), AIMessage(content='论文编号 1706.03762 的标题是 \"Attention Is All You Need\"，发表于 2023-08-02，作者是 Ashish Vaswani 等。该论文提出了一种基于注意力机制的全新网络架构 Transformer，摒弃了传统的循环和卷积结构，在机器翻译任务中表现出色。', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': '541d9f61-723f-4b0a-9970-60198f43570c', 'token_usage': {'input_tokens': 752, 'output_tokens': 80, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 832}}, id='lc_run--019b2210-f21c-72f3-97a5-8ddcde9f142a-0'), HumanMessage(content='你能总结一下我们查过哪些论文吗？100字即可', additional_kwargs={}, response_metadata={}, id='e32e26e3-3135-4870-adc6-995f5011bcbe'), AIMessage(content='我们查询过的论文包括：1605.08386，标题为 \"Heat-bath random walks with Markov bases\"；1706.03762，标题为 \"Attention Is All You Need\"。', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': 'ca8f051a-57cb-4df4-9f1b-cb668255a5a3', 'token_usage': {'input_tokens': 857, 'output_tokens': 52, 'prompt_tokens_details': {'cached_tokens': 512}, 'total_tokens': 909}}, id='lc_run--019b2210-f680-7100-9d7f-b5eca1bcdb46-0')]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit\n",
    "\n",
    "middleware_context = ContextEditingMiddleware(\n",
    "  edits=[\n",
    "    ClearToolUsesEdit(\n",
    "      trigger=200, # 超过 200 token清理\n",
    "      keep=1,    # 保留最近 1 次\n",
    "      placeholder=\"[旧结果已清理]\",\n",
    "      clear_tool_inputs=True\n",
    "    )\n",
    "  ]\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "  model=llm,\n",
    "  tools=tools,\n",
    "  system_prompt=\"You are a helpful assistant\",\n",
    "  middleware=[middleware_context],\n",
    "  checkpointer=memory\n",
    ")\n",
    "\n",
    "config2 = {\"configurable\": {\"thread_id\": \"user_4\"}}\n",
    "\n",
    "result1 = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"请使用 arxiv 工具查询论文编号 1605.08386，100字即可\"}]}, config=config2)\n",
    "result2 = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"请使用 arxiv 工具查询论文编号 1706.03762，100字即可\"}]}, config=config2)\n",
    "result3 = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"你能总结一下我们查过哪些论文吗？100字即可\"}]}, config=config2)\n",
    "\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd025a2",
   "metadata": {},
   "source": [
    "又比如我们希望某些特定工具的信息不被清理，同时清理掉别的工具信息的话，我们可以加上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4816446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "middleware_context = ContextEditingMiddleware(\n",
    "  edits=[\n",
    "    ClearToolUsesEdit(\n",
    "      trigger=200, # 超过 200 token清理\n",
    "      keep=1,    # 保留最近 1 次\n",
    "      placeholder=\"[旧结果已清理]\",\n",
    "      clear_tool_inputs=True,\n",
    "      exclude_tools=[\"arxiv\"] # 不清理 arxiv 工具调用记录\n",
    "    )\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2cedc9",
   "metadata": {},
   "source": [
    "这个时候我们再运行上面的例子的话，arxiv 工具的记忆都会得以保留下来。\n",
    "\n",
    "除了默认提供的 ClearToolUsesEdit 工具以外，其实我们还可以自己来进行打造类似的工具。本质上来说，任何我们定义的类，只要实现了一个和 LangChain 源码中 apply(messages, count_tokens=...) 方法，就可以当作一个编辑器（ContextEdit）来用。\n",
    "\n",
    "比如，我们写一个「清除所有 AI 回复超过 500 tokens 的上下文」的编辑器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29654d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrimLongAIResponsesEdit:\n",
    "    \"\"\"清除过长 AI 回复的上下文编辑器\"\"\"\n",
    "    def __init__(self, max_tokens_per_ai: int = 500):\n",
    "        self.max_tokens_per_ai = max_tokens_per_ai\n",
    "\n",
    "    def apply(self, messages: list, *, count_tokens):\n",
    "        for i, msg in enumerate(messages):\n",
    "            if isinstance(msg, AIMessage):\n",
    "                tokens = count_tokens([msg])\n",
    "                if tokens > self.max_tokens_per_ai:\n",
    "                    # 替换过长内容\n",
    "                    messages[i] = AIMessage(\n",
    "                        content=f\"[long response truncated: {tokens} tokens removed]\",\n",
    "                        id=msg.id,\n",
    "                        additional_kwargs=getattr(msg, \"additional_kwargs\", {}))\n",
    "\n",
    "agent = create_agent(\n",
    "  model=llm,\n",
    "  tools=tools,\n",
    "  middleware=[\n",
    "    ContextEditingMiddleware(\n",
    "      edits=[\n",
    "        TrimLongAIResponsesEdit(max_tokens_per_ai=500), \n",
    "      ]\n",
    "    )\n",
    "  ],\n",
    "  checkpointer=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea1162",
   "metadata": {},
   "source": [
    "### 3.2.3 Tools （工具）\n",
    "\n",
    "在提供给模型的上下文中，告诉模型当前可用工具列表使其决定调不调用工具、调哪个工具以及用什么参数调用也是非常关键的。比如说：\n",
    "- 管理员能用所有工具，访客只能读 → 用 Runtime Context.user_role\n",
    "- 用户未登录，不能用敏感工具 → 用 State[\"authenticated\"]\n",
    "- 用户已开启“实验功能” → 用 Store 中的 feature_flags\n",
    "\n",
    "在前面提到的工具相关的中间件 LLMToolSelectorMiddleware（智能工具筛选器）其实也能够快速帮我们筛选工具给到智能体去使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64827cc1",
   "metadata": {},
   "source": [
    "### 3.2.4 Model （模型上下文）\n",
    "\n",
    "那对于传给模型的上下文而言，虽然选择的模型本身不会改变上下文内容，但它会影响上下文能放多少、以及怎么理解上下文的信息。\n",
    "- 比如不同的模型有不同的上下文窗口，像 gpt-4o 有 128k 的上下文长度，而 gpt-3.5 只有 4-16k 的长下文长度。假如超过这个长度就会报错，所以当 agent 运行轮数较多时，可能需要选择长上下文窗口的模型。\n",
    "- 另外在模型的能力方面，不同的模型对于上下文的理解也有差异。比如最新的 gpt-5 模型的文本理解和指令跟随能力一定比旧的 gpt-3.5 要更强一些。\n",
    "- 除此之外对于工具调用（Function Calling）的能力上，由于很多模型是并不支持的，因此任务需要工具调用的话就需要找到合适的模型完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c4e57",
   "metadata": {},
   "source": [
    "### 3.2.5 Response Format（结构化输出）\n",
    "\n",
    "类似的，Response Format 结构化输出也并不是直接影响到上下文的信息。但是由于 agent 可能会面对不同的场景，有些时候直接输出字符串就可以了，有些时候要输出表格的内容，有些时候可能还需要展示思考（Reasoning）的过程。\n",
    "这个时候可能就需要动态的去调整输出的格式，从而满足实际的需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e96d7df",
   "metadata": {},
   "source": [
    "## 3.3 Tool Context（工具上下文）\n",
    "\n",
    "所谓的 Tool Context 指的是当模型调用工具后，LangChain 会执行你注册的工具函数，此时工具函数可以根据获取到的 Runtime、AgentState 以及 Store 信息进行判定是否允许执行、执行什么工具或以什么样的参数去执行该工具。\n",
    "\n",
    "其实在 Model Context 里也有 tools 部分的内容，两者的差别在于：\n",
    "- Model Context 里的 tools 是告诉 LLM “你能用哪些工具”\n",
    "- Tool Context 是告诉工具 “你可以访问哪些信息并做出决策”\n",
    "\n",
    "对于 Tool Context 而言，其主要的任务类型分为两个：\n",
    "- Reads（读取）：获取信息选择合适的策略\n",
    "- Writes（写入）：根据工具调用的结果跳转节点或将信息写入长短期记忆中\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b6bcd",
   "metadata": {},
   "source": [
    "## 3.4 Life-cycle Context（生命周期上下文）\n",
    "\n",
    "在构建智能 Agent 时，模型调用和工具执行只是表面流程，但 Agent 运行过程中的真正复杂性在于：\n",
    "- 如何在模型与工具之间传递、总结、保存重要信息；\n",
    "- 如何基于对话上下文判断流程走向（如是否终止、跳过某步、切换模型等）；\n",
    "- 如何跨步骤实现审计、监控、容错等“非功能性需求”。\n",
    "\n",
    "而要实现这些行为控制，不可能靠 prompt 或工具本身完成，必须引入中间件作为“钩子系统”来捕捉生命周期中关键节点，执行：\n",
    "- 上下文修改（如写入 memory、替换对话）\n",
    "- 控制流跳转（如重试、终止、fallback）\n",
    "- 信息记录（如打日志、统计 token、审计操作）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afc809",
   "metadata": {},
   "source": [
    "## 3.5 上下文工程最佳实践\n",
    "\n",
    "- 从简单开始：先用静态的提示词和工具列表起步，等有明确需求时再引入动态上下文。不要一开始就搞太复杂，避免调试困难。先让系统“能跑起来”，再优化上下文工程。\n",
    "- 逐步测试：每次只添加一个上下文工程特性进行测试，避免出错时难以定位。\n",
    "- 监控性能：监控模型调用次数、token 消耗、延迟时间，从而了解该策略是否生效。\n",
    "- 使用内置中间件：善用 LangChain 提供的中间件，如对话总结、自动选择工具等功能，没有必要重复造轮子。\n",
    "- 记录上下文策略：写文档、写注释，说明你的 middleware 为什么要注入这个 prompt、为什么要屏蔽某个 tool，让团队成员容易协作和维护。\n",
    "- 理解短期 vs 长期上下文的区别：模型上下文（Model Context）是临时的（仅当前调用有效），而生命周期上下文（Life-cycle Context）可以永久修改状态。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
